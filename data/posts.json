{
  "posts": [
    {
      "slug": "how-cadvisor-works",
      "title": "cAdvisor源码阅读",
      "date": "2021-08-01",
      "author": null,
      "tags": [
        "go",
        "cadvisor"
      ],
      "content": "\n基于 v0.39.0 版本。cadvisor是通过cgroup获取各个容器的指标的，支持docker、containerd、cri-o等多种容器运行时。\n\nmain 入口函数：\n``` go\nfunc main() {\n\t...\n    // 初始化存储，指标默认存储在内存中，默认仅保留2分钟以内的数据\n    // 也支持持久化，通过storage_driver参数可以指定持久化的存储\n    // 目前支持的存储有bigquery、elasticsearch、influxdb、kafka、redis、statsd、stdout\n\tmemoryStorage, err := NewMemoryStorage()\n\tif err != nil {\n\t\tklog.Fatalf(\"Failed to initialize storage driver: %s\", err)\n\t}\n\n  \t// 定义了一些获取主机文件系统信息的方法\n\tsysFs := sysfs.NewRealSysFs()\n\n  \t// 创建采集指标的http client\n\tcollectorHttpClient := createCollectorHttpClient(*collectorCert, *collectorKey)\n\n  \t// 初始化资源管理器\n\tresourceManager, err := manager.New(memoryStorage, sysFs, housekeepingConfig, includedMetrics, &collectorHttpClient, strings.Split(*rawCgroupPrefixWhiteList, \",\"), *perfEvents)\n\tif err != nil {\n\t\tklog.Fatalf(\"Failed to create a manager: %s\", err)\n\t}\n\n\t...\n\t// 这个函数会默认把容器label和env加到metrics的label里去，如果label和env很多，可能会导致程序占用很多内存，这时可以设置store_container_labels参数为false，同时设置whitelisted_container_labels仅保留需要的label\n\tcontainerLabelFunc := metrics.DefaultContainerLabels\n\tif !*storeContainerLabels {\n\t\twhitelistedLabels := strings.Split(*whitelistedContainerLabels, \",\")\n\t\tcontainerLabelFunc = metrics.BaseContainerLabels(whitelistedLabels)\n\t}\n\n\t// 注册Prometheus的handle\n\tcadvisorhttp.RegisterPrometheusHandler(mux, resourceManager, *prometheusEndpoint, containerLabelFunc, includedMetrics)\n\n\t// 启动资源管理器\n\tif err := resourceManager.Start(); err != nil {\n\t\tklog.Fatalf(\"Failed to start manager: %v\", err)\n\t}\n\n  \t...\n}\n```\nmain函数的主要逻辑是主机、文件系统、handle等的初始化，然后用这些数据去创建一个资源管理器，最后去启动这个资源管理器。\n\n资源管理器的初始化代码就不贴了，这里直接看下资源管理器的定义：\n``` go\ntype manager struct {\n  \t// 存储所有容器\n\tcontainers               map[namespacedContainerName]*containerData\n\tcontainersLock           sync.RWMutex\n  \t// 指标数据存储\n\tmemoryCache              *memory.InMemoryCache\n  \t// 文件系统信息\n\tfsInfo                   fs.FsInfo\n  \t// 主机系统信息\n\tsysFs                    sysfs.SysFs\n\tmachineMu                sync.RWMutex // protects machineInfo\n  \t// 主机信息\n\tmachineInfo              info.MachineInfo\n\tquitChannels             []chan error\n\tcadvisorContainer        string\n    // 是否直接运行在宿主机上，运行在容器内为false\n\tinHostNamespace          bool\n    // 事件处理器\n\teventHandler             events.EventManager\n  \t// 启动时间\n\tstartupTime              time.Time\n  \t// 指标采集的最大时间间隔\n\tmaxHousekeepingInterval  time.Duration\n\tallowDynamicHousekeeping bool\n    // 允许的指标\n\tincludedMetrics          container.MetricSet\n  \t// 容器监听器，支持监听多种容器运行时\n\tcontainerWatchers        []watcher.ContainerWatcher\n  \t// 容器事件channel，每watch到一个容器事件就往eventsChannel里写入一条数据\n\teventsChannel            chan watcher.ContainerEvent\n\tcollectorHTTPClient      *http.Client\n\tnvidiaManager            stats.Manager\n\tperfManager              stats.Manager\n\tresctrlManager           stats.Manager\n\t// List of raw container cgroup path prefix whitelist.\n\trawContainerCgroupPathPrefixWhiteList []string\n}\n```\n\nStart的主要逻辑是初始化不同的容器运行时，并注册监听器，然后监听容器的创建并做相应的处理：\n``` go\nfunc (m *manager) Start() error {\n  \t// 初始化容器运行时监听器，InitializePlugins里的plugins就是不同的容器运行时，如docker、containerd、cri-o等\n\tm.containerWatchers = container.InitializePlugins(m, m.fsInfo, m.includedMetrics)\n\t...\n}\n```\n\n看看InitializePlugins是如何初始化容器运行时的：\n``` go\nfunc InitializePlugins(factory info.MachineInfoFactory, fsInfo fs.FsInfo, includedMetrics MetricSet) []watcher.ContainerWatcher {\n\tpluginsLock.Lock()\n\tdefer pluginsLock.Unlock()\n\n\tcontainerWatchers := []watcher.ContainerWatcher{}\n\tfor name, plugin := range plugins {\n      \t// 注册各个容器运行时的监听器\n\t\twatcher, err := plugin.Register(factory, fsInfo, includedMetrics)\n\t\tif err != nil {\n\t\t\tklog.V(5).Infof(\"Registration of the %s container factory failed: %v\", name, err)\n\t\t}\n      \t// 若不为空，则将监听器保存到containerWatchers数组中\n\t\tif watcher != nil {\n\t\t\tcontainerWatchers = append(containerWatchers, watcher)\n\t\t}\n\t}\n\treturn containerWatchers\n}\n```\n按照我们的理解，返回的containerWatchers应该包含不同容器运行时的监听器，然而，通过查看各个容器运行时的Register方法，发现它们都没有实现ContainerWatcher这个interface：\n``` go\nfunc (p *plugin) Register(factory info.MachineInfoFactory, fsInfo fs.FsInfo, includedMetrics container.MetricSet) (watcher.ContainerWatcher, error) {\n\terr := Register(factory, fsInfo, includedMetrics)\n  \t// 返回的ContainerWatcher是nil\n\treturn nil, err\n}\n```\n也就是说，InitializePlugins并没有注册容器运行时的监听器，那么，不同容器运行时创建的容器到底是如何被监听到的呢？\n\n接着往下看Start方法的代码：\n``` go\nfunc (m *manager) Start() error {\n\tm.containerWatchers = container.InitializePlugins(m, m.fsInfo, m.includedMetrics)\n\n  \t// 这个Register里注册了一个raw类型的工厂方法，当有raw类型的容器被监听到，会使用注册的factory进行处理\n\terr := raw.Register(m, m.fsInfo, m.includedMetrics, m.rawContainerCgroupPathPrefixWhiteList)\n\tif err != nil {\n\t\tklog.Errorf(\"Registration of the raw container factory failed: %v\", err)\n\t}\n\n    // 这里创建的是一个raw类型的容器监听器\n  \trawWatcher, err := raw.NewRawContainerWatcher()\n\tif err != nil {\n\t\treturn err\n\t}\n  \t// 将raw watcher也保存到m.containerWatchers数组中\n\tm.containerWatchers = append(m.containerWatchers, rawWatcher)\n\t...\n\n\tquitWatcher := make(chan error)\n    // 看这儿，监听容器就在这个方法里\n\terr = m.watchForNewContainers(quitWatcher)\n\tif err != nil {\n\t\treturn err\n    }\n}\n``` \n可以看到注册了一个raw类型的watcher，这个rawWatcher实现了watch.ContainerWatcher接口，它才是真正的容器监听器，也就是说不同容器运行时创建的容器都是通过这个监听器监听到的，那它是如何做到的呢？\n\n直接看m.watchForNewContainers()方法的代码：\n``` go\nfunc (m *manager) watchForNewContainers(quit chan error) error {\n\twatched := make([]watcher.ContainerWatcher, 0)\n  \t// 遍历所有containerWatcher，实际上只有raw类型的watcher\n\tfor _, watcher := range m.containerWatchers {\n      \t// 就是这个Start方法了，启动监听器\n\t\terr := watcher.Start(m.eventsChannel)\n\t\tif err != nil {\n\t\t\tfor _, w := range watched {\n\t\t\t\tstopErr := w.Stop()\n\t\t\t\tif stopErr != nil {\n\t\t\t\t\tklog.Warningf(\"Failed to stop wacher %v with error: %v\", w, stopErr)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\twatched = append(watched, watcher)\n\t}\n \t...\n    go func() {\n\t\tfor {\n\t\t\tselect {\n              // 接收容器事件\n\t\t\tcase event := <-m.eventsChannel:\n\t\t\t\tswitch {\n\t\t\t\tcase event.EventType == watcher.ContainerAdd:\n\t\t\t\t\tswitch event.WatchSource {\n\t\t\t\t\tdefault:\n                        // 若为创建容器的事件，则调用createContainer，也就是收集该容器的各项指标保存在内存中，并定时更新\n\t\t\t\t\t\terr = m.createContainer(event.Name, event.WatchSource)\n\t\t\t\t\t}\n\t\t\t\tcase event.EventType == watcher.ContainerDelete:\n                    \t// 若为删除容器的事件，则清理该容器的指标数据\n\t\t\t\t\terr = m.destroyContainer(event.Name)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\tklog.Warningf(\"Failed to process watch event %+v: %v\", event, err)\n\t\t\t\t}\n\t\t\tcase <-quit:\n\t\t\t\tvar errs partialFailure\n\n\t\t\t\t// 若为退出事件，则停止所有的containerWatchers\n\t\t\t\tfor i, watcher := range m.containerWatchers {\n\t\t\t\t\terr := watcher.Stop()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terrs.append(fmt.Sprintf(\"watcher %d\", i), \"Stop\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif len(errs) > 0 {\n\t\t\t\t\tquit <- errs\n\t\t\t\t} else {\n\t\t\t\t\tquit <- nil\n\t\t\t\t\tklog.Infof(\"Exiting thread watching subcontainers\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\treturn nil\n}\n```\n可以看到，它起了一个协程接收容器事件，容器事件来源于watcher.Start(m.eventsChannel)方法里，即真正的监听行为在这个Start里，看它的实现：\n``` go\nfunc (w *rawContainerWatcher) Start(events chan watcher.ContainerEvent) error {\n\t// Watch this container (all its cgroups) and all subdirectories.\n\twatched := make([]string, 0)\n  \t// 首先遍历cgroup子系统，如/sys/fs/cgroup/cpu、/sys/fs/cgroup/memory等等\n\tfor _, cgroupPath := range w.cgroupPaths {\n      \t// watchDirectory是一个递归的方法，它会监听cgroup子系统其所有子目录\n\t\t_, err := w.watchDirectory(events, cgroupPath, \"/\")\n\t\tif err != nil {\n\t\t\tfor _, watchedCgroupPath := range watched {\n\t\t\t\t_, removeErr := w.watcher.RemoveWatch(\"/\", watchedCgroupPath)\n\t\t\t\tif removeErr != nil {\n\t\t\t\t\tklog.Warningf(\"Failed to remove inotify watch for %q with error: %v\", watchedCgroupPath, removeErr)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\twatched = append(watched, cgroupPath)\n\t}\n\n\t// 起个协程处理内核事件\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n              // 这里的w.watcher就是之前初始化的内核事件watcher\n\t\t\tcase event := <-w.watcher.Event():\n            \t// 接收到内核事件后，交给processEvent处理\n\t\t\t\terr := w.processEvent(event, events)\n\t\t\t\tif err != nil {\n\t\t\t\t\tklog.Warningf(\"Error while processing event (%+v): %v\", event, err)\n\t\t\t\t}\n\t\t\tcase err := <-w.watcher.Error():\n\t\t\t\tklog.Warningf(\"Error while watching %q: %v\", \"/\", err)\n\t\t\tcase <-w.stopWatcher:\n\t\t\t\terr := w.watcher.Close()\n\t\t\t\tif err == nil {\n\t\t\t\t\tw.stopWatcher <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn nil\n}\n```\n可以发现，这个rawContainerWatcher其实是监听的cgroup子系统下的所有子目录，因为每个容器都会有对应的cgroup子目录，所以当监听到有一个cgroup子目录被创建时，就可以认为有一个容器被创建了。而且这样做的好处是屏蔽了具体的容器运行时，不管是哪种容器运行时创建的容器都可以被监听到，真是妙啊！\n\n监听目录又是怎么监听的呢？归根到底，是监听的内核事件，上述代码中的w.watch就是内核事件监听器：\n``` go\nfunc NewRawContainerWatcher() (watcher.ContainerWatcher, error) {\n\t...\n\twatcher, err := common.NewInotifyWatcher()\n  \t...\n}\n\nfunc NewInotifyWatcher() (*InotifyWatcher, error) {\n\tw, err := inotify.NewWatcher()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &InotifyWatcher{\n\t\twatcher:           w,\n\t\tcontainersWatched: make(map[string]map[string]bool),\n\t}, nil\n}\n\n  func NewWatcher() (*Watcher, error) {\n\tfd, errno := syscall.InotifyInit1(syscall.IN_CLOEXEC)\n\tif fd == -1 {\n\t\treturn nil, os.NewSyscallError(\"inotify_init\", errno)\n\t}\n\tw := &Watcher{\n\t\tfd:      fd,\n\t\twatches: make(map[string]*watch),\n\t\tpaths:   make(map[int]string),\n\t\tEvent:   make(chan *Event),\n\t\tError:   make(chan error),\n\t\tdone:    make(chan bool, 1),\n\t}\n\t// 这里起了一个协程去监听内核事件\n\tgo w.readEvents()\n\treturn w, nil\n  }\n```\n\n接着看看接收到内核事件后，processEvent是如何处理的：\n``` go\nfunc (w *rawContainerWatcher) processEvent(event *inotify.Event, events chan watcher.ContainerEvent) error {\n\t// 将内核事件转换为容器事件\n\tvar eventType watcher.ContainerEventType\n\tswitch {\n\tcase (event.Mask & inotify.InCreate) > 0:\n\t\teventType = watcher.ContainerAdd\n\tcase (event.Mask & inotify.InDelete) > 0:\n\t\teventType = watcher.ContainerDelete\n\tcase (event.Mask & inotify.InMovedFrom) > 0:\n\t\teventType = watcher.ContainerDelete\n\tcase (event.Mask & inotify.InMovedTo) > 0:\n\t\teventType = watcher.ContainerAdd\n\tdefault:\n\t\t// Ignore other events.\n\t\treturn nil\n\t}\n\n\t// Derive the container name from the path name.\n\tvar containerName string\n\tfor _, mount := range w.cgroupSubsystems.Mounts {\n\t\tmountLocation := path.Clean(mount.Mountpoint) + \"/\"\n\t\tif strings.HasPrefix(event.Name, mountLocation) {\n\t\t\tcontainerName = event.Name[len(mountLocation)-1:]\n\t\t\tbreak\n\t\t}\n\t}\n\tif containerName == \"\" {\n\t\treturn fmt.Errorf(\"unable to detect container from watch event on directory %q\", event.Name)\n\t}\n\n\t// Maintain the watch for the new or deleted container.\n\tswitch eventType {\n\tcase watcher.ContainerAdd:\n\t\t// 若为新增容器事件，表名有新的子目录被创建，则监听该目录及其子目录\n\t\talreadyWatched, err := w.watchDirectory(events, event.Name, containerName)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Only report container creation once.\n\t\tif alreadyWatched {\n\t\t\treturn nil\n\t\t}\n\tcase watcher.ContainerDelete:\n\t\t// 若为删除容器事件，则移除对该目录的监听\n\t\tlastWatched, err := w.watcher.RemoveWatch(containerName, event.Name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Only report container deletion once.\n\t\tif !lastWatched {\n\t\t\treturn nil\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown event type %v\", eventType)\n\t}\n\n\t// 将容器事件写入到eventChannel\n\tevents <- watcher.ContainerEvent{\n\t\tEventType:   eventType,\n\t\tName:        containerName,\n\t\tWatchSource: watcher.Raw,\n\t}\n\n\treturn nil\n}\n```\n可以看到，processEvent就是将内核事件转换为了容器事件，并对新建或删除容器的cgroup目录进行监听或移除监听，最后把容器事件写入channel，这样watchForNewContainers()方法里的协程接收到容器事件后就可以对该容器进行相应的处理。\n\n下面简单看看接收到容器事件后，做了什么操作：\n``` go\nfunc (m *manager) createContainerLocked(containerName string, watchSource watcher.ContainerWatchSource) error {\n\t...\n    // 获取容器运行时handler\n\thandler, accept, err := container.NewContainerHandler(containerName, watchSource, m.inHostNamespace)\n\tif err != nil {\n\t\treturn err\n    }\n\n  \t...\n    // 每个容器生成一个containerData对象，也就是一个容器管理器\n  \tcont, err := newContainerData(containerName, m.memoryCache, handler, logUsage, collectorManager, m.maxHousekeepingInterval, m.allowDynamicHousekeeping, clock.RealClock{})\n\tif err != nil {\n\t\treturn err\n    }\n  \t...\n\t// 运行这个容器管理器，会定期更新相关数据\n\treturn cont.Start()\n}\n\nfunc (cd *containerData) Start() error {\n  \t// 这个housekeeping会定期去采集容器指标数据\n\tgo cd.housekeeping()\n\treturn nil\n}\n```\n\n如何获取handler的，看NewContainerHandler的代码：\n``` go\nfunc NewContainerHandler(name string, watchType watcher.ContainerWatchSource, inHostNamespace bool) (ContainerHandler, bool, error) {\n\tfactoriesLock.RLock()\n\tdefer factoriesLock.RUnlock()\n\n\t// 这个factories就是之前plugin.Register()里注册的\n\tfor _, factory := range factories[watchType] {\n\t\tcanHandle, canAccept, err := factory.CanHandleAndAccept(name)\n\t\tif err != nil {\n\t\t\tklog.V(4).Infof(\"Error trying to work out if we can handle %s: %v\", name, err)\n\t\t}\n\t\tif canHandle {\n\t\t\tif !canAccept {\n\t\t\t\tklog.V(3).Infof(\"Factory %q can handle container %q, but ignoring.\", factory, name)\n\t\t\t\treturn nil, false, nil\n\t\t\t}\n\t\t\tklog.V(3).Infof(\"Using factory %q for container %q\", factory, name)\n          \t// 使用第一个可用的容器运行时handler进行处理\n\t\t\thandle, err := factory.NewContainerHandler(name, inHostNamespace)\n\t\t\treturn handle, canAccept, err\n\t\t}\n\t\tklog.V(4).Infof(\"Factory %q was unable to handle container %q\", factory, name)\n\t}\n\n\treturn nil, false, fmt.Errorf(\"no known factory can handle creation of container\")\n}\n```\n不同的容器运行时handler有不同的实现，要看当前节点上运行了哪种容器运行时。\n\n完。\n"
    },
    {
      "slug": "how-it-works-helm-install-in-v2",
      "title": "Helm install源码阅读",
      "date": "2020-12-07",
      "author": null,
      "tags": [
        "go",
        "helm"
      ],
      "content": "\n## 说明\n通过源码了解 `helm install` 的执行流程，helm 版本为 `v2.17.0`\n\n## 客户端\n#### 1. helm 的 main 函数（cmd/helm/helm.go）\n\n   main 函数很好理解，调用了 newRootCmd()，就是 helm 命令的实现，函数内部通过 Cobra 的 AddCommand() 添加了 helm 的所有子命令，其他的就是解析参数，初始化环境变量等。有个地方需要注意，它声明了 PersistentPostRun() ，这个函数在 Run() 函数结束后将被执行，也就是一个收尾操作，收尾操作里执行了一个 teardown() 函数，看下它的内容：\n   ``` go\n   func teardown() {\n        if tillerTunnel != nil {\n            tillerTunnel.Close()\n        }\n    }\n   ```\n   也就是说，每次执行命令之后都会把这个 tillerTunnel 关闭掉，这个 tillerTunnel 是本地到 tiller server 的一个连接，后面再讲。\n   \n#### 2. helm install 的实现\n   直接看 newInstallCmd() 的内容，比较简单，不展示代码了（cmd/helm/install.go）\n   \n   RunE() 函数最终调用的是 inst.run()，在此之前的 locateChartPath() 是根据命令行传入的 chart 目录或压缩包解析得到其绝对路径。此外，还需要注意 PreRunE 里调用的 setupConnection()，以及 inst.client 的赋值也比较重要，这两个后面再看，先看 inst.run() 的主要内容。\n   ``` go\n    func (i *installCmd) run() error {\n        debug(\"CHART PATH: %s\\n\", i.chartPath)\n\n        // 如果命名空间为空，则会根据kubeconfig设置，如果出现异常，则会设为default\n        if i.namespace == \"\" {\n            i.namespace = defaultNamespace()\n        }\n\n        // 用于解析命令行参数的 values，包括 --set、--set-file、--set-string，这些 values 会覆盖 values.yaml 文件的默认参数\n        rawVals, err := vals(i.valueFiles, i.values, i.stringValues, i.fileValues, i.certFile, i.keyFile, i.caFile)\n        if err != nil {\n            return err\n        }\n        // 省略部分代码，下同\n        ...\n\n        // 把 chart 包转换成 Chart 对象\n        // Check chart requirements to make sure all dependencies are present in /charts\n        chartRequested, err := chartutil.Load(i.chartPath)\n        if err != nil {\n            return prettyError(err)\n        }\n\n        if chartRequested.Metadata.Deprecated {\n            fmt.Fprintln(os.Stderr, \"WARNING: This chart is deprecated\")\n        }\n        \n        // 加载 requirements.yaml 文件，并检查是否有依赖其他 chart 包，有的话将依赖的 chart 对象合并到 chartRequested，没有则不处理\n        if req, err := chartutil.LoadRequirements(chartRequested); err == nil {\n            ...\n        } else if err != chartutil.ErrRequirementsNotFound {\n            return fmt.Errorf(\"cannot load requirements: %v\", err)\n        }\n        \n        // 通过 client 去安装 chart\n        res, err := i.client.InstallReleaseFromChart(\n            chartRequested,\n            i.namespace,\n            helm.ValueOverrides(rawVals),\n            helm.ReleaseName(i.name),\n            helm.InstallDryRun(i.dryRun),\n            helm.InstallReuseName(i.replace),\n            helm.InstallDisableHooks(i.disableHooks),\n            helm.InstallDisableCRDHook(i.disableCRDHook),\n            helm.InstallSubNotes(i.subNotes),\n            helm.InstallTimeout(i.timeout),\n            helm.InstallWait(i.wait),\n            helm.InstallDescription(i.description))\n        if err != nil {\n            ...\n        }\n\n        rel := res.GetRelease()\n        if rel == nil {\n            return nil\n        }\n        ...\n\n        // 获取刚刚创建的 release 的状态\n        // Print the status like status command does\n        status, err := i.client.ReleaseStatus(rel.Name)\n        if err != nil {\n            return prettyError(err)\n        }\n        // 返回给用户\n        return write(i.out, &statusWriter{status}, outputFormat(i.output))\n    }\n   ```\n   \n   整个过程还是比较简单的，来看下几个重要的部分：\n   1. chart 是如何被转换成 Chart 对象的，即 chartutil.Load(i.chartPath) 的详细过程\n   2. chart 是如何被安装到 k8s 集群的，即 i.client.InstallReleaseFromChart() 的详细过程\n   3. i.client 是什么\n   \n#### 3. chart 是如何被转换成 Chart 对象的\n\n   看下 chartutil.Load(i.chartPath) 的代码（pkg/chartutil/load.go），它会判断传入的参数是否是目录，是则调用 LoadDir()，否则当做压缩包调用 LoadFile()。比如，通过目录部署 helm install ./mychart，则调用 LoadDir()，通过压缩包部署 helm install ./mychart.tgz，则调用 LoadFile()\n   ``` go\n    func Load(name string) (*chart.Chart, error) {\n        name = filepath.FromSlash(name)\n        fi, err := os.Stat(name)\n        if err != nil {\n            return nil, err\n        }\n        if fi.IsDir() {\n            if validChart, err := IsChartDir(name); !validChart {\n                return nil, err\n            }\n            return LoadDir(name)\n        }\n        return LoadFile(name)\n    }\n   ```\n   \n   先看 LoadFile()，比较简单，这里就不贴代码了，它会判断文件是否存在，然后打开这个文件，再判断这个文件是否是压缩包格式，再通过调用 LoadArchive() 得到 Chart 对象，看下 LoadArchive() 代码：\n   ``` go\n   // LoadArchive loads from a reader containing a compressed tar archive.\n    func LoadArchive(in io.Reader) (*chart.Chart, error) {\n        // 解析 chart 压缩包的内容，返回的 files 存储了整个 chart 内容\n        files, err := loadArchiveFiles(in)\n        if err != nil {\n            return nil, err\n        }\n        return LoadFiles(files)\n    }\n   ```\n   \n   将 chart 转换成 Chart 对象的主要代码如下：\n   ``` go\n   // LoadFiles loads from in-memory files.\n    func LoadFiles(files []*BufferedFile) (*chart.Chart, error) {\n        // 初始化一个 Chart 对象\n        c := &chart.Chart{}\n        subcharts := map[string][]*BufferedFile{}\n        \n        // 遍历 loadArchiveFiles() 返回的 files，最后得到一个完整的 Chart 对象\n        for _, f := range files {\n            if f.Name == \"Chart.yaml\" {\n                m, err := UnmarshalChartfile(f.Data)\n                if err != nil {\n                    return c, err\n                }\n                c.Metadata = m\n                var apiVersion = c.Metadata.ApiVersion\n                if apiVersion != \"\" && apiVersion != ApiVersionV1 {\n                    return c, fmt.Errorf(\"apiVersion '%s' is not valid. The value must be \\\"v1\\\"\", apiVersion)\n                }\n            } else if f.Name == \"values.toml\" {\n                return c, errors.New(\"values.toml is illegal as of 2.0.0-alpha.2\")\n            } else if f.Name == \"values.yaml\" {\n                c.Values = &chart.Config{Raw: string(f.Data)}\n            } else if strings.HasPrefix(f.Name, \"templates/\") {\n                c.Templates = append(c.Templates, &chart.Template{Name: f.Name, Data: f.Data})\n            } else if strings.HasPrefix(f.Name, \"charts/\") {\n                if filepath.Ext(f.Name) == \".prov\" {\n                    c.Files = append(c.Files, &any.Any{TypeUrl: f.Name, Value: f.Data})\n                    continue\n                }\n                cname := strings.TrimPrefix(f.Name, \"charts/\")\n                if strings.IndexAny(cname, \"._\") == 0 {\n                    // Ignore charts/ that start with . or _.\n                    continue\n                }\n                parts := strings.SplitN(cname, \"/\", 2)\n                scname := parts[0]\n                subcharts[scname] = append(subcharts[scname], &BufferedFile{Name: cname, Data: f.Data})\n            } else {\n                c.Files = append(c.Files, &any.Any{TypeUrl: f.Name, Value: f.Data})\n            }\n        }\n        \n        ...\n        // 依赖的 chart 也会转换到 Chart 对象\n        for n, files := range subcharts {\n            ...\n        }\n    }\n   ```\n   \n   上面的 files 参数是一个 BufferFile 类型的数组，BufferFile 类型声明如下，仅有 Name 和 Data 两个字段：\n   ``` go\n   // BufferedFile represents an archive file buffered for later processing.\n    type BufferedFile struct {\n        Name string\n        Data []byte\n    }\n   ```\n   结合上面代码可以知道，Name 存放的是文件名，Data 存放的是文件内容，也就是说 loadArchiveFiles() 做的事情就是把压缩包的所有文件名和文件内容（包括依赖的 chart）都存储在了一个 BufferedFile 类型的数组。而 LoadFiles() 通过遍历这个数组，给初始化的 Chart 赋值，最后得到了一个完整的 Chart 对象，以上就是通过压缩包部署的主要逻辑。\n   \n   再去看通过 chart 目录部署的核心代码，即 LoadDir()，代码如下：\n   ``` go\n    // LoadDir loads from a directory.\n    //\n    // This loads charts only from directories.\n    func LoadDir(dir string) (*chart.Chart, error) {\n        topdir, err := filepath.Abs(dir)\n        if err != nil {\n            return nil, err\n        }\n\n        // Just used for errors.\n        c := &chart.Chart{}\n\n        // 读取 .helmignore 的内容\n        rules := ignore.Empty()\n        ifile := filepath.Join(topdir, ignore.HelmIgnore)\n        if _, err := os.Stat(ifile); err == nil {\n            r, err := ignore.ParseFile(ifile)\n            if err != nil {\n                return c, err\n            }\n            rules = r\n        }\n        rules.AddDefaults()\n\n        files := []*BufferedFile{}\n        topdir += string(filepath.Separator)\n\n        walk := func(name string, fi os.FileInfo, err error) error {\n            n := strings.TrimPrefix(name, topdir)\n            if n == \"\" {\n                // No need to process top level. Avoid bug with helmignore .* matching\n                // empty names. See issue 1779.\n                return nil\n            }\n\n            // Normalize to / since it will also work on Windows\n            n = filepath.ToSlash(n)\n\n            if err != nil {\n                return err\n            }\n            if fi.IsDir() {\n                // Directory-based ignore rules should involve skipping the entire\n                // contents of that directory.\n                if rules.Ignore(n, fi) {\n                    return filepath.SkipDir\n                }\n                return nil\n            }\n            \n            // 匹配到 .helmignore 中定义的文件，则不会存储到 files\n            // If a .helmignore file matches, skip this file.\n            if rules.Ignore(n, fi) {\n                return nil\n            }\n\n            // Irregular files include devices, sockets, and other uses of files that\n            // are not regular files. In Go they have a file mode type bit set.\n            // See https://golang.org/pkg/os/#FileMode for examples.\n            if !fi.Mode().IsRegular() {\n                return fmt.Errorf(\"cannot load irregular file %s as it has file mode type bits set\", name)\n            }\n            \n            // 根据文件名读取文件内容\n            data, err := ioutil.ReadFile(name)\n            if err != nil {\n                return fmt.Errorf(\"error reading %s: %s\", n, err)\n            }\n            \n            // 存储到 files\n            files = append(files, &BufferedFile{Name: n, Data: data})\n            return nil\n        }\n        // 遍历整个目录，处理所有文件\n        if err = sympath.Walk(topdir, walk); err != nil {\n            return c, err\n        }\n        \n        // 调用 LoadFiles 得到 Chart 对象\n        return LoadFiles(files)\n    }\n   ```\n   \n   可以看出，和处理压缩包的原理也是一样，也把目录下的所有文件存储在了 BufferedFile 数组，最后调用 LoadFiles() 得到 Chart 对象。不过有一点差异的是，目录部署没有把在 .helmignore 中忽略的文件存到 BufferedFile 数组，而压缩包部署则是存储了所有。\n   \n   以上就是 chart 被转换成 Chart 对象的过程，没什么花头，就是读取 chart 的所有文件，再赋值给 Chart 对象而已。\n\n#### 4. chart 是如何被安装到 k8s 集群的\n\n   拿到 Chart 对象之后，调用了 i.client.InstallReleaseFromChart() 去安装 chart，通过跳转可以定位到函数声明的地方（pkg/helm/client.go），经过层层调用，通过以下函数构造一个 rls.InstallReleaseRequest 对象:\n   ``` go\n    // InstallReleaseFromChartWithContext installs a new chart and returns the release response while accepting a context.\n    func (h *Client) installReleaseFromChartWithContext(ctx context.Context, chart *chart.Chart, ns string, opts ...InstallOption) (*rls.InstallReleaseResponse, error) {\n        // 构造 rls.InstallReleaseRequest 对象\n        // apply the install options\n        reqOpts := h.opts\n        for _, opt := range opts {\n            opt(&reqOpts)\n        }\n        req := &reqOpts.instReq\n        req.Chart = chart\n        req.Namespace = ns\n        req.DryRun = reqOpts.dryRun\n        req.DisableHooks = reqOpts.disableHooks\n        req.DisableCrdHook = reqOpts.disableCRDHook\n        req.ReuseName = reqOpts.reuseName\n        ctx = FromContext(ctx)\n\n        if reqOpts.before != nil {\n            if err := reqOpts.before(ctx, req); err != nil {\n                return nil, err\n            }\n        }\n        err := chartutil.ProcessRequirementsEnabled(req.Chart, req.Values)\n        if err != nil {\n            return nil, err\n        }\n        err = chartutil.ProcessRequirementsImportValues(req.Chart)\n        if err != nil {\n            return nil, err\n        }\n\n        return h.install(ctx, req)\n    }\n   ```\n   \n   最后实际调用的是下面这个函数发送请求：\n   ``` go\n   // install executes tiller.InstallRelease RPC.\n    func (h *Client) install(ctx context.Context, req *rls.InstallReleaseRequest) (*rls.InstallReleaseResponse, error) {\n        // 建立 RPC 连接\n        c, err := h.connect(ctx)\n        if err != nil {\n            return nil, err\n        }\n        defer c.Close()\n        \n        // 调用 RPC 接口，将数据发送给 tiller server\n        rlc := rls.NewReleaseServiceClient(c)\n        return rlc.InstallRelease(ctx, req)\n    }\n   ```\n   到这里就可以看出其实就是先建立一个到 tiller 的 RPC 连接，再通过 RPC 接口调用 tiller 服务，最后由 tiller 服务来部署 chart，也就是说 chart 的安装是在 tiller 服务里执行的，这个我们等会再看。\n   \n   先来看下是如何建立到 tiller 的连接的，即 connect 内容：\n   ``` go\n    // connect returns a gRPC connection to Tiller or error. The gRPC dial options\n    // are constructed here.\n    func (h *Client) connect(ctx context.Context) (conn *grpc.ClientConn, err error) {\n        // 建立 RPC 连接时的可选参数\n        opts := []grpc.DialOption{\n            grpc.WithBlock(),\n            grpc.WithKeepaliveParams(keepalive.ClientParameters{\n                // Send keepalive every 30 seconds to prevent the connection from\n                // getting closed by upstreams\n                Time: time.Duration(30) * time.Second,\n            }),\n            grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxMsgSize)),\n        }\n        switch {\n        case h.opts.useTLS:\n            opts = append(opts, grpc.WithTransportCredentials(credentials.NewTLS(h.opts.tlsConfig)))\n        default:\n            opts = append(opts, grpc.WithInsecure())\n        }\n        ctx, cancel := context.WithTimeout(ctx, h.opts.connectTimeout)\n        defer cancel()\n        // 调用 grpc 包建立连接，注意 h.opts.host 参数是如何确定的\n        if conn, err = grpc.DialContext(ctx, h.opts.host, opts...); err != nil {\n            return nil, err\n        }\n        return conn, nil\n    }\n   ```\n   这里有个问题，h.opts.host 究竟是多少？也就是 tiller 服务的地址是如何赋值的？ok，需要回过头去看 i.client.InstallReleaseFromChart() 的 i.client 是如何赋值的。\n    \n#### 5. i.client 是如何赋值的\n\n   来看下 newInstallCmd() 的部分代码：\n   ``` go\n    func newInstallCmd(c helm.Interface, out io.Writer) *cobra.Command {\n        // inst 初始化，这里的 c 是 nil，在 helm.go 里调用时传入的\n        inst := &installCmd{\n            out:    out,\n            client: c,\n        }\n\n        cmd := &cobra.Command{\n            Use:     \"install [CHART]\",\n            Short:   \"Install a chart archive\",\n            Long:    installDesc,\n            PreRunE: func(_ *cobra.Command, _ []string) error { return setupConnection() },\n            RunE: func(cmd *cobra.Command, args []string) error {\n                ... \n                // client 赋值就是这里了\n                inst.client = ensureHelmClient(inst.client)\n                inst.wait = inst.wait || inst.atomic\n\n                return inst.run()\n            },\n        }\n    }\n   ```\n   \n   接着看 ensureHelmClient() 的代码，调用了 newClient() (cmd/helm/helm.go)\n   ``` go\n    // ensureHelmClient returns a new helm client impl. if h is not nil.\n    func ensureHelmClient(h helm.Interface) helm.Interface {\n        if h != nil {\n            return h\n        }\n        return newClient()\n    }\n   ```\n   \n   newClient代码：\n   ``` go\n    func newClient() helm.Interface {\n        // 初始化 tiller 的地址和连接 tiller 的超时时间\n        options := []helm.Option{helm.Host(settings.TillerHost), helm.ConnectTimeout(settings.TillerConnectionTimeout)}\n\n        if settings.TLSVerify || settings.TLSEnable {\n            debug(\"Host=%q, Key=%q, Cert=%q, CA=%q\\n\", settings.TLSServerName, settings.TLSKeyFile, settings.TLSCertFile, settings.TLSCaCertFile)\n            tlsopts := tlsutil.Options{\n                ServerName:         settings.TLSServerName,\n                KeyFile:            settings.TLSKeyFile,\n                CertFile:           settings.TLSCertFile,\n                InsecureSkipVerify: true,\n            }\n            if settings.TLSVerify {\n                tlsopts.CaCertFile = settings.TLSCaCertFile\n                tlsopts.InsecureSkipVerify = false\n            }\n            tlscfg, err := tlsutil.ClientConfig(tlsopts)\n            if err != nil {\n                fmt.Fprintln(os.Stderr, err)\n                os.Exit(2)\n            }\n            options = append(options, helm.WithTLS(tlscfg))\n        }\n        // 即 newInstallCmd() 中赋给 i.client 的值\n        return helm.NewClient(options...)\n    }\n   ```\n   \n   也就是说 tiller 的地址来自 settings.TillerHost，这里的 settings 是 environment 包的 EnvSettings 类型（pkg/helm/environment/environment.go），在 cmd/helm/helm.go 里初始化的\n   ``` go\n    var (\n        tillerTunnel *kube.Tunnel\n        // helm_env 就是 \"pkg/helm/environment\"\n        settings     helm_env.EnvSettings\n    )\n   ```\n   TillerHost 是 EnvSettings 类型的一个字段，通过 AddFlags() 函数发现，可以通过命令行参数 `host` 给 TillerHost 赋值，并会覆盖 $HELM_HOST 变量，默认是空字符串。settings.AddFlags() 是在 newRootCmd() 中调用的，也就是入口函数里。\n   ``` go\n    // AddFlags binds flags to the given flagset.\n    func (s *EnvSettings) AddFlags(fs *pflag.FlagSet) {\n        ...\n        fs.StringVar(&s.TillerHost, \"host\", \"\", \"Address of Tiller. Overrides $HELM_HOST\")\n        ...\n    }\n   ```\n   什么？默认是空字符串？那是怎么和 tiller 建立连接的？肯定有个地方赋值了的。\n   \n   还记得第二点提到的 PreRunE 中调用的 setupConnection() 吗？就是在这里赋值的：\n   ``` go\n   func setupConnection() error {\n        if settings.TillerHost == \"\" {\n            config, client, err := getKubeClient(settings.KubeContext, settings.KubeConfig)\n            if err != nil {\n                return err\n            }\n            \n            // 这里的 tillerTunnel 是本地到 tiller Pod 的一个通信隧道，隧道的建立就是通过端口转发的\n            tillerTunnel, err = portforwarder.New(settings.TillerNamespace, client, config)\n            if err != nil {\n                return err\n            }\n            \n            // tiller 的地址设为本地地址和一个随机端口\n            settings.TillerHost = fmt.Sprintf(\"127.0.0.1:%d\", tillerTunnel.Local)\n            debug(\"Created tunnel using local port: '%d'\\n\", tillerTunnel.Local)\n        }\n\n        // Set up the gRPC config.\n        debug(\"SERVER: %q\\n\", settings.TillerHost)\n\n        // Plugin support.\n        return nil\n    }\n   ```\n   \n   因为 PreRunE 会在参数解析之后程序正式运行之前才会被执行，所以命令行没有设置 host 参数导致 TillerHost 为空也没有关系，setupConnection() 里会设置。从这个函数的内容可以看出，如果 TillerHost 为空，则把其地址设为 `fmt.Sprintf(\"127.0.0.1:%d\", tillerTunnel.Local)`，这个 tillerTunnel.Local 是本地随机监听的一个端口。\n   \n#### 6. portforward 端口转发建立连接隧道\n\n   看下 portforwarder.New() 的代码：\n   ``` go\n   // New creates a new and initialized tunnel.\n    func New(namespace string, client kubernetes.Interface, config *rest.Config) (*kube.Tunnel, error) {\n        // 获取 tiller 的 pod 名称\n        podName, err := GetTillerPodName(client.CoreV1(), namespace)\n        if err != nil {\n            return nil, err\n        }\n        // 初始化 tunnel，使用 tiller 的默认 44134 端口\n        t := kube.NewTunnel(client.CoreV1().RESTClient(), config, namespace, podName, environment.DefaultTillerPort)\n        // 建立隧道的代码在 t.ForwardPort \n        return t, t.ForwardPort()\n    }\n   ```\n   首先会获取当前 tiller 的 pod 名称（有多个则获取第一个 pod的），然后把 pod 名称和默认的 tiller 端口传给 kube.NewTunnel()并调用，NewTunnel()是初始化一个 Tunnel 对象，实际建立隧道连接的是 t.ForwardPort()\n   ``` go\n   // ForwardPort opens a tunnel to a kubernetes pod\n    func (t *Tunnel) ForwardPort() error {\n        // Build a url to the portforward endpoint\n        // example: http://localhost:8080/api/v1/namespaces/helm/pods/tiller-deploy-9itlq/portforward\n        u := t.client.Post().\n            Resource(\"pods\").\n            Namespace(t.Namespace).\n            Name(t.PodName).\n            SubResource(\"portforward\").URL()\n\n        transport, upgrader, err := spdy.RoundTripperFor(t.config)\n        if err != nil {\n            return err\n        }\n        dialer := spdy.NewDialer(upgrader, &http.Client{Transport: transport}, \"POST\", u)\n\n        // 本地随机监听一个端口\n        local, err := getAvailablePort()\n        if err != nil {\n            return fmt.Errorf(\"could not find an available port: %s\", err)\n        }\n        t.Local = local\n\n        // 本地端口到 tiller pod 的端口，t.Remote 就是 environment.DefaultTillerPort\n        ports := []string{fmt.Sprintf(\"%d:%d\", t.Local, t.Remote)}\n\n        // 建立本地端口到 tiller pod 的端口的连接隧道，这里的 portforward.New 是 client-go 提供的\n        pf, err := portforward.New(dialer, ports, t.stopChan, t.readyChan, t.Out, t.Out)\n        if err != nil {\n            return err\n        }\n\n        errChan := make(chan error)\n        go func() {\n            errChan <- pf.ForwardPorts()\n        }()\n\n        select {\n        case err = <-errChan:\n            return fmt.Errorf(\"forwarding ports: %v\", err)\n        case <-pf.Ready:\n            return nil\n        }\n    }\n   ```\n   \n   该端口转发的方式其实就是 kubernetes 提供的 `kubectl port-forward` 命令。当我们访问本地这个端口时，也就相当于访问 tiller pod 的默认 44134 端口，即：\n   ```\n   127.0.0.1:随机端口 ---> podIP:44134\n   ```\n   \n   连接建立后，就可以正常通信了，以上，就是执行 helm install 的客户端的流程，下面接着看服务端的流程。\n   \n## 服务端\n#### 7. 服务端 tiller 是如何处理客户端 install 请求的\n\n   通过 RPC 接口可以知道，服务端调用的是 `func (s *ReleaseServer) InstallRelease(c ctx.Context, req *services.InstallReleaseRequest) (*services.InstallReleaseResponse, error)`（位于 pkg/tiller/release_install.go），代码如下：\n   ``` go\n   // InstallRelease installs a release and stores the release record.\n    func (s *ReleaseServer) InstallRelease(c ctx.Context, req *services.InstallReleaseRequest) (*services.InstallReleaseResponse, error) {\n    \ts.Log(\"preparing install for %s\", req.Name)\n        // 预处理，准备创建 release 和 k8s 资源的数据\n        rel, err := s.prepareRelease(req)\n        if err != nil {\n            s.Log(\"failed install prepare step: %s\", err)\n            res := &services.InstallReleaseResponse{Release: rel}\n\n            // On dry run, append the manifest contents to a failed release. This is\n            // a stop-gap until we can revisit an error backchannel post-2.0.\n            if req.DryRun && strings.HasPrefix(err.Error(), \"YAML parse error\") {\n                err = fmt.Errorf(\"%s\\n%s\", err, rel.Manifest)\n            }\n            return res, err\n        }\n\n        s.Log(\"performing install for %s\", req.Name)\n        // 创建release，创建k8s资源\n        res, err := s.performRelease(rel, req)\n        if err != nil {\n            s.Log(\"failed install perform step: %s\", err)\n        }\n        return res, err\n    }\n   ```\n   执行过程分为两步，分别是 prepareRelease() 预处理和 performRelease() 实际执行。\n   \n   预处理就是准备创建 release 和 k8s 资源所需的数据：\n   ``` go\n   // prepareRelease builds a release for an install operation.\n    func (s *ReleaseServer) prepareRelease(req *services.InstallReleaseRequest) (*release.Release, error) {\n        ...\n        // 生成值，用于后面的模板渲染\n        valuesToRender, err := chartutil.ToRenderValuesCaps(req.Chart, req.Values, options, caps)\n        ...\n        // 渲染 manifest、notes，manifestDoc 就是需要创建的所有 k8s 资源\n        hooks, manifestDoc, notesTxt, err := s.renderResources(req.Chart, valuesToRender, req.SubNotes, caps.APIVersions)\n        ...\n\n        // 构造release对象并返回\n        // Store a release.\n        rel := &release.Release{\n            Name:      name,\n            Namespace: req.Namespace,\n            Chart:     req.Chart,\n            Config:    req.Values,\n            Info: &release.Info{\n                FirstDeployed: ts,\n                LastDeployed:  ts,\n                Status:        &release.Status{Code: release.Status_PENDING_INSTALL},\n                Description:   \"Initial install underway\", // Will be overwritten.\n            },\n            Manifest: manifestDoc.String(),\n            Hooks:    hooks,\n            Version:  int32(revision),\n        }\n        if len(notesTxt) > 0 {\n            rel.Info.Status.Notes = notesTxt\n        }\n\n        return rel, nil\n    }\n   ```\n   \n   performRelease() 就是创建 release 和 k8s 资源，看下代码：\n   ``` go\n    func (s *ReleaseServer) performRelease(r *release.Release, req *services.InstallReleaseRequest) (*services.InstallReleaseResponse, error) {\n       ...\n       switch h, err := s.env.Releases.History(req.Name); {\n        // 名称已存在且指定了replace，则为更新操作\n        // if this is a replace operation, append to the release history\n        case req.ReuseName && err == nil && len(h) >= 1:\n            ...\n            s.recordRelease(r, false)\n            if err := s.ReleaseModule.Update(old, r, updateReq, s.env); err != nil {\n                msg := fmt.Sprintf(\"Release replace %q failed: %s\", r.Name, err)\n                s.Log(\"warning: %s\", msg)\n                old.Info.Status.Code = release.Status_SUPERSEDED\n                r.Info.Status.Code = release.Status_FAILED\n                r.Info.Description = msg\n                s.recordRelease(old, true)\n                s.recordRelease(r, true)\n                return res, err\n            }\n\n        default:\n            // 默认为新增操作\n            // nothing to replace, create as normal\n            // regular manifests\n            s.recordRelease(r, false)\n            if err := s.ReleaseModule.Create(r, req, s.env); err != nil {\n                msg := fmt.Sprintf(\"Release %q failed: %s\", r.Name, err)\n                s.Log(\"warning: %s\", msg)\n                r.Info.Status.Code = release.Status_FAILED\n                r.Info.Description = msg\n                s.recordRelease(r, true)\n                return res, fmt.Errorf(\"release %s failed: %s\", r.Name, err)\n            }\n        }\n    }\n   ```\n   前面是 dry-run、hooks 的判断以及 manifest 的校验，主要看 switch 那块代码，它会先获取同名 release 的历史记录，如果找到历史记录且允许使用相同的名字，则对 release 执行 update 操作，否则默认执行 release 的新增操作，最后返回一个 InstallReleaseResponse 的对象。\n   \n   接着看 default 部分，调用了两个函数，s.recordRelease() 和 s.ReleaseModule.Create()，先看下 recordRelease 的代码：\n   ``` go\n   // recordRelease with an update operation in case reuse has been set.\n    func (s *ReleaseServer) recordRelease(r *release.Release, reuse bool) {\n        if reuse {\n            if err := s.env.Releases.Update(r); err != nil {\n                s.Log(\"warning: Failed to update release %s: %s\", r.Name, err)\n            }\n        } else if err := s.env.Releases.Create(r); err != nil {\n            s.Log(\"warning: Failed to record release %s: %s\", r.Name, err)\n        }\n    }\n   ```\n   其中，s.env 是在 tiller server 启动时赋值的（cmd/tiller/tiller.go），它是一个 Environment 类型，这个类型包含三个字段，声明如下：\n   ``` go\n    // Environment provides the context for executing a client request.\n    //\n    // All services in a context are concurrency safe.\n    type Environment struct {\n        // EngineYard provides access to the known template engines.\n        EngineYard EngineYard\n        // Releases stores records of releases.\n        Releases *storage.Storage\n        // KubeClient is a Kubernetes API client.\n        KubeClient KubeClient\n    }\n   ```\n   我们只看 Release，s.env.Release 是一个 storage.Storage 类型，这个类型包含了一个 driver.Driver 接口，tiller 里面实现了 Memory、ConfigMap、Secret、SQL 四种驱动接口，所以这里的 s.env.Release 其实就是一个存储引擎，env 初始化的时候默认存储引擎是 Memory，而 tiller 的启动参数默认使用的 ConfigMap，也就是说 release 默认是存储在 ConfigMap 的。\n   所以，上述 s.env.Releases.Create(r) 调用的其实是 ConfigMap 驱动的 Create 方法（pkg/storage/driver/cfgmaps.go），代码如下：\n   ```go\n    // Create creates a new ConfigMap holding the release. If the\n    // ConfigMap already exists, ErrReleaseExists is returned.\n    func (cfgmaps *ConfigMaps) Create(key string, rls *rspb.Release) error {\n        // set labels for configmaps object meta data\n        var lbs labels\n\n        lbs.init()\n        lbs.set(\"CREATED_AT\", strconv.Itoa(int(time.Now().Unix())))\n\n        // 创建一个包含 release 的 ConfigMap 对象\n        // create a new configmap to hold the release\n        obj, err := newConfigMapsObject(key, rls, lbs)\n        if err != nil {\n            cfgmaps.Log(\"create: failed to encode release %q: %s\", rls.Name, err)\n            return err\n        }\n        // 这里的 impl 是在 tiller.go 里初始化 ConfigMap 存储引擎的时候传入的 clientset.CoreV1().ConfigMaps(namespace())\n        // push the configmap object out into the kubiverse\n        if _, err := cfgmaps.impl.Create(obj); err != nil {\n            if apierrors.IsAlreadyExists(err) {\n                return storageerrors.ErrReleaseExists(key)\n            }\n\n            cfgmaps.Log(\"create: failed to create: %s\", err)\n            return err\n        }\n        return nil\n    }\n   ```\n   可以很清楚的看到，最终是通过 clientset 创建 ConfigMap 资源。去集群里查看 ConfigMap 资源，可以看到每个 release 版本都是存储在一个ConfigMap 中的。\n   \n   到这里，release 的处理就结束了，剩下的就是创建 k8s 资源了，即 s.ReleaseModule.Create()\n   \n   这个 s.ReleaseModule 是一个 ReleaseModule 接口，有 LocalReleaseModule 和 RemoteReleaseModule 两种实现，其初始值根据 useRemote 的值来决定的，默认情况是 false，即使用 LocalReleaseModule：\n   ``` go\n    // NewReleaseServer creates a new release server.\n    func NewReleaseServer(env *environment.Environment, clientset kubernetes.Interface, useRemote bool) *ReleaseServer {\n        var releaseModule ReleaseModule\n        if useRemote {\n            releaseModule = &RemoteReleaseModule{}\n        } else {\n            releaseModule = &LocalReleaseModule{\n                clientset: clientset,\n            }\n        }\n\n        return &ReleaseServer{\n            env:           env,\n            clientset:     clientset,\n            ReleaseModule: releaseModule,\n            Log:           func(_ string, _ ...interface{}) {},\n        }\n    }\n   ```\n   \n   查看 LocalReleaseModule 的 Create 方法：\n   ```\n    // Create creates a release via kubeclient from provided environment\n    func (m *LocalReleaseModule) Create(r *release.Release, req *services.InstallReleaseRequest, env *environment.Environment) error {\n        b := bytes.NewBufferString(r.Manifest)\n        return env.KubeClient.Create(r.Namespace, b, req.Timeout, req.Wait)\n    }\n   ```\n   \n   其中 env.KubeClient 是在 tiller 服务初始化时赋值的，调用了 kube.New()：\n   ``` go\n    func start() {\n        ...\n        kubeClient := kube.New(nil)\n        kubeClient.Log = newLogger(\"kube\").Printf\n        env.KubeClient = kubeClient\n        ...\n    }\n   ```\n   \n   因此，env.KubeClient.Create() 实际上调用的是 kube 包里 Client 类型的 Create 方法，通过代码可以看出，最终也是通过 clientset 创建 k8s 资源的，如下：\n   ``` go\n    // Create creates Kubernetes resources from an io.reader.\n    //\n    // Namespace will set the namespace.\n    func (c *Client) Create(namespace string, reader io.Reader, timeout int64, shouldWait bool) error {\n        client, err := c.KubernetesClientSet()\n        if err != nil {\n            return err\n        }\n        if err := ensureNamespace(client, namespace); err != nil {\n            return err\n        }\n        c.Log(\"building resources from manifest\")\n        infos, buildErr := c.BuildUnstructured(namespace, reader)\n        if buildErr != nil {\n            return buildErr\n        }\n        c.Log(\"creating %d resource(s)\", len(infos))\n        // createResource 是一个辅助函数，用户创建 k8s 资源\n        if err := perform(infos, createResource); err != nil {\n            return err\n        }\n        if shouldWait {\n            return c.waitForResources(time.Duration(timeout)*time.Second, infos)\n        }\n        return nil\n    }\n   ```\n   \n   c.BuildUnstructured() 用于构造一个 unstructured 的数据，再通过 perform() 函数去创建资源，查看 perform 调用的 batchPerform 代码可以发现，它是并行去创建不同 GVK 的资源的。\n   ``` go\n    func batchPerform(infos Result, fn ResourceActorFunc, errs chan<- error) {\n        var kind string\n        var wg sync.WaitGroup\n        for _, info := range infos {\n            currentKind := info.Object.GetObjectKind().GroupVersionKind().Kind\n            if kind != currentKind {\n                wg.Wait()\n                kind = currentKind\n            }\n            wg.Add(1)\n            // 并行创建资源\n            go func(i *resource.Info) {\n                errs <- fn(i)\n                wg.Done()\n            }(info)\n        }\n    }\n   ```\n   \n   最后，回到第一点的 PersistentPostRun，资源创建完毕后，命令结束之前会调用 teardown() 函数，来关闭本地端口到 tiller pod 的连接隧道，至此，整个 helm install 的流程就结束了。\n   \n   简单总结一下整个 install 的过程就是：将 chart 转换成一个 Chart 对象，再构造一个 InstallReleaseRequest 对象，客户端与 tiller 服务端通过端口转发的形式建立 rpc 连接，然后把这个对象发给 tiller 服务端，服务端接收到 install 请求后，会新建一个 release，并把这个 release 存储在 ConfigMap 中，最后通过 clientset 去创建 k8s 资源。\n"
    },
    {
      "slug": "s6-and-s6-overlay",
      "title": "容器进程管理之S6和S6-overlay",
      "date": "2019-12-24",
      "author": null,
      "tags": [
        "docker",
        "s6",
        "s6-overlay"
      ],
      "content": "\n## 什么是s6\n[s6](https://skarnet.org/software/s6/overview.html) 是一个用于 UNIX 的小型的、安全的守护进程管理组件，其英文全称\nskarnet.org's small and secure supervision software suite，因为首字母有6个s，所以被称为 s6。\n\n#### s6包含的组件\ns6 包含很多组件，其核心组件有四个，分别是：s6-svscan、s6-supervise、s6-svscanctl、s6-svc。理论上来说，只要有了这四个核心组件，就可以使用 s6 的功能了，其他的组件只是提供一些附加功能。\n\n其中，s6-svscan 和 s6-supervise 是进程管理树的组件，他们是常驻的，而 s6-svscanctl 和 s6-svc 是用于控制 s6-svscan 和 s6-supervise的，他们不是常驻的。\n\n* s6-supervise\n    s6-supervise 用于监听和维护守护进程的状态，当守护进程挂掉以后，s6-supervise 会重启该进程，它是每个守护进程的直接父进程。\n\n* s6-svscan\n    s6-svscan 用于为每个需要启动的守护进程启动一个 s6-supervise 进程，并监听和维护 s6-supervise 进程的状态，是 s6-supervise 的直接父进程。\n\n* s6-svc\n     s6-svc 用于控制运行中的 s6-supervise 进程，如 `s6-svc -k /var/run/s6/services/nginx`表示杀掉 nginx 进程，`s6-svc -r /var/run/s6/services/nginx` 表示重启 nginx 进程。\n\n* s6-svscanctl\n    s6-svscanctl 是用于控制 s6-svscan 进程的命令行工具，类似 s6-svc 控制 s6-supervise。\n\n#### s6是如何启动的\ns6 是用于管理守护进程的，那这些守护进程是如何被启动的呢？\n\n从上面几个组件可以看出，守护进程是由 s6-supervise 启动的，而 s6-supervise 又是由 s6-svscan 启动的，因此，只需要启动 s6-svscan 就行了。\n\ns6-svscan 启动时需要指定一个目录，如 `s6-svscan -t0 /var/run/s6/services`（若不指定则为当前目录），这个目录用于存放一些子目录，每个子目录表示需要启动的守护进程，子目录的目录名一般为进程名，如下表示需要启动 cron、nginx、ssh三个进程。\n```\n[root@0491df61dd4a services]# pwd\n/var/run/s6/services\n[root@0491df61dd4a services]# ls\ncron  nginx  ssh\n```\n\n各个子目录中包含 run 和 finish 两个脚本文件，run 脚本文件用于启动守护进程，进程必须是前台常驻的，finish 脚本用于进程退出后执行一些清理操作。如下用于启动 nginx 进程的 run 脚本文件：\n```\n[root@0491df61dd4a nginx]# cat run\n#!/usr/bin/with-contenv sh\nexec /usr/local/nginx/sbin/nginx -g 'daemon off;\n```\n\ns6-svscan 会进入到指定的目录，并扫描该目录下的所有子目录。对于每一个子目录，都会创建一个对应的 s6-supervise 进程，再由 s6-supervise 创建对应的守护进程。\n\n通过 ps -ef 可以看到进程树如下所示：\n```\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 11:23 ?        00:00:00 s6-svscan -t0 /var/run/s6/services\nroot        25     1  0 11:23 ?        00:00:00 s6-supervise s6-fdholderd\nroot       155     1  0 11:23 ?        00:00:00 s6-supervise cron\nroot       156     1  0 11:23 ?        00:00:00 s6-supervise ssh\nroot       158     1  0 11:23 ?        00:00:00 s6-supervise nginx\nroot       159   155  0 11:23 ?        00:00:03 /usr/sbin/crond -n\nroot       160   156  0 11:23 ?        00:00:03 /usr/sbin/sshd -D\nroot       162   158  0 11:23 ?        00:00:04 nginx: master process /usr/local/nginx/sbin/nginx -g daemon off;\ndaemon     166   162  0 11:23 ?        00:00:00 nginx: worker process\ndaemon     167   162  0 11:23 ?        00:00:00 nginx: worker process\ndaemon     168   162  0 11:23 ?        00:00:00 nginx: worker process\ndaemon     169   162  0 11:23 ?        00:00:00 nginx: worker process\n```\n\n通过 pstree 命令查看树形图：\n```\ns6-svscan─┬─s6-supervise\n                  ├─s6-supervise───crond\n                  ├─s6-supervise───sshd\n                  └─s6-supervise───nginx───4*[nginx]\n```\n\n## 什么是s6-overlay\n[s6-overlay](\nhttps://github.com/just-containers/s6-overlay) 是基于 s6 的用于容器内多进程管理的工具。其实就是对 s6 做了一下封装，通常使用是在构建镜像时将 s6 的压缩包 s6-overlay.tag.gz 解压到镜像中，并指定镜像的启动命令为其 init 脚本。\n\n```\nFROM busybox\nADD https://github.com/just-containers/s6-overlay/releases/download/v1.21.8.0/s6-overlay-amd64.tar.gz /tmp/\nRUN gunzip -c /tmp/s6-overlay-amd64.tar.gz | tar -xf - -C /\nENTRYPOINT [\"/init\"]\n```\n\n## s6-overlay的执行过程\ns6-overlay的执行过程分为 3 个阶段：\n1. 预处理阶段\n用于准备一些环境变量，并检查一些文件的权限。\n\n2. 启动阶段\n启动阶段又可以分为三个阶段，分别是：\n    * 执行修改相关权限的脚本，脚本位于 /etc/fix-attrs.d 目录。\n    * 执行初始化脚本，用于处理一些初始化的操作，脚本位于 /etc/cont-init.d目录。\n    * 拷贝用户的 /etc/services.d 目录中的文件到 s6 启动时指定的目录中，该 /etc/services.d 目录中存放的内容就是用于启动守护进程的子目录。\n\n3. 结束阶段\n容器退出时， 会先执行 /etc/cont-finish.d 目录中的脚本文件，用于清理相关内容，最后停止容器中的服务。\n\n查看容器启动日志：\n```\n[s6-init] making user provided files available at /var/run/s6/etc...exited 0.\n[s6-init] ensuring user provided files have correct perms...exited 0.\n[fix-attrs.d] applying ownership & permissions fixes...\n[fix-attrs.d] done.\n[cont-init.d] executing container initialization scripts...\n[cont-init.d] done.\n[services.d] starting services\n[services.d] done.\n```\n\ns6-init 即为预处理阶段，后面的 fix-attrs.d、cont-init.d、services.d 即为启动阶段。\n\n查看停掉容器时的日志：\n```\n[cont-finish.d] executing container finish scripts...\n[cont-finish.d] done.\n[s6-finish] waiting for services.\n[s6-finish] syncing disks.\n[s6-finish] sending all processes the TERM signal.\n[s6-finish] sending all processes the KILL signal and exiting.\n```\n以上即为结束阶段。\n\n## 启动脚本init\ns6-overlay的启动命令就是一个 init 脚本，通过查看该脚本内容，发现它调用了以下命令：\n```\n[root@test /]# cat /init\n#!/usr/bin/execlineb -S0\n/etc/s6/init/init-stage1 $@\n```\n\n其中，`/etc/s6/init/init-stage1` 后面有个 `$@`  参数，说明我们也可以在 init 后面指定要启动的服务，比如：\n```\n...\nENTRYPOINT [\"/init\"]\nCMD [\"nginx\"]\n```\n表示仅启动 nginx 服务。\n\n继续查看 /etc/s6/init/init-stage1 文件内容，发现最后一行也是调用了另一个脚本文件：\n```\n[root@test /]# cat /etc/s6/init/init-stage1\n...\n/etc/s6/init-no-catchall/init-stage1 $@\n```\n\n继续查看 /etc/s6/init-no-catchall/init-stage1 内容：\n```\n...\ns6-svscan -t0 /var/run/s6/services\n```\n\n发现最终调用了 `s6-svscan -t0 /var/run/s6/services`，进入容器查看ps -ef，可以看到这个 `s6-svscan -t0 /var/run/s6/services` 也是 1 号进程。所以，到这里也就可以看出 s6-overlay 实际上就是调用了 s6 的功能，是使用 s6-svscan 来管理进程的。\n"
    },
    {
      "slug": "go-limit-concurrent-goroutine",
      "title": "Go的并发控制",
      "date": "2019-04-07",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "## 并发\n下面很简单的一个并发例子：\n\n定义一个全局变量名为 requests 的 `channel`  ，每当有一个请求过来，都会往这个 `channel` 里写入当前时间，同时 handleRequest 函数会一直遍历该 `channel`，每当有一个请求过来，会启一个 goroutine 的 myhandle 函数去处理相应业务逻辑。\n```\npackage main\n\nimport (  \n\t\"fmt\"  \n\t\"log\" \n\t\"net/http\" \n\t\"time\"\n)  \n  \nvar requests = make(chan string)  \n  \nfunc main() {  \n   go handleRequests()  \n   http.HandleFunc(\"/test\", test)  \n   log.Fatal(http.ListenAndServe(\":8080\", nil))  \n}  \n  \nfunc test(w http.ResponseWriter, r *http.Request) {  \n   nowtime := time.Now().Format(\"2006-01-02 15:04:05\")  \n   requests <- nowtime  \n}  \n  \nfunc handleRequests() {  \n   for request := range requests {  \n      go myhandle(request)  \n   }  \n}  \n  \nfunc myhandle(request string) {  \n   fmt.Println(request)  \n}\n```\n\n这种方法在处理比较的小的并发业务时没有什么问题，但是，一旦并发量非常大的时候，会创建非常多的 goroutine 去处理业务，这就会把所在机器负载打的很高，甚至导致宕机。因此，对于并发量很大的业务，需要控制并发量，也就是控制 goroutine 的数量。\n\n## 控制并发量\n看下面的例子：\n\n定义一个全局的缓冲区为100的 channel 变量 size，在 myhandle 函数处理业务逻辑时，往该  channel 里写入一个数值，处理完成后再读取该 channel 一个值。在这种情况下，当并发量超过100时，size 这个 channel 就会被堵塞，因此最多只会有100个 goroutine 在同时处理业务，这样就达到了控制并发的效果。\n```\nvar size = make(chan int, 100)\n\nfunc handleRequests() {  \n   for request := range requests {  \n      go myhandle(request)  \n   }  \n}  \n  \nfunc myhandle(request string) {  \n   size <- 1  \n   fmt.Println(request)  \n   // 模拟业务处理\n   time.Sleep(time.Second)  \n   <-size  \n}\n```\n\n虽然这个例子可以控制并发量，但是有一个问题。尽管最多只会有100个 goroutine 在并发处理，但是 handleRequests 函数为每个请求都创建了一个 goroutine，当并发量达到10000时，会有10000个 goroutine 被创建而只有100个在执行，由于每创建一个 goroutine 也是会消耗资源的，这样就会导致并发量越来越大的时候，程序会不断消耗资源，也可能出现机器负载很高甚至宕机的情况。因此，我们还要控制创建 goroutine 的数量。\n\n## 控制goroutine数量\n这里将上面的例子改进一下，在创建 goroutine 前，先往 size 这个 channel 里写入一个数值，当 size 的长度超过100后，就无法创建更多的 goroutine 了，这样就达到了控制 goroutine 数量的效果。\n```\nvar size = make(chan int, 100)\n\nfunc handleRequests() {  \n   for request := range requests {  \n      size <- 1  \n\t  go myhandle(request, size)  \n   }  \n}  \n  \nfunc myhandle(request string, size chan int) {  \n   fmt.Println(request)  \n   time.Sleep(time.Second)  \n   <-size  \n}\n```\n\n还有一种控制 goroutine 数量的方法，就是在程序启动时启动固定数量的 goroutine，如下：\n```\npackage main  \n  \nimport (  \n\t\"fmt\"  \n\t\"log\" \n\t\"net/http\" \n\t\"time\"\n)  \n  \nvar requests = make(chan string)  \nvar size = 100  \n  \nfunc main() {  \n   handleRequests()  \n   http.HandleFunc(\"/test\", test)  \n   log.Fatal(http.ListenAndServe(\":8080\", nil))  \n}  \n  \nfunc test(w http.ResponseWriter, r *http.Request) {  \n   nowtime := time.Now().Format(\"2006-01-02 15:04:05\")  \n   requests <- nowtime  \n}  \n  \nfunc handleRequests() {  \n   for i := 0; i < size; i++ {  \n      go myhandle()  \n   }  \n}  \n  \nfunc myhandle() {  \n   for request := range requests {  \n      time.Sleep(time.Second)  \n      fmt.Println(request)  \n   }  \n}\n```\n\n这种方式一般不推荐，因为在并发量很小的时候，启动的 goroutine 是一直占用资源的，造成资源浪费，因此使用第一种方式控制 goroutine 数量更佳。\n"
    },
    {
      "slug": "k8s-vpa",
      "title": "Kubernetes之VPA使用介绍",
      "date": "2019-04-06",
      "author": null,
      "tags": [
        "k8s"
      ],
      "content": "\n## 什么是VPA\nVPA 全称 `Vertical Pod Autoscaler`，即垂直 Pod 自动扩缩容，可以根据容器资源使用情况自动设置 CPU 和 内存 的请求值，从而允许在节点上进行适当的调度，以便为每个 Pod 提供适当的资源。它既可以缩小过度请求资源的容器，也可以根据其使用情况随时提升资源不足的容量。**注意：VPA 不会改变 Pod 的资源限制值**。\n\n## 为什么要使用VPA\n使用VPA可以带来以下好处：\n* 因为 Pod 完全用其所需，所以集群节点使用效率高。\n* Pod 会被安排到具有适当可用资源的节点上。\n* 不必运行耗时的基准测试任务来确定 CPU 和内存请求的合适值。\n* VPA 可以随时调整 CPU 和内存请求，而无需执行任何操作，因此可以减少维护时间。\n\n## 开始使用\n#### 使用VPA的先决条件\n* 确保集群中已经启用 `MutatingAdmissionWebhooks`，可以通过以下命令检测：\n```bash\n$ kubectl api-versions | grep admissionregistration\nadmissionregistration.k8s.io/v1beta1\n```\nkubernetes 版本从 1.9 开始，`MutatingAdmissionWebhooks`是默认启用的。\n* 确保集群中部署了`Metrics Server`，若没有，可参考官方github部署。\n\n#### 部署Metrics Server\n```bash\n$ git clone https://github.com/kubernetes-incubator/metrics-server.git\n$ cd metrics-server\n\n一般使用以下命令即可一键部署：\n# Kubernetes > 1.8\n$ kubectl create -f deploy/1.8+/\n\n若kubernete版本比较旧（1.7），可用以下命令安装：\n# Kubernetes 1.7\n$ kubectl create -f deploy/1.7/\n```\n\n这里我部署完成后，查看 Metrics Server 的日志，发现无法获取到指标值：\n```bash\n$ kubectl logs -f metrics-server-67bd89c88d-hzqd4    -n kube-system\nE0215 09:51:18.038199       1 manager.go:102] unable to fully collect metrics: unable to fully scrape metrics from source kubelet_summary:k8s-dev-0-201: unable to fetch metrics from Kubelet k8s-dev-0-21 (k8s-dev-0-201): Get https://k8s-dev-0-201:10250/stats/summary/: dial tcp 220.250.3.201:10250: connect: connection timed out\n```\n\n解决方法：\n\n修改deploy/1.8+/metrics-server-deployment.yaml文件，加入以下内容：\n```yaml\n...\ncommand:\n- /metrics-server\n- --kubelet-preferred-address-types=InternalIP\n...\n```\n相关issue：\n* [https://github.com/kubernetes-sigs/metrics-server/issues/131#issuecomment-418081930](https://github.com/kubernetes-sigs/metrics-server/issues/131#issuecomment-418081930)\n\n重新部署之后又遇到以下报错：\n```\nE1229 07:09:05.013998 1 summary.go:97] error while getting metrics summary from Kubelet kube-node3(10.204.0.21:10250): Get https://10.204.0.21:10250/stats/summary/: x509: cannot validate certificate for 10.204.0.21 because it doesn't contain any IP SANs\n```\n\n这个是证书的问题，可以修改deployment文件，给metrics-server启动命令加入 kubelet-insecure-tls 参数解决：\n```yaml\n...\ncommand:\n- /metrics-server\n- --kubelet-preferred-address-types=InternalIP\n- --kubelet-insecure-tls\n...\n```\n当然，更加安全的方式是使用证书。\n\n#### 部署VPA\nVPA 目前有两个版本，分别是 `0.2.x`和 `0.3.x`，`0.2.x`被称为 `alpha`版，`0.3.x`被称为 `beta`版，`apiVersion`也从 `poc.autoscaling.k8s.io/v1alpha1` 变为了 `\nautoscaling.k8s.io/v1beta1`。\n\n安装步骤如下：\n```bash\n$ git clone https://github.com/kubernetes/autoscaler.git\n$ cd autoscaler/vertical-pod-autoscaler\n$ ./hack/vpa-up.sh\n```\n> **注意**：vpa-up.sh 脚本会读取当前的环境变量：`$REGISTRY` 和 `$TAG`，分别是镜像仓库地址和镜像版本，默认分别是 `k8s.gcr.io`和 `0.3.1`。由于网络的原因，我们无法拉取`k8s.gcr.io`的镜像，因此建议修改 `$REGISTRY`为国内可访问的镜像仓库地址。\n\n若已经安装了 `alpha`版本的 VPA，想要升级到 `beta`版本，最安全的方法是通过 `vpa-down.sh`脚本删除老版本，然后通过 `vpa-up.sh`脚本安装新版本。\n\n若没有修改镜像地址，执行 `vpa-up.sh`脚本后，有以下三个镜像可能无法成功拉取：\n```bash\nk8s.gcr.io/vpa-recommender:0.3.1\nk8s.gcr.io/vpa-updater:0.3.1\nk8s.gcr.io/vpa-admission-controller:0.3.1\n```\n\n可以通过以下两种方式获取镜像：\n1. 配置 Docker 代理\n2. 获取其他镜像源，参考 [https://github.com/anjia0532/gcr.io_mirror](https://github.com/anjia0532/gcr.io_mirror)\n\n检查 VPA 组件是否正常运行：\n```bash\n$ kubectl --namespace=kube-system get pods|grep vpa\nvpa-admission-controller-dfc9bf76d-bq26q   1/1        Running   0          23h\nvpa-recommender-75dc447cdc-lr2h4             1/1        Running   0          23h\nvpa-updater-675cb7944c-d45nz                      1/1        Running   0          23h\n```\n\n#### 仅获取资源推荐不更新Pod示例\n###### 创建 Deployment\n声明一个有 2 个副本，没有资源申请的 Deployment，保存为`my-rec-deployment.yaml`\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: my-rec-deployment\n  labels:\n    purpose: try-recommend\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        purpose: try-recommend\n    spec:\n      containers:\n      - name: my-rec-container\n        image: nginx:latest\n```\n\n创建该 Deployment\n```yaml\nkubectl create -f my-rec-deployment.yaml\n```\n\n###### 创建 VPA\n声明一个更新策略为 `Off`的 VPA，保存为 `my-rec-vpa.yaml`\n```yaml\napiVersion: autoscaling.k8s.io/v1beta1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: my-rec-vpa\nspec:\n  selector:\n    matchLabels:\n      purpose: try-recommend\n  updatePolicy:\n    updateMode: \"Off\"\n```\n\n创建该 VPA\n```yaml\nkubectl create -f my-rec-vpa.yaml\n```\n\n等待几分钟后，查看该 VPA 的详细信息\n```yaml\nkubectl get vpa my-rec-vpa -o yaml\n```\n\n```yaml\n...\n  recommendation:\n    containerRecommendations:\n    - containerName: my-rec-container\n      lowerBound:\n        cpu: 25m\n        memory: 262144k\n      target:\n        cpu: 25m\n        memory: 262144k\n      upperBound:\n        cpu: 25m\n        memory: 262144k\n...\n```\n\n其中`lowerBound` 、`target`、`upperBound` 分别表示 `下限值`、`推荐值`、`上限值`，上述结果表明，推荐的 Pod 的 CPU 请求为 25m，推荐的内存请求为 262144k 字节。\n\nVPA 使用 `lowerBound` 和 `upperBound` 来决定是否删除 Pod 并使用推荐值重新创建。如果 Pod 的请求小于下限或大于上限，则 VPA 将删除 Pod 并重新创建。\n\n因为这里设置 VPA 的更新策略为`Off`，所以对应 Pod 的资源请求不会自动更新。下面来测试自动更新资源请求。\n\n#### 自动更新Pod资源请求示例\n###### 创建 Deployment\n* 声明一个有 2 个副本，CPU 请求 100m，内存请求 50Mi 的 Deployment，保存为`my-deployment.yaml`\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: my-deployment\n  labels:\n    purpose: try-auto-requests\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        purpose: try-auto-requests\n    spec:\n      containers:\n      - name: my-container\n        image: alpine:latest\n        resources:\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"]\n```\n\n创建 Deployment\n```yaml\nkubectl create -f my-deployment.yaml\n```\n\n然后监听对应 Pod 的状态\n```bash\n$ kubectl get pod -w|grep my-deployment\nmy-deployment-79f7977c8-hrnt4        1/1       Running             0          44s\nmy-deployment-79f7977c8-r27kk        1/1       Running             0          44s\n```\n\n###### 创建 VPA\n打开一个新的 Xshell 窗口\n* 声明一个更新策略为 `Auto`的 VPA，保存为 `my-vpa.yaml`\n```yaml\napiVersion: autoscaling.k8s.io/v1beta1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: my-vpa\nspec:\n  selector:\n    matchLabels:\n      purpose: try-auto-requests\n  updatePolicy:\n    updateMode: \"Auto\"\n```\n\n创建 VPA\n```yaml\nkubectl create -f my-vpa.yaml\n```\n\n等待几分钟后，获取该 VPA 的详细信息\n```yaml\nkubectl get vpa my-vpa -o yaml\n```\n\n```yaml\n...\n  recommendation:\n    containerRecommendations:\n    - containerName: my-container\n      lowerBound:\n        cpu: 25m\n        memory: 262144k\n      target:\n        cpu: 35m\n        memory: 262144k\n      upperBound:\n        cpu: 117m\n        memory: 262144k\n```\n\n同时刚刚监听 Pod 的窗口可以看到对应的 Pod 重启了\n```bash\n$ kubectl get pod -w|grep my-deployment\nmy-deployment-79f7977c8-hrnt4        1/1       Running             0          44s\nmy-deployment-79f7977c8-r27kk        1/1       Running             0          44s\nmy-deployment-79f7977c8-r27kk   1/1       Terminating   0         2m\nmy-deployment-79f7977c8-r27kk   1/1       Terminating   0         2m\nmy-deployment-79f7977c8-29kl9   0/1       Pending   0         0s\nmy-deployment-79f7977c8-29kl9   0/1       Pending   0         1s\nmy-deployment-79f7977c8-29kl9   0/1       ContainerCreating   0         1s\nmy-deployment-79f7977c8-29kl9   1/1       Running   0         20s\nmy-deployment-79f7977c8-hrnt4   1/1       Terminating   0         3m\nmy-deployment-79f7977c8-hrnt4   1/1       Terminating   0         3m\nmy-deployment-79f7977c8-558bg   0/1       Pending   0         0s\nmy-deployment-79f7977c8-558bg   0/1       Pending   0         0s\nmy-deployment-79f7977c8-558bg   0/1       ContainerCreating   0         1s\nmy-deployment-79f7977c8-558bg   1/1       Running   0         16s\n```\n\n查看 event 事件消息\n```bash\n$ kubectl get event|grep my-deployment\n9s          9s           1         my-deployment-79f7977c8-hrnt4.1583ca6f62741cc3   Pod                                                       Normal    EvictedByVPA             vpa-updater                 Pod was evicted by VPA Updater to apply resource recommendation.\n4s          4s           2         my-deployment-79f7977c8-hrnt4.1583ca70a5ea6637   Pod                       spec.containers{my-container}   Normal    Killing                  kubelet, k8s-dev-0-21       Killing container with id docker://my-container:Need to kill Pod\n```\n\n其中，vpa-updater Pod was evicted by VPA Updater to apply resource recommendation 表明由于要更新资源请求，Pod 被 VPA Updater驱逐了。\n\n查看重启后的 Pod 详细信息\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    vpaUpdates: 'Pod resources updated by my-vpa: container 0: cpu request, memory request'\nspec:\n    ...\n    resources:\n      requests:\n        cpu: 35m\n        memory: 262144k\n    ...\n```\n\n可以看到，Pod 的 CPU 和 内存 请求都已经改变，请求值就是 VPA 中 target，而且 Pod 的annotations也多了一行 `vpaUpdates`，表明该 Pod 是由 VPA 更新的。\n\n**注意： 需要使用 VPA 的 Pod 必须属于副本集，比如属于 Deployment 或 StatefulSet，这样才能保证 Pod 被驱逐后能自动重启，也就是说部署了 Pod 类型的应用后，VPA 无法更新其资源请求，但 VPA 对象中仍然会显示推荐的资源，这时只能手动删除 Pod，然后重新创建，VPA Admission Controller拦截后才能更改 Pod 的请求值。**\n\n\n## 原理\n#### VPA 更新策略\nVPA 有以下四种更新策略：\n* Initial：仅在 Pod 创建时修改资源请求，以后都不再修改。\n* Auto：默认策略，在 Pod 创建时修改资源请求，并且在 Pod 更新时也会修改。\n* Recreate：类似 `Auto`，在 Pod 的创建和更新时都会修改资源请求，不同的是，只要Pod 中的请求值与新的推荐值不同，VPA 都会驱逐该 Pod，然后使用新的推荐值重新启一个。因此，一般不使用该策略，而是使用 `Auto`，除非你真的需要保证请求值是最新的推荐值。\n* Off：不改变 Pod 的资源请求，不过仍然会在 VPA 中设置资源的推荐值。\n\n若要禁止 VPA 修改 Pod 的请求资源，有以下三种方式：\n1. 将 VPA 的更新策略改为 `Off`\n2. 删除 VPA\n3. 去除 Pod 的 label，使其不再被 VPA 匹配到\n\n> **注意：** 禁用 VPA 后，处于运行状态的 Pod 的资源请求值仍然是 VPA 的推荐值，只有更新 Pod 后，才会使用指定的请求值。\n\n#### VPA 组件\nVPA 主要包含三个组件：\n* Admission Controller\n* Recommender\n* Updater\n\n###### Admission Controller\nAdmission Controller 会拦截所有 Pod 的创建请求，如果 Pod 和某个 VPA 匹配且该 VPA 的更新策略不为 `Off`，Admission Controller 会使用推荐值修改 Pod 的资源请求，否则不会修改。\n\nAdmission Controller 从 Recommender 获取资源的推荐值，如果获取超时或失败，则会使用缓存在对应 VPA 中的推荐值，如果这个推荐值也无法获取，则使用 Pod 指定的请求值。\n\n> **注意：** 以后可能通过将 Pod 标记为 \"requiring VPA\" 来强制使用 VPA，在创建相应的 VPA 之前将不会调度Pod。\n\n###### Recommender\nRecommender 负责计算推荐资源，该组件启动时会获取所有 Pod 的历史资源利用率（无论是否使用了VPA），以及历史存储（如Promethues，通过参数配置）中的 Pod OOM 事件的历史记录，然后聚合这些数据并存储在内存中。\n\nRecommender 会监听集群中的所有 Pod 和 VPA ，对于和某个 VPA 匹配的Pod，它会计算推荐的资源并在对应 VPA 中设置推荐值。\n\n###### Updater\nUpdater 监听集群中的所有 Pod 和 VPA，通过调用 Recommender API 定期获取 VPA 中的推荐值，当一个 Pod 的推荐资源与实际配置的资源相差较大时，Updater 会驱逐这个 Pod（注意：Updater并不负责 Pod 资源的更新），Pod 被其控制器重新创建时，Admission Controller 会拦截这个创建请求，并使用推荐值修改请求值，然后 Pod 使用推荐值被创建。\n\n\n###### 工作流程图\n![vertical-pod-autoscaler.png](/img/2019/04/vertical-pod-autoscaler.png)\n\n\n## 参考\n* [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)\n* [https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md)\n* [https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler?hl=zh-cn](https://cloud.google.com/kubernetes-engine/docs/concepts/verticalpodautoscaler?hl=zh-cn)\n* [https://banzaicloud.com/blog/k8s-vertical-pod-autoscaler/](https://banzaicloud.com/blog/k8s-vertical-pod-autoscaler/)\n\n"
    },
    {
      "slug": "k8s-affinity-topologykey",
      "title": "Kubernetes之Pod亲和性与反亲和性的TopologyKey",
      "date": "2019-03-31",
      "author": null,
      "tags": [
        "kubernetes"
      ],
      "content": "\n## Pod亲和性与反亲和性\nPod 间的亲和性与反亲和性根据已经在 Node 上运行的 Pod 的标签来调度新的 Pod 到哪个 Node 上，这些规则的形式是：\n\n> 如果 X 已经运行一个或多个符合规则 Y 的 Pod，那么这个 Pod 应该（如果是反亲和性，则不应该）运行在 X 上。\n\n和 Node不同，由于 Pod 都是有命名空间的，所以基于 Pod 标签的标签选择器（Label Selector）必须指定命名空间。可以通过 `namespaces`（与 `labelSelector` 和 `topologyKey` 同一级） 指定，默认情况下为拥有亲和性（或反亲和性）的 Pod 所属的命名空间，如果定义了 `namespaces` 但值是空的，则表示使用 `all` 命名空间。\n\n那么，我需要 Pod 亲和性或反亲和性的同时，又能指定 Pod 调度到某个 Node 该如何处理呢？这就要用到接下来讲的 `topologyKey` 了。\n\n## 什么是topologyKey\n顾名思义，`topology` 就是 `拓扑` 的意思，这里指的是一个 `拓扑域`，是指一个范围的概念，比如一个 Node、一个机柜、一个机房或者是一个地区（如杭州、上海）等，实际上对应的还是 Node 上的标签。这里的 `topologyKey` 对应的是 Node 上的标签的 Key（没有Value），可以看出，其实 `topologyKey` 就是用于筛选 Node 的。通过这种方式，我们就可以将各个 Pod 进行跨集群、跨机房、跨地区的调度了。\n\n## 如何使用topologyKey\n看下面的例子：\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-pod-affinity\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: security\n            operator: In\n            values:\n            - S1\n        topologyKey: failure-domain.beta.kubernetes.io/zone\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: security\n              operator: In\n              values:\n              - S2\n          topologyKey: kubernetes.io/hostname\n  containers:\n  - name: with-pod-affinity\n    image: k8s.gcr.io/pause:2.0\n```\n\n这里 Pod 的亲和性规则是：这个 Pod 要调度到的 Node 必须有一个标签为 `security: S1` 的 Pod，且该 Node 必须有一个 Key 为 `failure-domain.beta.kubernetes.io/zone`  的 标签，即 Node 必须属于 `failure-domain.beta.kubernetes.io/zone` 拓扑域。\n\nPod 的反亲和性规则是：这个 Pod 尽量不要调度到这样的 Node，其包含一个 Key 为 `kubernetes.io/hostname` 的标签，且该 Node 上有标签为 `security: S2` 的 Pod。\n\n\n## topologyKey详解\n既然 `topologyKey` 是拓扑域，那 Pod 之间怎样才是属于同一个拓扑域？\n\n如果使用 `k8s.io/hostname`，则表示拓扑域为 Node 范围，那么 `k8s.io/hostname` 对应的值不一样就是不同的拓扑域。比如 Pod1 在 `k8s.io/hostname=node1` 的 Node 上，Pod2 在 `k8s.io/hostname=node2` 的 Node 上，Pod3 在 `k8s.io/hostname=node1` 的 Node 上，则 Pod2 和 Pod1、Pod3 不在同一个拓扑域，而Pod1 和 Pod3在同一个拓扑域。\n\n如果使用 `failure-domain.k8s.io/zone` ，则表示拓扑域为一个区域。同样，Node 的标签 `failure-domain.k8s.io/zone` 对应的值不一样也不是同一个拓扑域，比如 Pod1 在 `failure-domain.k8s.io/zone=beijing` 的 Node 上，Pod2 在 `failure-domain.k8s.io/zone=hangzhou` 的 Node 上，则 Pod1 和 Pod2 不属于同一个拓扑域。\n\n当然，topologyKey 也可以使用自定义标签。比如可以给一组 Node 打上标签 `custom_topology`，那么拓扑域就是针对这个标签了，则该标签相同的 Node 上的 Pod 属于同一个拓扑域。\n\n\n## 注意事项\n原则上，topologyKey 可以是任何合法的标签 Key。但是出于性能和安全原因，对 topologyKey 有一些限制：\n\n1. 对于亲和性和 `requiredDuringSchedulingIgnoredDuringExecution` 的 Pod 反亲和性，topologyKey 不能为空。\n2. 对于 `requiredDuringSchedulingIgnoredDuringExecution` 的 Pod 反亲和性，引入 `LimitPodHardAntiAffinityTopology` 准入控制器来限制 topologyKey 只能是 `kubernetes.io/hostname`。如果要使用自定义拓扑域，则可以修改准入控制器，或者直接禁用它。\n3. 对于 `preferredDuringSchedulingIgnoredDuringExecution` 的 Pod 反亲和性，空的 topologyKey 表示所有拓扑域。截止 `v1.12` 版本，所有拓扑域还只能是 `kubernetes.io/hostname`、`failure-domain.beta.kubernetes.io/zone` 和 `failure-domain.beta.kubernetes.io/region` 的组合。\n4. 除上述情况外，topologyKey 可以是任何合法的标签 key。\n\n\n## 参考\n* [https://k8smeetup.github.io/docs/concepts/configuration/assign-pod-node/](https://k8smeetup.github.io/docs/concepts/configuration/assign-pod-node/)\n* [https://www.cnblogs.com/cocowool/p/kubernetes_affinity.html](https://www.cnblogs.com/cocowool/p/kubernetes_affinity.html)\n* [https://segmentfault.com/a/1190000018446833#articleHeader6](https://segmentfault.com/a/1190000018446833#articleHeader6)\n\n"
    },
    {
      "slug": "make-money",
      "title": "股票、债券、基金",
      "date": "2019-03-24",
      "author": null,
      "tags": [
        "生活"
      ],
      "content": "\n## 股票\n#### 什么是股票\n> 股票是股份公司发给股东证明其所持股份的一种有价证券，是持股人对公司拥有股权的凭证。\n\n也就是说，只要你买了哪家公司的股票，你就是那家公司的股东，就可以行使股东的权利，比如参与公司利润分红，有权出席股东大会，参与公司决策等。\n\n#### A股、B股、H股\n我国根据公司的上市地点和所面对的投资者将股票分为A股、B股和H股。\n* A股\n\n  A股的正式名称是人民币普通股票。它是由我国境内的公司发行，供国内的机构、组织或个人以人民币认购和交易的普通股股票。也就是说A股只能国内居民（不含港澳台同胞）通过人民币买卖，外国人不能买卖A股，我们通常所谈论的股票一般也是指A股。A股公司的注册地和上市地都在国内。\n  \n* B股\n   \n  B股的正式名称是人民币特种股票。它是以人民币标明面值，以外币认购和买卖，在国内上市交易的股票。2001年之前，B股只允许港澳台和外国人买卖，2001年之后开始开放国内居民买卖B股。B股公司的注册地和上市地也都在国内。\n  \n * H股\n   \n   H股也称港股，是指在国内注册在香港上市的股票，通过港币进行交易。那国内居民可以买卖H股吗？可以，不过比较麻烦，而且还有一些限制，不如A股方便，具体不细说了。\n   \n#### 什么是股市\n股票不能退股，就是你买了股票不能退，只能将股票转让或者卖给别人。股票转让、买卖和流通的场所就叫股市，只有上市公司的股票才可以在股市上进行交易，非上市公司的股票不能在股市上交易。\n\n目前中国大陆股市有沪市和深市，交易场所分别为`上海证券交易所`和`深圳证券交易所`。\n* 上海证券交易所\n\n  于1990年11月26日由中国人民银行总行批准成立，同年12月19日正式营业，其主要指数为`上证指数`。\n\t\n* 深圳证券交易所\n\t\n  于1989年11月15日筹建，1990年12月1日开始营业，其主要指数为`深证指数`。\n\n证券交易所除了股票交易，还做哪些事呢？其主要职责有：\n* 提供证券交易的场所和设施\n* 接受上市申请，安排证券上市\n* 组织、监督证券交易\n* 对上市公司进行监管\n* 管理和公布市场信息\n\n#### 什么是上证指数\n先说说指数，什么是指数？\n> 指数是一种对比性的统计指标，用于反映复杂现象总体数量上的变动\n\n上证指数全称`上海证券交易所股票价格综合指数`，用于反映上海证券交易所股票总体走势的统计指标，是以该市所有股票为样本，通过某种计算方式得出的。\n\n通常所说的股市大涨也就是指数大涨，我们可以通过指数的上涨或下跌来判断当前市场行情好坏。同样的还有深证指数，其他的指数分类还有上证50指数、沪深300指数、中证500指数等。\n\n#### 沪深两市交易规则\n* 交易时间\n\n\t周一至周五，上午9：30至11：30，下午13：00至15：00，法定节假日除外。\n\t\n* 交易规则\n\n\t`T+1`，表示当天买进一家公司的股票，不可当天卖出，只能在下一个交易日卖出。股票卖出后，当天不能提现，同样只能在下一个交易日提现。\n\t\n* 委托规则\n\n\t股票最小成交单位为手，1手=100股，假设同花顺公司当前股价100元，那么买入该公司的股票至少买1手，需要10000元，当然还需要少量手续费。\n\t\n* 涨跌幅限制\n\n\t股票涨跌幅限制为10%，当一只股票涨幅达到+10%，其当日股价就无法继续上涨，称之为`涨停`，股票涨停后，就比较难买进，因为买进的人很多。同样，当一只股票跌幅达到-10%，其当日股价就无法继续下跌，称之为`跌停`，股票跌停后，就比较难卖出了，因为卖出的人很多。不过，有两类股的涨跌幅限制不是10%，一是新股上市首日，其涨幅可以达到44%；二是ST股票，ST股票是指连续三年亏损的股票，股票名称前会加上ST两个字母，该类股有面临退市的风险，用于警示投资者，其当日涨跌幅为5%。\n\n#### 股票涨跌\n股票为什么会涨，为什么会跌，股票的涨跌由什么决定？\n\n这里先举个例子，假如我手上有一瓶珍藏的红酒，现在想卖掉，你想买红酒，愿意出500元的价格，于是500元就是这瓶红酒的成交价。假设这时候有第三个人也想买这瓶红酒，他觉得这瓶红酒有收藏价值，并且愿意出600元，那么我会以600元的价格卖给他，这时这瓶红酒的成交价就是600元，也就是说这瓶红酒的价格上涨了20%。\n\n再回到股市中，对于想买股票的人，肯定希望价格越低越好，对于想卖股票的人则希望价格越高越好，不同的人有不同的价格，所以股票有它的成交规则：`价格优先、时间优先`。现在假设同花顺的股价当前成交价是100元，这时有买方一以99元挂买单1手，买方二以100元挂买单1手，买方三以101元挂买单1手；有卖方一以101元挂卖单1手，卖方二以102元挂卖单1手，卖方三以103元挂卖单1手，卖方四以101元挂卖单1手。依据价格优先的规则，买方三的101元买单价格会排在前面优先处理，卖方一和卖方四的101元买入价格会排在前面，依据时间优先规则，卖方一比卖方四先挂单，卖方一的卖单会被优先处理，这时买方和卖方出价一样可以成交，所以此时股票的成交价是101元，即股价上涨了1%。如果这时有买方愿意以更高的价格买进，也有人以对应的价格卖出，那么买卖单就会一直被成交，股价就会一直上涨。事实上，每个时间点都有很多买方和卖方，只要有更多的买方以更高的价格买进股票，股价就会上涨，反之如果有更多的卖方以更低的价格卖出股票，股价就会下跌。现在明白了股票为什么会涨跌了吧。\n\n那么，为什么有人愿意花比别人更高的价格去买一件商品呢？因为他觉得值，大家都想要，价格自然就上去了。就是说，当人们看好某件商品的价值时，为了得到这件商品，他们就会以更高的价格买进，所以商品的价格就上涨了，其实这也就是市场供求关系导致的。\n\n#### 警告\n**炒股有风险，投资需谨慎。炒股是一种风险比较高的投资行为，且需要耗费较多时间和精力，对于个人而言，如果没有时间和精力去关注股票，那么购买基金是个不错的选择，不需要我们花时间去打理，且风险相对炒股来说较低。**\n\n## 债券\n#### 什么是债券\n> 债券是发行者为筹集资金发行的、在约定时间支付一定比例的利息，并在到期时偿还本金的一种有价证券。\n\n简单点说，就是借钱给别人，别人会支付利息给你，并到期后归还本金。债券持有者是债权人，发行者为债务人。\n\n根据发行方不同，债券主要分为以下几类：\n* 政府债券\n\n\t主要是国债和地方政府债券，就是我们把钱借给国家或地方政府。\n\t\n* 金融债券\n\n\t主要是银行和非银行机构发行的债券，就是我们把钱借给金融机构。\n\t\n* 企业债券\n\n\t主要是上市企业或非上市企业发行的债券，就是我们把钱借给企业。\n\n这三者中，政府债券因为有政府税收作为保障，因而风险最小，但收益也最小，企业债券风险最大，可能的收益也最大。债券不论何种形式，大都可以在市场上进行买卖，并因此形成了债券市场。\n\n#### 债券的收益计算\n债券发行时，其票面利率（收益率）是确定的，就是说债权人持有期内获得的利息也是确定。债券的利息是按天计息的，持有几天就算几天利息，此外，还有一部分收益来自债券价格的上涨，因为债券可以在市场上进行买卖，所以债券价格会出现波动，价格上涨会带来收益，当然，下跌也会带来损失。\n \n#### 债券的风险\n相对股票来说，债券的风险还是很小的，其主要风险有以下几种：\n* 违约风险\n\n\t因为购买债券的本质是把钱借出去，俗话说，欠债还钱，天经地义，但也会出现欠钱不还或者还不了的情况。在债券市场，我们称之为`违约`，如果债券发生违约，则可能无法收回本金，其中最大的违约风险来自企业债券。不过目前市场上企业债券的发行都受到了严格的监管，要发行企业债券都会经过严格的审核，以此来减小违约风险，所以相对来说，企业债券发生违约还是小概率事件。\n\t\n* 流动性风险\n\n\t债券市场不想股市那么活跃，债券的流动性较小，买卖债券的人很少，可能出现我想卖掉但是没人愿意买的情况。当然，如果准备持有债券到期，则不需要考虑流动性风险，因为到期后，我们获得了利息并可以收回本金。\n\t\n* 价格波动风险\n\n\t因为债券是可以在债券市场上进行交易的，所以债券价格和股票一样是实时变动的，虽然价格上涨会带来收益，但价格下跌也会带来损失，不过债券的价格也不像股票波动那么大。\n\n  \n## 基金\n#### 什么是基金\n说完股票和债券，再来重点说说基金。\n\n> 什么是基金呢？基金其实就是一种委托关系，就是把你的钱交给专业机构（基金管理公司）帮你管理和进行投资，同时他们也会收取少量手续费。基金管理公司在投资的时候，会进行分散投资，比如投多个股票或者股票、债券结合等，因此风险相对而言就比股票小。\n\n股票和债券都属于直接投资，什么叫直接投资？就是你的钱直接买了商品，而基金属于间接投资，就是你的钱不是直接买商品，而是交给专业人士去帮你买，但是你不能指定他去买什么，买什么由他自己决定，他也会告诉你买了哪些商品。\n  \n#### 基金的分类\n基金按投资标的分类，主要有以下几类：\n* 货币型基金\n\n\t资金主要投资于短期货币工具，如债券、央行票据、银行定期存单等有价证券。简单点理解就是，**货币基金是一种风险极低、收益低而且很灵活的基金**。货币基金可以随存随取，年化收益率一般在 2% ~ 3% 左右，有些货币基金有几个月甚至几年的封闭期，同时收益率也更高，一般在 4% ~ 5% 左右，余额宝就是一种货币基金。\n\t\n* 股票型基金\n\n\t大部分（80%以上）资金投资于股票的基金。因为上市的股票太多，一般的股票基金都会按行业再进行细分，比如白酒类基金，资金主要投资白酒类股票；券商类基金，资金主要投资券商类股票。**因为股票型基金主要投资于股票，所以其风险是各类基金中最高的，同时收益也是最高的**。\n\n* 债券型基金\n\n\t大部分（80%以上）资金投资于债券的基金，基金名称一般含有“债券”两个字，不同债券基金的风险和收益也不同。\n\n* 混合型基金\n\n\t资金投资于股票、债券、货币等，啥都买点，没有特定的投资对象。\n\n* 指数型基金\n\n\t资金主要投资于特定指数（如沪深300、上证50）的基金，其实我们并不能买股票指数，实际上买的是指数中的成分股。比如沪深300指数，就是选取沪深两市具有代表性的300只股票，通过某种计算方式，得出指标值；同样，上证50指数，就是选取沪市具有代表性的50只股票。那沪深300指数基金是买下所有300只股票吗？不是的，基金经理会从这300只股票种挑选几只股票买入，不同的沪深300指数基金，其买入的成分股也不大一样。这些指数基金的总体走势与对应指数的走势基本相当，因此，如果牛市即将到来钱，买入指数基金是肯定会赚钱，因为牛市指数肯定会上涨。相反，如果股市处于熊市的话，就最好不要买指数基金，一般都是稳亏不赚的，当然，如果你有资本买下整个熊市一直到牛市的到来，那也是可以的。\n\n以上几类基金的风险和收益大致顺序为：股票型基金 > 指数型基金 > 混合型基金 > 债券型基金 > 货币型基金。\n\n#### 基金如何计算收益\n\n在讲基金如何计算收益之前，先了解下什么是基金净值和单位基金净值。\n* 基金净值\n\n\t> 基金净值是在某一时点上，基金资产的总市值扣除负债后的余额，即`基金净值 = 总资产 - 总负债`。\n\t\n\t其中，总资产指基金拥有的所有资产，包括股票、债券、银行存款和其他有价证券等。总负债指基金运作及融资时所形成的负债，包括应付给他人的各项费用、应付资金利息等\n\t\n* 单位基金净值\n\n\t> 单位基金净值是计算收益的重要依据。`单位基金资产净值 = (总资产 - 总负债) / 基金单位总数`。\n\n\t其中，基金单位总数是指当时发行在外的基金单位的总量。因为基金所投资的股票、债券等市场价格每个交易日都是变动的，所以基金净值也是随之变动的。\n\n基金如何计算收益呢？来看个例子：\n\n申购：\n```\n用1万元申购某基金，申购费率是1.5％，该基金当日净值是1.2000。\n申购手续费＝10000.00×1.5％＝150.00 元\n申购份额＝(10000.00－150.00)÷1.2000＝8208.33 份\n```\n\n赎回：\n```\n1个月后赎回该基金，当日净值是1.4000，赎回费率是0.5％。\n赎回总额＝8208.33×1.4000＝11491.66 元\n赎回手续费＝11491.66×0.5％＝57.46 元\n赎回净额＝11491.66－57.46＝11434.20 元\n净收益＝11434.20－10000.00＝1434.20 元\n```\n\n通过这个例子，其实可以把购买基金类比为购买股票，单位基金净值就是股价，份额就是股数。简单来说，单位基金净值涨了，就赚钱，单位基金净值跌了，就亏钱。\n\n#### 如何买基金\n可以在支付宝、同花顺爱基金、天天基金网等各个平台上购买基金，推荐在支付宝上购买，手续费比较低，而且非常方便。\n  \n\n## 年化收益率\n> 年化收益率，就是投资一年获得的收益率。\n\n举个例子，假如我在2018年1月1日买入10000元的同花顺股票，于2018年12月31日卖出赚了1000元，那么我2018年买卖股票的年化收益率就是10%。实际上，通常我们所见到的年化收益率是把当前的收益率换算成年收益率后得出的，只是一种理论计算，并不是实际的收益率。比较常见的就是七日年化收益率，它是把最近七天的收益率，在接下来一年的每个七天都按照这种收益率计算得出的。例如余额宝当前的七日年化收益率为2.5%，如果我们现在存入10000元，那么理论上一年后的收益是250元。\n\n#### 72法则\n是指在固定的年化收益率上，`资产翻一番所需年数 = 72 / 年化收益率`。\n\n假设每年的投资年化收益率为8%，那么资产翻一番所需的年数为：72 / 8 = 9年。\n\n#### 复利的威力\n> 复利是世界第八大奇迹。      ——   爱因斯坦\n\n真的是爱因斯坦说的。\n\n啥是复利？其实就是我们所说的利滚利，把上一年的本金和利息作为下一年的本金来计算利息。其计算公式为：`Y=（1+X%）^n`\n\n如果你存入10000元，每年有10%的年化收益率，按照复利计算，那么\n* 10年后，将得到 10000 * (1+10%)^10 = 25937 元\n* 20年后，将得到 10000 * (1+10%)^20 = 67275 元\n* 30年后，将得到 10000 * (1+10%)^30 = 174494 元\n\n如果你每年存10000元，存30年，按照复利计算，那么\n* 若每年年化收益率为2%， 30年后将得到 405681 元\n* 若每年年化收益率为10%，30年后将得到 1644940 元\n* 若每年年化收益率为20%，30年后将得到 11818816 元\n\n这个是怎么算的，大家可以去算一下，我没算，这个是我从网上摘抄下来的。通过上面两个例子可以看出，复利真正的威力所在，在于时间长和年化收益率高。这里我想让大家懂得的事是，趁早投资，定期投资，即使年化收益率低，在几十年后也将是一笔不菲的回报。\n\n复利告诉我们一个道理，我们一定要投资自己，每天比别人努力一点，并坚持下去，在不久的将来，我们一定会收获巨大的回报。加油吧！\n\n## 写在最后\n我为什么要写这篇博客，因为我最近想买点指数基金和债券基金，但是我不太懂基金是什么，不知道基金是如何计算收益的，也不知道买哪只基金。于是我就想花点时间去研究一下，刚好下周一我们小组又轮到我分享了，于是就有了这篇博客，不过写点东西真的很费时啊，尤其对于我这种极其讨厌写作文的人，花了我周末两天时间，不过总算是写完啦。有些地方还没有深入了解，有时间再去研究吧。\n\n（完）\n"
    },
    {
      "slug": "understanding-jwt",
      "title": "理解JWT",
      "date": "2018-07-29",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n## 什么是JWT\n官方解释如下：\n> JWT全称JSON Web Token，是一种开放标准（RFC 7519），它定义了一种紧凑且独立的方式，可以在通信双方以JSON对象安全地传输信息。此信息可以通过数字签名进行验证和信任。JWT可以使用密钥（使用HMAC算法）或者使用RSA或ECDSA的公钥/私钥对进行签名。\n\n通俗点讲，JWT就是一种认证规范、标准。\n\n## JWT的组成\nJWT就是一个token，其结构是一个字符串，由三部分组成，以点号 `.` 分隔，通常像这样：\n```\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n```\n\n三个部分分别为：\n* Header（头部）\n* Payload（载体）\n* Signature（签名）\n\n#### Header\nHeader原数据是一个JSON对象，有`alg`和`typ`两个字段，`alg`表示生成JWT使用的散列算法，如`HMAC SHA256`、`RSA`等，默认是`HMAC SHA256`（简称`HS256`）；`typ`表示JWT的类型，其值一般就是JWT。\n```\n{\n  \"alg\": \"HS256\",\n  \"typ\": \"JWT\"\n}\n```\n然后将该JSON对象进行Base64URL编码成字符串。\n\n#### Payload\nPayload部分也是一个JSON对象，用来存放需要传递的数据，官方提供但非必需的字段有以下[七个](https://tools.ietf.org/html/rfc7519#section-4.1)：\n* iss (Issuer)：签发人\n* sub (Subject)：主题\n* aud (Audience)：受众\n* exp (Expiration Time)：过期时间\n* nbf (Not Before)：生效时间\n* iat (Issued At)：签发时间\n* jti (JWT ID)：编号\n\n可以发现官方声明的字段都只有三个字符，这是因为JWT意味着紧凑。\n\n我们也可以定义一些自己的私有字段：\n```\n{\n  \"sub\": \"1234567890\",\n  \"name\": \"John Doe\",\n  \"admin\": true\n}\n```\n然后将该JSON对象进行Base64URL编码成字符串。\n\n**注意：不要将敏感信息放在`Header`或`Payload`中，因为Base64URL编码后的字符串可以被解码，任何人都可以获取到其中包含的信息。**\n\n#### Signature\n要生成一个签名，我们必须先有以下三部分信息：\n* Base64URL编码后的`Header`\n* Base64URL编码后的`Payload`\n* 密钥secret\n\n然后将编码后的`Header`和编码后的`Payload`以 `.` 拼接成一个字符串，最后将这个字符串与密钥使用 `Header` 中定义的加密算法进行加密，生成的字符串就是签名，算法如下：\n```\nHMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secret)\n```\n\n最后将`Header`、`Payload`、`Signature`三部分以 `.` 拼接，获得的字符串就是JWT，也就是通常所说的token。\n\n**注意：上面Header和Payload使用的编码都是Base64URL，而不是Base64。因为JWT有时会以参数的形式放在URL中，如http://GaryFeng.com/?token=xxx ，而Base64编码后的字符串有三个字符`+`、`/`、`=`，这三个字符在URL中有特殊含义，可能会导致一些问题。而Base64URL编码后的字符串将这三个字符分别使用`-`、`_`、`空格`进行了替换，使得token可以作为URL的参数。**\n\n## JWT工作原理\nJWT是如何验证客户端传过来的token的？\n\n服务端收到客户端传过来的token之后，会对token以 `.` 进行分隔，获得`Header`、`Payload`、`Signature`三部分，再解码`Header`获得其中的签名算法`alg`，然后使用`Header`、`Payload`和`secret`生成新的签名，最后比较新的签名和原始签名是否一致，若不一致则表示token无效。若用户篡改了token的`Payload`信息，则生成的新的签名和token中包含的原始签名肯定不一致，也就无法通过认证。\n\n可以看到，认证过程最终比较的是签名，由于生成签名使用的算法是不可逆的，且用户不知道`secret`，所以无法篡改签名。但如果用户知道了服务端存储的`secret`，就可以任意更改token了，这就相当于用户自己给自己签名了，因此一定要注意不能泄露`secret`。\n\n## JWT如何使用\n用户登录成功后，服务端会返回一个token，用户可以将这个token存储在Cookie或localStorage。每当用户与服务端通信，访问需要授权的资源时，都要传递这个token，传递方式一般是以`Authorization`字段放在HTTP请求头中，并带上Bearer标注：\n```\nAuthorization: Bearer <token> \n```\n\n当然也可以直接在URL中以参数的形式传递。\n```\nhttp://GaryFeng.com/?token=xxx\n```\n\n注意：不建议将token以Cookie的形式传递给服务端，因为这会存在跨域问题，也可能会有CSRF攻击的风险，而放在请求头中就不会有这样的问题。\n\n## JWT vs. Session\n#### Session\n一般session认证过程：用户登录成功后，服务端会生成sessionID并存储，同时在客户端以cookie的形式存储，然后客户端每次请求都会带上这个cookie，服务端再去通过session做校验和认证。\n\n这种方式使服务端必须把sessionID存储在内存或数据库中，可不管存在哪里都有缺点。若存储在内存中，对于分布式应用则需要多台服务器之间同步session；若存储在数据库中，则每次请求都要去查一次数据库。\n* 优点\n  * 可以主动清除session信息\n* 缺点\n  * 占用更多内存\n\n#### JWT\n相比session，JWT是无状态的，token存储在客户端，服务端只保存密钥secret，不存储任何session信息。那么，服务端是无法清除token让用户退出登录的，只能等待token过期。\n\n* 优点\n  * 服务端不用存储session信息，节省内存\n  * 解决跨域问题，防CSRF攻击（token通过请求头传递）\n* 缺点\n  * 服务端无法主动清除token，只能等待token过期\n  * 无法保证实效性，若token中存储了用户角色信息，而服务后台修改了该用户的角色，在该token过期之前，用户的角色不会变更。\n\n**针对服务端无法主动清除token的问题，查阅了一些文章，解决办法是把token存到Redis或其他数据库，当需要时再去清除或更新token。个人觉得使用JWT，服务端不应该存储token，保证其无状态特性。如果服务端存储了token，那和session又有什么区别？**\n\n## JWT使用场景\nJWT适用于具有时效性的一次性授权token的设计。如：\n* 邮箱验证。\n* restful api的鉴权。用户一旦登录，其每个后续请求都要包含token，来访问需要授权的路由、服务和资源。\n\n是否适用于会话管理？\n> 网上有人认为不适合会话管理，认为使用传统的session + cookie方案更好。也有人认为可以做会话管理，认为不适合会话管理的问题都可以解决。就我个人观点来看，我倾向于不适合做会话管理，认为服务端不应该存储token。\n\n## 使用JWT注意事项\n* 不要在Payload中存储敏感信息\n* 不要泄露secret\n* 尽量使用HTTPS\n因为token一旦被其他人获取，则他们可以冒充我们向服务器发起任意请求了。比如在HTTP请求中，我们发送token给服务端时，该token可能会被人抓包获取。针对这个问题，也没有什么解决办法，一般建议使用HTTPS而非HTTP。虽然这样不能保证token一定不会被人获取，但可以更加安全。\n\n（完）\n"
    },
    {
      "slug": "ioutil-readall-read-stdout-blocked-in-golang",
      "title": "Go的ioutil.ReadAll()读取标准输出的问题",
      "date": "2018-05-20",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n## 问题描述\n之前在使用`ioutil.ReadAll()`读取`cmd.CombinedOutput()`的标准输出时遇到程序挂起的问题，代码如下：\n```\nfunc main() {\n    cmd := exec.Command(\"git\", \"clone\",\"https://github.com/test/test.git\")\n    stdout, _ := cmd.StdoutPipe()\n    stderr, _ := cmd.StderrPipe()\n    if err := cmd.Run(); err != nil {\n        fmt.Printf(\"run error:%s\\n\",err)\n        return\n    }\n    stderrBytes, _ := ioutil.ReadAll(stderr)\n    stdoutBytes, _ := ioutil.ReadAll(stdout)\n    if len(stderrBytes) > 0 {\n        fmt.Printf(\"stderr:%s\\n\", stderrBytes)\n        return\n    }\n    fmt.Printf(\"stdout:%s\\n\", stdoutBytes)\n}\n```\n\n执行代码后发现程序挂起了，并没有结果打印出来。开始调试、换用`cmd`的其他方法，代码如下：\n```\nfunc main() {\n    cmd := exec.Command(\"git\", \"clone\",\"https://github.com/test/test.git\")\n    stdout, _ := cmd.StdoutPipe()\n    stderr, _ := cmd.StderrPipe()\n    if err := cmd.Start(); err != nil {\n        fmt.Println(err)\n        return\n    }\n    if err := cmd.Wait(); err != nil {\n        fmt.Println(err)\n        return\n    }\n    stderrBytes, _ := ioutil.ReadAll(stderr)\n    stdoutBytes, _ := ioutil.ReadAll(stdout)\n    if len(stderrBytes) > 0 {\n        fmt.Printf(\"stderr:%s\\n\", stderrBytes)\n        return\n    }\n    fmt.Printf(\"stdout:%s\\n\", stdoutBytes)\n}\n```\n\n一顿操作之后，问题仍然没有解决，于是开启Google+Stack Overflow模式，最终找到`golang`项目的一个issue。\n\n## 解决方案\n这个issue描述的和我遇到的问题一样，有人给出了解决方案，截取内容如下：\n> This is unfortunately just how Unix pipes work. You need to read from both pipes at the same time. What's happening is that cat is trying to write to stdout, but its attempt to write is blocked because the stdout buffer is full. You're trying to ReadAll from stderr, but stderr won't be closed until cat exits, which won't happen until it finishes writing to stdout. So, deadlock.  \n>\n> This is why Command provides Output and CombinedOutput methods; they are careful to always read from both pipes at once. If you want both stdout and stderr but not in the same byte slice, you can also do what CombinedOutput does under the covers and assign separate bytes.Buffer to Stdout and Stderr. Or you can just use Goroutines to read from both at once.\n\n意思就是说`cat`指令尝试往`stdout`里面写数据，但因为stdout buffer满了导致这个写操作被堵住了，这时`ReadAll`函数尝试从`stderr`读数据，但`stderr`只有在`cat`指令退出才会被关闭，而`cat`指令只有往`stdout`里写完了才会退出，so，最终导致死锁了。原文地址：[https://github.com/golang/go/issues/16787](https://github.com/golang/go/issues/16787)\n\n解决方法如下：\n```\nfunc main() {\n    cmd := exec.Command(\"git\", \"clone\",\"https://github.com/test/test.git\")\n    var stdout, stderr bytes.Buffer\n    cmd.Stdout = &stdout\n    cmd.Stderr = &stderr\n    if err := cmd.Run(); err != nil {\n        fmt.Printf(\"stderr:%s\\n\", stderr.Bytes())\n        return\n    }\n    fmt.Printf(\"stdout:%s\\n\", stdout.Bytes())\n}\n\n```\n\n若不需要单独获取stdout和stderr，使用`CombinedOutput`或`Output`即可，如下：\n```\nfunc main() {\n    cmd := exec.Command(\"git\", \"clone\",\"https://github.com/test/test.git\")\n    stdoutStderr, err := cmd.CombinedOutput()\n    // stdoutStderr, err := cmd.Output()\n    if err != nil {\n        fmt.Printf(\"stderr:%s\\n\", stdoutStderr)\n        return\n    }\n    fmt.Printf(\"stdout:%s\\n\", stdoutStderr)\n}\n\n```\n\n\n## 总结\n遇到问题善用该项目GitHub的issue，到issue里搜一搜，绝大多数问题都能找到答案。\n"
    },
    {
      "slug": "how-to-use-libgit2-git2go-in-golang",
      "title": "Go如何使用libgit2/git2go",
      "date": "2018-05-14",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n`libgit2`是基于C语言实现的操作git的api，`git2go`是golang用来调用其api的包，GitHub地址[https://github.com/libgit2/git2go](https://github.com/libgit2/git2go)，目前也实现了多个语言版本，详见[https://libgit2.github.com](https://libgit2.github.com)\n\n## 环境需求\n需要C编译的环境，请确保已安装`cmake`\n`yum install cmake -y`\n\n## 使用\ngo get 下载包文件，这个过程比较耗时，需要好几分钟......\n```\ngo get github.com/libgit2/git2go\n```\n\n下载完成后可能会出现以下报错，即使`yum install libgit2`安装了`libgit2`后重新`go get`仍然会报错，查过其他文章，说是正常情况，呵呵，so，直接忽略吧\n```\n# pkg-config --cflags libgit2\nPackage libgit2 was not found in the pkg-config search path.\nPerhaps you should add the directory containing `libgit2.pc'\nto the PKG_CONFIG_PATH environment variable\nNo package 'libgit2' found\npkg-config: exit status 1\n```\n\n开始进行编译\n```\ncd $GOPATH/src/github.com/libgit2/git2go\ngit checkout next\ngit submodule update --init\nmake install\n```\n\n## 包管理\n因为需要对`git2go`进行编译，所以不能像其他的第三方包`godep save`到vendor目录就完事了。建议将`go get`下来的代码执行执行`git submodule update --init`后拷贝到vendor目录下，不要执行`make install`操作，然后源码目录放一个shell脚本用来做编译操作。这样做的好处是其他同事加入该项目开发时，不需要重新`go get`和执行git操作，因为真的太耗时了，如下\n```\ncd $GOPATH/src/github.com/libgit2/git2go\ngit checkout next\ngit submodule update --init\nrm -rf .git*\ncp $GOPATH/src/github.com/libgit2/git2go $GOPATH/src/myProject/vendor/github.com/libgit2/git2go\n```\n\n编译脚本init.sh\n```\n#!/bin/bash\nif [ -z $GOPATH ]\nthen\n    echo -e \"\\$GOPATH is null\\n\"\n    exit 1\nfi\n\ncd $GOPATH/src/myProject/vendor/github.com/libgit2/git2go\necho -e \"compiling libgit2...\\n\"\nmake install\n\necho -e \"compile completed!\\n\"\nexit 0\n```\n\n最后上传到代码管理仓库。\n\n## 参考\n* [http://www.petethompson.net/blog/golang/2015/10/04/getting-going-with-git2go/](http://www.petethompson.net/blog/golang/2015/10/04/getting-going-with-git2go/)\n* [https://github.com/odewahn/git2go-test](https://github.com/odewahn/git2go-test)\n\n（完）\n"
    },
    {
      "slug": "deploy-ingress-in-kubernetes",
      "title": "Kubernetes部署Ingress",
      "date": "2018-02-10",
      "author": null,
      "tags": [
        "k8s",
        "docker"
      ],
      "content": "\n## 前言\nKubernetes暴露服务的方式有多种，如LoadBalancer、NodePort、Ingress等。LoadBalancer一般用于云平台，平常一般用NodePort暴露服务，非常方便。但是由于NodePort需要指定宿主机端口，一旦服务多起来，多个端口就难以管理。那么，这种情况下，使用Ingress暴露服务更加合适。\n\n## Ingress组成\nIngress一般包含三个组件：\n* **反向代理负载均衡器**  \n比如Nginx、Apache等。\n\n\n* **Ingress Controller**  \n目前官方最新版本的镜像ingress-controller:0.10.2已经集成了Nginx作为负载均衡，所以现在Ingress一般指Ingress Controller和Ingress资源两个组件。Ingress Controller负责实时监听Ingress资源，一旦Ingress发生更改，Controller会立即根据Nginx模板文件更新Nginx的配置，模板文件可以在[这里](https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/template/nginx.tmpl)找到，也可以根据自己的需求定制ingress-controller镜像。一个集群内可以部署多个Ingress Controller。\n\n\n* **Ingress**  \n通俗点讲，就是用来定义转发规则的，如下是一个很简单的ingress.yml配置：\n\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        backend:\n           serviceName: test\n           servicePort: 80\n```\n\n若需要添加新的转发规则，只需修改上述文件，然后执行`kubectl apply -f ingress.yml`即可，或者执行`kubectl edit`直接编辑后保存，通过`kubectl logs`可以看到ingress-controller的Nginx配置是否更新成功。Ingress可以和Ingress Controller不在同一namespace，但必须与声明的服务在同一namespace。同样，一个集群内也可以部署多个Ingress，一个Controller可以匹配多个Ingress。\n\n## 部署\n部署一些必要的服务：\n```\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml \\\n    | kubectl apply -f -\n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml \\\n    | kubectl apply -f -\n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml \\\n    | kubectl apply -f -\n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml \\\n    | kubectl apply -f -\n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml \\\n    | kubectl apply -f -\n```\n上面的default-backend.yml用于部署默认服务，当ingress找不到相应的请求时会返回默认服务，官方的默认服务返回404，也可以定制自己的默认服务。\n\n基于RBAC部署Ingress Controller：\n```\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml \\\n    | kubectl apply -f -\n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml \\\n    | kubectl apply -f -\n```\n\n也可以基于非RBAC模式部署：\n```\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/without-rbac.yaml \\\n    | kubectl apply -f -\n```\n\n部署Ingress，假设集群内已经存在一个test服务，创建ingress.yml声明的规则如下：\n```\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-nginx\nspec:\n  rules:\n  - host: GaryFeng.com\n    http:\n      paths:\n      - backend:\n           serviceName: test\n           servicePort: 80\n```\n\n至此，ingress就部署完成了。配置hosts到Controller的PodIP，然后集群外访问GaryFeng.com就可以访问test服务了。**注意：因为官方的Ingress Controller默认并没有开启`hostNetwork`模式，所以这里hosts配置的是Controller的PodIP。但是考虑到Pod重新调度后其IP会更改，那么hosts配置也要同时更改，所以一般建议开启`hostNetwork`模式，使Controller监听宿主机的端口，这样配置hosts时只需要配置Pod所在的节点IP即可。有人会说，如果Pod重新调度到其他节点了，hosts配置不是也要改变吗？不错，这种情况下，我们可以通过nodeSelector指定Ingress Controller调度到某个节点。这样hosts配置就不用变了。**修改如下：\n```\n...\nnodeSelector:                   # 指定Ingress Controller调度到某个节点\n  nodeName: myNodeName\nhostNetwork: true               # 开启hostNetwork模式\ncontainers:\n  - name: nginx-ingress-controller\n    image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.2\n    args:\n      - /nginx-ingress-controller\n      - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n      - --configmap=$(POD_NAMESPACE)/nginx-configuration\n      - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n      - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n      - --annotations-prefix=nginx.ingress.kubernetes.io\n...\n```\n建议将上述yaml文件下载到本地使用`kubectl create`部署，可以根据需求做相应更改。\n\n## Ingress Controller匹配Ingress\n当集群内创建多个Controller时，如何使某个Controller只监听对应的Ingress呢？这里就需要在Ingress中指定[annotations](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/annotations.md)，如下：\n```\nmetadata:\n  name: nginx-ingress      \n  namespace: ingress-nginx      \n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"                  # 指定ingress.class为nginx\n```\n然后在Controller中指定参数`--ingress-class=nginx`：\n```\nargs:\n  - /nginx-ingress-controller\n  - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n  - --configmap=$(POD_NAMESPACE)/nginx-configuration\n  - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n  - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n  - --annotations-prefix=nginx.ingress.kubernetes.io\n  - --ingress-class=nginx                                 # 指定ingress-class值为nginx，与对应的Ingress匹配\n```\n这样，该Controller就只监听带有`kubernetes.io/ingress.class: \"nginx\"`annotations的Ingress了。我们可以声明多个带有相同annotations的Ingress，它们都会被对应Controller监听。Controller中的nginx默认监听80和443端口，若要更改可以通过`--http-port`和`--https-port`参数来指定，更多参数可以在[这里](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/cli-arguments.md)找到。\n\n在实际应用场景，常常会把多个服务部署在不同的namespace，来达到隔离服务的目的，比如A服务部署在namespace-A，B服务部署在namespace-B。这种情况下，就需要声明Ingress-A、Ingress-B两个Ingress分别用于暴露A服务和B服务，且Ingress-A必须处于namespace-A，Ingress-B必须处于namespace-B。否则Controller无法正确解析Ingress的规则。\n\n## 总结\n* 集群内可以声明多个Ingress和多个Ingress Controller\n* 一个Ingress Controller可以监听多个Ingress\n* Ingress和其定义的服务必须处于同一namespace\n\n参考：  \n[https://github.com/kubernetes/ingress-nginx/blob/master/README.md](https://github.com/kubernetes/ingress-nginx/blob/master/README.md)  \n[https://mritd.me/2017/03/04/how-to-use-nginx-ingress/](https://mritd.me/2017/03/04/how-to-use-nginx-ingress/)  \n[https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)  \n\n（完）\n\n"
    },
    {
      "slug": "annual-summary-and-plan",
      "title": "2017总结2018计划",
      "date": "2018-02-09",
      "author": null,
      "tags": [
        "生活"
      ],
      "content": "\n\n又到年底，回顾2017年初定下的[目标](http://GaryFeng.com/2017/02/12/annual-plan/)，咳咳咳....\n\n## 2017\n2017，定了一堆目标，想当初定目标时信心满满，肯定能全部完成，然而事实证明，理想很丰满，现实啪啪啪。\n\n#### 技术\n去年的工作内容主要在一些Go项目和容器技术上，技术栈是Go+Docker+Kubernetes，这三样基本是现学现卖，一边学习一边实践，做了几个go项目，帮助部门定制了一套开发流程并部署了开发环境，主要是基于Gitlab+Docker，效果还算满意。自从工作重心转到容器技术上之后，PHP代码也没怎么写了，不过现在更喜欢写Go的项目，因为写Go更能给我带来成就感。\n\n博客：去年写了15篇博客，主要是go和容器技术相关的文章，我从小写作能力就差，一篇文章能写几个小时，组织语言得想半天，写博客一方面是想提升自己的写作能力，一方面是对自己所学的知识做一个整理、总结。\n\n#### 生活\n摄影：买了台单反，拍了点照片，学了点后期，但修的图仍然不太满意，有段时间感觉自己花在后期上的时间有点多了，很是纠结，不知该如何分配花在摄影与代码上的时间。仔细权衡了一下，还是以工作为重，毕竟摄影只是一门兴趣爱好，除了放假，其他时间一概不碰。\n\n健身：坚持了三四个月之后就没继续了，具体什么原因没坚持下去我也不知道，好像是因为工作经常加班，慢慢的就放弃了。\n\n理财：就是炒股啦，炒了大半年，亏了大几千，怪自己过于贪婪。\n\n#### 看过的书\n去年也看了点书，还是太少了\n* 《皮囊》\n* 《乌合之众》\n* 《人性的弱点》\n* 《岛上书店》\n* 《追风筝的人》\n* 《灿烂千阳》\n* 《贫穷的本质》\n* 《Docker进阶与实践》\n* 《RESTful Web APIs》\n* 《沉默的大多数》  未看完\n\n另外，有些目标仍然没有完成，设计模式、消息队列这些没怎么看，微信小程序没写，技术书籍没看完，健身没有坚持下去。总结下来，主要问题还是在于自己不够自律，有些事情明明有时间可以去做，却是一拖再拖。最重要的，女盆友仍然是个谜:joy:。\n\n## 2018\n2018，务实一点。\n\n#### 目标\n* 看30本书并做笔记\n* 写20篇技术博客\n* 学习摄影后期，拍3000张照片，精修50张照片\n* 去两个城市旅游\n* 找女朋友:yum:\n\n愿自己的能力可以早日满足自己的欲望。\n\n（完）\n"
    },
    {
      "slug": "kubernetes-uses-the-security-context-and-sysctl",
      "title": "Kubernetes的SecurityContext和Sysctl",
      "date": "2017-12-23",
      "author": null,
      "tags": [
        "k8s",
        "docker"
      ],
      "content": "\n## 前言\n在运行一个容器时，有时候需要使用`sysctl`修改内核参数，比如`net.`、`vm.`、`kernel`等，`sysctl`需要容器拥有超级权限，容器启动时加上`--privileged`参数即可。那么，在kubernetes中是如何使用的呢？\n\n## Security Context\nkubernetes中有个字段叫`securityContext`，即`安全上下文`，它用于定义Pod或Container的权限和访问控制设置。其设置包括：\n\n* **Discretionary Access Control: 根据用户ID（UID）和组ID（GID）来限制其访问资源（如：文件）的权限**\n\n针对pod设置：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    runAsUser: 1000\n    fsGroup: 2000\n  volumes:\n  - name: sec-ctx-vol\n    emptyDir: {}\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    volumeMounts:\n    - name: sec-ctx-vol\n      mountPath: /data/demo\n    securityContext:\n      allowPrivilegeEscalation: false\n```\n\n针对container设置：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: sec-ctx-demo-2\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      runAsUser: 2000\n      allowPrivilegeEscalation: false\n```\n\n* **Security Enhanced Linux (SELinux): 给容器指定SELinux labels**\n\n```\n...\nsecurityContext:\n  seLinuxOptions:\n    level: \"s0:c123,c456\"\n```\n\n* **Running as privileged or unprivileged：以`privileged`或`unprivileged`权限运行**\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-4\nspec:\n  containers:\n  - name: sec-ctx-4\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      privileged: true\n```\n\n* **Linux Capabilities: 给某个特定的进程privileged权限，而不用给root用户所有的`privileged`权限**\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-4\nspec:\n  containers:\n  - name: sec-ctx-4\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add: [\"NET_ADMIN\", \"SYS_TIME\"]\n```\n\n* **AppArmor: 使用程序文件来限制单个程序的权限**\n\n* **Seccomp: 限制一个进程访问文件描述符的权限**\n\n* **AllowPrivilegeEscalation: 控制一个进程是否能比其父进程获取更多的权限，`AllowPrivilegeEscalation`的值是bool值，如果一个容器以privileged权限运行或具有`CAP_SYS_ADMIN`权限，则`AllowPrivilegeEscalation`的值将总是true。**\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo-2\nspec:\n  securityContext:\n    runAsUser: 1000\n  containers:\n  - name: sec-ctx-demo-2\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      runAsUser: 2000\n      allowPrivilegeEscalation: false\n```\n\n**注意：要开启容器的privileged权限，需要提前在`kube-apiserver`和`kubelet`启动时添加参数`--allow-privileged=true`，默认已添加。**\n\t\t\n## 使用sysctl\n`sysctl -a`可以获取sysctl所有参数列表。\n\n从v1.4开始，kubernetes将sysctl分为`safe`和`unsafe`，其对safe的sysctl定义如下：\n\n* 不会影响该节点的其他pod\n* 不会影响节点的正常运行\n* 不会获取超出`resource limits`范围的CPU和内存资源\n\n目前属于`safe sysctl`的有：\n\n* kernel.shm_rmid_forced\n* net.ipv4.ip_local_port_range\n* net.ipv4.tcp_syncookies\n\n其余的都是`unsafe sysctl`，当kubelet支持更好的隔离机制时，`safe sysctl`列表将在未来的Kubernetes版本中扩展。\n\n使用`safe sysctl`例子:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sysctl-example\n  annotations:\n    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=1\nspec:\n  ...\n```\n\n而使用`unsafe sysctl`时，需要在kubelet的启动参数中指定`--experimental-allowed-unsafe-sysctls`，如`--experimental-allowed-unsafe-sysctls=net.core.somaxconn`，具体操作如下:\n\n编辑kubelet配置文件，修改`ExecStart=/usr/bin/kubelet`那一行，在后面加上`--experimental-allowed-unsafe-sysctls=net.core.somaxconn`，如：\n```\nExecStart=/usr/bin/kubelet --experimental-allowed-unsafe-sysctls=net.core.somaxconn\n```\n\n因为我是用kubeadm安装的kubernetes，所以在`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`文件中加了倒数第3行内容：\n```\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true\"\nEnvironment=\"KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin\"\nEnvironment=\"KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local\"\nEnvironment=\"KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt\"\nEnvironment=\"KUBELET_CADVISOR_ARGS=--cadvisor-port=0\"\nEnvironment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd\"\nEnvironment=\"KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki\"\nEnvironment=\"KUBELET_EXTRA_ARGS=--experimental-allowed-unsafe-sysctls=net.core.somaxconn\"\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS\n```\n\n重启kubelet：\n```\nsystemctl daemon-reload\nsystemctl restart kubelet\n```\n\n在Pod中使用`unsafe sysctl`，开启privileged权限：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sysctl-example\n  annotations:\n    security.alpha.kubernetes.io/unsafe-sysctls: net.core.somaxconn=65535                 #使用unsafe sysctl，设置最大连接数\nspec:\n  securityContext:\n    privileged: true                                                                      #开启privileged权限\n  ...\n```\n\n## 总结\n线上环境请谨慎使用`privileged`权限，使用不慎可能导致整个容器崩掉，相关信息可自行查阅。\n\n参考：  \n[https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/](https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/)  \n[https://kubernetes.io/docs/tasks/configure-pod-container/security-context/](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)  \n"
    },
    {
      "slug": "the-management-of-resource-in-kubernetes",
      "title": "Kubernetes的资源管理",
      "date": "2017-12-04",
      "author": null,
      "tags": [
        "k8s",
        "docker"
      ],
      "content": "\n## 什么是资源\n在Kubernetes中，资源指的是可以被pod或容器“请求”,“分配”,“消费”的那些东西。例如：CPU，内存，硬盘。资源又分为可压缩资源和不可压缩资源，目前CPU是唯一支持的可压缩资源，内存和硬盘是不可压缩资源。对于CPU这种可压缩资源，如果pod中服务使用CPU超过限额，该pod不会被kill掉但会被限制，而对于内存这种不可压缩资源，如果pod中服务使用内存超过限额，该pod中容器的进程会因OOM（Out of Memory）被kernel kill掉。\n\n## 资源限制\n如果未做过节点nodeSelector、亲和性（node affinity）或pod亲和、反亲和性（pod affinity/anti-affinity）等[Pod高级调度策略设置](http://dockone.io/article/2635)，我们无法指定pod部署到指定机器上，这可能会造成CPU或内存等密集型的pod同时分配到相同节点，造成资源竞争。另一方面，如果未对资源进行限制，一些关键的服务可能会因为资源竞争因OOM等原因被kill掉，或者被限制使用CPU。\n\n#### CPU、内存资源\n在部署pod时，可以指定每个容器的资源请求和资源限额。分别由requests和limits控制：\n\n* requests：资源请求，表示需要多少资源。\n* limits：  资源限制，表示最多可以使用多少资源。\n\n**注：requests设置范围是0到节点最大配置，即0 <= request <= Node Allocatable，而limits设置范围是requests到无穷大，即requests <= limits <= Infinity。**\n\n当给一个容器指定了`resource requests`时，调度器可以更好地决定将pod放在哪个node上，目前容器仅支持CPU和内存资源的requests和limits。\n\n###### 内存溢出示例\n下面是一个测试内存溢出的示例，启动一个可以不断申请内存的应用，测试一个容器使用内存超过限额后，kubernetes将如何处理。Deployment配置如下：\n```yaml\napiVersion: extensions/v1beta1 \nkind: Deployment               \nmetadata:\n  name: test-oom               \nspec:\n  replicas: 3                  \n  template:\n    metadata:\n      labels:                  \n        app: test-oom          \n    spec:                      \n      containers:              \n        - name: test-oom       \n          image: test-oom\n          resources:           \n            requests:          \n              memory: 60Mi     \n              cpu: 1\n            limits:\n              memory: 200Mi\n              cpu: 2\n```\n该配置表示应用有3个pod，每个容器请求60MB内存、1颗CPU，限额1GB内存、2颗CPU\n\n创建deployment：\n```yaml\n[root@docker22 kubernetes]# kubectl create -f test-oom.yml \ndeployment \"test-oom\" created\n```\n\n查看各个pod的状态：\n```\n[root@docker22 kubernetes]# kubectl get pod -o wide\nNAME                        READY     STATUS    RESTARTS   AGE       IP             NODE\ntest-oom-85b67d8699-mwwbt   1/1       Running   0          6s        10.244.2.120   docker24\ntest-oom-85b67d8699-nzjw2   1/1       Running   0          6s        10.244.0.134   docker22\ntest-oom-85b67d8699-tsdcp   1/1       Running   0          6s        10.244.1.88    docker23\n```\n\n打开一个新的终端窗口A，随便进入一台节点服务器，比如docker22，先找到test-oom容器id，通过docker stats查看该节点的test-oom容器的资源占用情况，发现一切正常。\n```\n[root@docker22 kubernetes]# docker ps|grep test-oom\n69f66c496f69        test-oom@sha256:5ac78c8c3ee39798a75507608f9815892b43e73051fb9580210471b1b624a242                       \"sh -c /tmp/test-oom\"    2 minutes ago       Up 2 minutes                               k8s_test-oom_test-oom-85b67d8699-nzjw2_default_1dac89e0-dcaa-11e7-8280-001517872530_0\n75f63eff0507        gcr.io/google_containers/pause-amd64:3.0                                                                                         \"/pause\"                 2 minutes ago       Up 2 minutes \n[root@docker22 kubernetes]# docker stats|grep 69f66c496f69\n69f66c496f69        0.00%               1.406 MiB / 200 MiB     0.70%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.406 MiB / 200 MiB     0.70%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.406 MiB / 200 MiB     0.70%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.406 MiB / 200 MiB     0.70%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.406 MiB / 200 MiB     0.70%               0 B / 0 B           0 B / 0 B           6\n```\n\n然后在原来的终端执行curl http://10.244.0.87:8080/kill（该接口是一个不断申请内存的死循环程序），并返回终端A再次查看资源占用情况，会发现内存占用在飙升，而且docker stats很快就停止刷新了。结果如下：\n```\n[root@docker22 kubernetes]# docker stats|grep 69f66c496f69\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        0.00%               1.414 MiB / 200 MiB     0.71%               0 B / 0 B           0 B / 0 B           6\n69f66c496f69        71.59%              26.91 MiB / 200 MiB     13.45%              0 B / 0 B           242 kB / 0 B        11\n69f66c496f69        71.59%              26.91 MiB / 200 MiB     13.45%              0 B / 0 B           242 kB / 0 B        11\n69f66c496f69        243.48%             83.75 MiB / 200 MiB     41.88%              0 B / 0 B           242 kB / 0 B        12\n69f66c496f69        243.48%             83.75 MiB / 200 MiB     41.88%              0 B / 0 B           242 kB / 0 B        12\n69f66c496f69        131.74%             137.5 MiB / 200 MiB     68.75%              0 B / 0 B           242 kB / 0 B        12\n69f66c496f69        131.74%             137.5 MiB / 200 MiB     68.75%              0 B / 0 B           242 kB / 0 B        12\n```\n\n这是因为该pod被kill掉了，然后pod重启，所以容器也重启了。查看pod状态，发现该pod的RESTART变为1了，说明该pod重启过一次，如下：\n```\n[root@docker22 kubernetes]# kubectl get pod -o wide\nNAME                        READY     STATUS    RESTARTS   AGE       IP             NODE\ntest-oom-85b67d8699-mwwbt   1/1       Running   0          8m        10.244.2.120   docker24\ntest-oom-85b67d8699-nzjw2   1/1       Running   1          8m        10.244.0.134   docker22\ntest-oom-85b67d8699-tsdcp   1/1       Running   0          8m        10.244.1.88    docker23\n```\n\n#### 硬盘资源\n###### emptyDir\n卷，该类型的挂载卷可以使用宿主机全部的硬盘空间，要注意的是，emptyDir类型的挂载卷生命周期持续到Pod终止，即使Pod内的容器重启或终止，只要Pod存活，该挂载卷也会一直存在。只有当Pod终止时，挂载卷的数据才会被清除。\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /cache\n      name: cache-volume\n  volumes:\n  - name: cache-volume\n    emptyDir: {}\n```\n\n###### PersistentVolume\n持久卷，缩写为PV，是集群中的一块存储，跟Node一样，也是集群的资源。PV跟Volume(卷)类似，不过它有独立于Pod的生命周期，该类型挂载卷是永久存在于宿主机的。\n```yaml\nkind: PersistentVolume         \napiVersion: v1                 \nmetadata:\n  name: task-pv-volume         \n  labels:                      \n    type: local                \nspec:\n  storageClassName: manual     \n  capacity:\n    storage: 10Gi              \n  accessModes:                 \n    - ReadWriteOnce            \n  hostPath:\n    path: \"/tmp/data\"  \n```\n\n上面定义了一个PersistentVolume资源，挂载宿主机的/tmp/data目录，容量为10GB，访问模式为读写，其中定义的storageClassName=manual用于将PersistentVolumeClaim的requests绑定到这个PV。\n其中，accessModes可选三种访问模式：\n\n* ReadWriteOnce：该卷能够以读写模式被加载到一个节点上。\n* ReadOnlyMany：该卷能够以只读模式加载到多个节点上。\n* ReadWriteMany：该卷能够以读写模式被多个节点同时加载。\n\t\n###### PersistentVolumeClaim\n持久卷申请，缩写为PVC，是用户对PV的一个请求，跟Pod类似。Pod消费Node的资源，PVC消费PV的资源。Pod 能够申请特定的资源（CPU和内存）；PVC能够申请特定的尺寸和访问模式（例如可以加载一个读写，或多个只读实例）。\n```yaml\nkind: PersistentVolumeClaim    \napiVersion: v1                 \nmetadata:\n  name: task-pv-claim          \nspec:\n  storageClassName: manual     \n  accessModes:                 \n    - ReadWriteOnce            \n  resources:                   \n    requests:\n      storage: 3Gi  \n```\n\n###### 示例\n在Pod中使用PVC，yaml配置文件如下：\n```yaml\nkind: Pod\napiVersion: v1\nmetadata:\n  name: task-pv-pod\nspec:\n  volumes:\n    - name: task-pv-storage\n      persistentVolumeClaim:\n       claimName: task-pv-claim\n  containers:\n    - name: task-pv-container\n      image: nginx\n      ports:\n        - containerPort: 80\n          name: \"http-server\"\n      volumeMounts:\n        - mountPath: \"/usr/share/nginx/html\"\n          name: task-pv-storage\n```\n这里创建了一个PVC类型的资源，PVC创建后，k8s的controller会去查找匹配的PV，一旦找到，则通过storageClassName与其绑定，申请PV 3GB硬盘空间，实际上就是宿主机3GB硬盘空间。\n\n**注意：PVC绑定是独占的，即一旦PVC与某个PV绑定，其他的PVC就不能再绑定到这个PV了。**\n\n## Resource Quotas\n资源配额（Resource Quotas）是用来限制用户资源用量的一种机制。\n\n它的工作原理为：\n\n* 资源配额应用在Namespace上，并且每个Namespace最多只能有一个ResourceQuota对象\n* 开启计算资源配额后，创建容器时必须配置计算资源请求或限制（也可以用LimitRange设置默认值）\n* 用户超额后禁止创建新的资源\n\n\n资源配额的类型：\n   \n* 计算资源，包括cpu和内存\n    > cpu, limits.cpu, requests.cpu\n    > memory, limits.memory, requests.memory\n\n* 存储资源，包括存储资源的总量以及指定storage class的总量\n    > requests.storage：存储资源总量，如500Gi\n    > persistentvolumeclaims：pvc的个数\n\n* 对象数，即可创建的对象的个数\n    > pods, replicationcontrollers, configmaps, secrets\n    > resourcequotas, persistentvolumeclaims\n    > services, services.loadbalancers, services.nodeports\n\n资源配额示例\n```yaml\napiVersion: v1\nkind: ResourceQuota            \nmetadata:\n  name: resource-quotas        \nspec:\n  hard:                        \n    pods: \"4\"                  \n    requests.cpu: \"1\"          \n    requests.memory: 1Gi       \n    limits.cpu: \"2\"            \n    limits.memory: 2Gi         \n    persistentvolumeclaims: \"10\" \n```\n\n该配置表示，该命名空间下\n> 最多只能有4个pod\n> CPU请求1核，限制2核\n> 内存请求1GB，限制2GB\n>最多可以声明10个PVC\n\t\n###使用示例\n创建myspace命名空间：\n```\n[root@docker22 kubernetes]# kubectl create namespace myspace\nnamespace \"myspace\" created\t\n```\n\n创建myspace命名空间的资源配额对象：\n```\n[root@docker22 kubernetes]# kubectl create -f resoureQuota.yml -n myspace\nresourcequota \"resource-quotas\" created\n```\n\n查看该命名空间下资源配额占用情况：\n```\n[root@docker22 kubernetes]# kubectl describe quota/resource-quotas -n myspace\nName:                   resource-quotas\nNamespace:              myspace\nResource                Used  Hard\n--------                ----  ----\nlimits.cpu              0     2\nlimits.memory           0     2Gi\npersistentvolumeclaims  0     10\npods                    0     4\nrequests.cpu            0     1\nrequests.memory         0     1Gi\n```\n\n编写应用app-myspace.yaml配置，如下：\n```yaml\napiVersion: extensions/v1beta1 \nkind: Deployment               \nmetadata:\n  name: app-myspace         \nspec:\n  replicas: 3                  \n  template:\n    metadata:                  \n      labels:                  \n        app: app-myspace    \n    spec:\n      containers:              \n      - name: app-myspace   \n        image: app-myspace:k8s-test\n        resources:             \n          requests:\n            memory: 100Mi      \n            cpu: 100m          \n          limits:              \n            memory: 500Mi      \n            cpu: 500m\n```\n\n创建Deployment：\n```\n[root@docker22 kubernetes]# kubectl create -f app-myspace.yml -n myspace\ndeployment \"app-myspace\" created\n```\n\n查看pod的状态，正常：\n[root@docker22 kubernetes]# kubectl get pod -n myspace -o wide\nNAME                              READY     STATUS    RESTARTS   AGE       IP             NODE\napp-myspace-7d64df76c7-8cpjj   1/1       Running   0          4m        10.244.0.135   docker22\napp-myspace-7d64df76c7-fb6xl   1/1       Running   0          4m        10.244.1.89    docker23\napp-myspace-7d64df76c7-p5w24   1/1       Running   0          4m        10.244.2.121   docker24\n\n查看资源占用情况：\n```\n[root@docker22 kubernetes]# kubectl describe quota/resource-quotas -n myspace\nName:                   resource-quotas\nNamespace:              myspace\nResource                Used    Hard\n--------                ----    ----\nlimits.cpu              1500m   2\nlimits.memory           1500Mi  2Gi\npersistentvolumeclaims  0       10\npods                    3       4\nrequests.cpu            300m    1\nrequests.memory         300Mi   1Gi\n```\n\n**注意：若把上述resource.limits.cpu改为1，则启动3个pod需要3颗CPU，而我们在Resource quota里声明了limits.cpu=2，无法满足创建3个pod的条件，所以这种情况下只能创建2个pod。**\n\n## LimitRange\n默认情况下，Kubernetes中所有容器都没有任何CPU和内存限制。LimitRange用来给Namespace增加一个默认资源限制，包括最小、最大值。\n\n配置示例：\n```yaml\napiVersion: v1\nkind: LimitRange               \nmetadata:      \n  name: mylimits               \nspec:                          \n  limits:\n  - max:\n      cpu: \"2\"                 \n      memory: 1Gi\n    min:\n      cpu: 200m                \n      memory: 6Mi              \n    type: Pod                  \n  - default:\n      cpu: 300m\n      memory: 200Mi            \n    defaultRequest:            \n      cpu: 200m                \n      memory: 100Mi            \n    max:\n      cpu: \"2\"\n      memory: 1Gi\n    min:\n      cpu: 100m\n      memory: 3Mi\n    type: Container\n```\n该配置表示，默认情况下：\n> 一个Pod内所有容器内存使用最小6M，最大1G；CPU使用最小200m，最大2核。\n> 一个容器内存使用最小3M，最大1G，requests 100M，limits 200M；CPU使用最小100m，最大2核，requests 200m，limits 300m。\n\n若一个Pod的资源限制条件不满足该namespace下的limitRange，则该Pod不会被创建。即必须满足以下条件：`min <= request <= limit <= max`\n\n## QoS\nQoS是Quality of Service的缩写，即服务质量。为了实现资源被有效调度和分配的同时提高资源利用率，Kubernetes针对不同服务质量的预期，通过QoS（Quality of Service）来对pod进行服务质量管理。对于一个pod来说，服务质量体现在两个具体的指标：CPU和内存。当节点上内存资源紧张时，kubernetes会根据预先设置的不同QoS类别进行相应处理。\n\nQoS主要分为Guaranteed、Burstable和Best-Effort三类，优先级从高到低。\n\n#### Guaranteed\n属于该级别的pod有以下两种：\n1. Pod中的所有容器都且仅设置了CPU和内存的limits\n2. pod中的所有容器都设置了CPU和内存的requests和limits，且单个容器内的requests==limits（requests不等于0）\n\npod中的所有容器都且仅设置了limits：\n```yaml\ncontainers:\n  name: foo\n    resources:\n      limits:\n        cpu: 10m\n        memory: 1Gi\n  name: bar\n    resources:\n      limits:\n        cpu: 100m\n        memory: 100Mi\n```\n\npod中的所有容器都设置了requests和limits，且单个容器内的requests==limits：\n```yaml\ncontainers:\n  name: foo\n    resources:\n      limits:\n        cpu: 10m\n        memory: 1Gi\n      requests:\n        cpu: 10m\n        memory: 1Gi\n\n  name: bar\n    resources:\n      limits:\n        cpu: 100m\n        memory: 100Mi\n      requests:\n        cpu: 100m\n        memory: 100Mi\n```\n容器foo和bar内resources的requests和limits均相等，该pod的QoS级别属于Guaranteed。\n\n#### Burstable\npod中只要有一个容器的requests和limits的设置不相同，该pod的QoS即为Burstable。\n\n容器foo指定了resource，而容器bar未指定：\n```yaml\ncontainers:\n  name: foo\n    resources:\n      limits:\n        cpu: 10m\n        memory: 1Gi\n      requests:\n        cpu: 10m\n        memory: 1Gi\n\n  name: bar\n```\n\n容器foo设置了内存limits，而容器bar设置了CPU limits：\n```yaml\ncontainers:\n  name: foo\n    resources:\n      limits:\n        memory: 1Gi\n\n  name: bar\n    resources:\n      limits:\n        cpu: 100m\n```\n\n**注意：若容器指定了requests而未指定limits，则limits的值等于节点resource的最大值；若容器指定了limits而未指定requests，则requests的值等于limits。**\n\n#### Best-Effort\n如果Pod中所有容器的resources均未设置requests与limits，该pod的QoS即为Best-Effort。\n\n容器foo和容器bar均未设置requests和limits：\n```yaml\ncontainers:\n  name: foo\n    resources:\n  name: bar\n    resources:\n```\n\n#### 根据QoS进行资源回收策略\nKubernetes通过cgroup给pod设置QoS级别，当资源不足时先kill优先级低的pod，在实际使用过程中，通过OOM分数值来实现，OOM分数值范围为0-1000。\nOOM分数值根据OOM_ADJ参数计算得出，对于Guaranteed级别的pod，OOM_ADJ参数设置成了-998，对于Best-Effort级别的pod，OOM_ADJ参数设置成了1000，对于Burstable级别的POD，OOM_ADJ参数取值从2到999。对于kuberntes保留资源，比如kubelet，docker，OOM_ADJ参数设置成了-999，表示不会被OOM kill掉。OOM_ADJ参数设置的越大，计算出来的OOM分数越高，表明该pod优先级就越低，当出现资源竞争时会越早被kill掉，对于OOM_ADJ参数是-999的表示kubernetes永远不会因为OOM将其kill掉。\n\n#### QoS pods被kill掉场景与顺序\n*Best-Effort pods：系统用完了全部内存时，该类型pods会最先被kill掉。   \n*Burstable pods：系统用完了全部内存，且没有Best-Effort类型的容器可以被kill时，该类型的pods会被kill掉。  \n*Guaranteed pods：系统用完了全部内存，且没有Burstable与Best-Effort类型的容器可以被kill时，该类型的pods会被kill掉。  \n\n#### QoS使用建议\n如果资源充足，可将QoS pods类型均设置为Guaranteed。用计算资源换业务性能和稳定性，减少排查问题时间和成本。如果想更好的提高资源利用率，业务服务可以设置为Guaranteed，而其他服务根据重要程度可分别设置为Burstable或Best-Effort。\n\n## 总结\nkubernetes中的资源是很大一块内容，本文还有些东西没有讲到，因为我也没搞清楚，有兴趣的可以去官网查阅文档了解下。\n\n参考：  \n[https://feisky.gitbooks.io/kubernetes/concepts/quota.html](https://feisky.gitbooks.io/kubernetes/concepts/quota.html)  \n[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md)  \n[http://dockone.io/article/2592](http://dockone.io/article/2592)  \n"
    },
    {
      "slug": "using-kubeadm-install-kubernetes",
      "title": "Kubeadm安装Kubernetes集群",
      "date": "2017-11-20",
      "author": null,
      "tags": [
        "k8s",
        "docker"
      ],
      "content": "\n## 前言\nkubeadm是kubernetes官方提供的快速安装kubernetes集群的工具，相比以前手动安装各个组件，kubeadm可以说是非常方便了。我在安装的过程中遇到了很多坑，而引起这些坑的根本原因就是网络不通，因为要去拉谷歌的镜像，如果服务器没有配代理的话，会遇到各种各样的问题。所以，建议大家在安装前先配好代理，如果没有代理只能墙内安装，需要从其他镜像仓库把各个镜像拉下来，并修改各个yaml文件。下面详细介绍下我使用代理安装单master k8s集群的过程。\n\n## 准备工作\n**说明：此次安装是在CentOS 7上安装`v1.8.0`版本的k8s。**\n\n* 检查以下端口在各个节点是否被占用\n\n```shell\n#主节点：\n6443*\t        Kubernetes API server\n2379-2380\t    etcd server client API\n10250\t        Kubelet API\n10251\t        kube-scheduler\n10252\t        kube-controller-manager\n10255\t        Read-only Kubelet API (Heapster)\n\n#工作节点：\n10250\t        Kubelet API\n10255\t        Read-only Kubelet API (Heapster)\n30000-32767    \tDefault port range for NodePort Services.\n```\n\n* 一些准备工作\n\n```\n#关闭防火墙\nsystemctl stop firewalld\nsystemctl disable firewalld\n\n#禁用SELinux，允许容器访问宿主机的文件系统\nsetenforce 0\n\n#将net.bridge.bridge-nf-call-iptables设为1\ncat <<EOF >  /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl -p /etc/sysctl.d/k8s.conf\n\n#关闭swap，Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动\nswapoff -a\n```\n\n## 安装docker\n这里安装的是`17.03.2.ce`版本\n```\n#卸载已安装的docker\nyum list installed | awk '{print $1}' | grep docker | xargs yum -y remove\n\nyum makecache fast\nyum install -y yum-utils device-mapper-persistent-data lvm2\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\nyum list docker-ce.x86_64  --showduplicates |sort -r\nyum install -y --setopt=obsoletes=0  docker-ce-17.03.2.ce-1.el7.centos  docker-ce-selinux-17.03.2.ce-1.el7.centos\n\n#启动docker\nsystemctl enable docker\nsystemctl start docker\n```\n\nDocker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨node的pod无法通信，在各个Docker节点执行以下命令：\n```\niptables -P FORWARD ACCEPT\n```\n\n这里建议在各个node将该命令加入到docker的启动配置中，在/etc/systemd/system/docker.service文件中加入以下内容：\n```\nExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\n```\n\n然后重启docker:\n```\nsystemctl daemon-reload\nsystemctl restart docker\n```\n\n## 配置代理\n```\n#配置全局代理\ncat <<EOF >  ~/.bashrc\nexport http_proxy=http://username:password@ip:port\nexport https_proxy=http://username:password@ip:port\nexport no_proxy=localhost,127.0.0.1,<your-server-ip>(本机ip地址)\nEOF\nsource ~/.bashrc\t\n\n#配置docker代理，拉谷歌镜像要用到：\nmkdir -p /etc/systemd/system/docker.service.d/\ncat <<EOF > /etc/systemd/system/docker.service.d/http-proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://username:password@ip:port\" \"HTTPS_PROXY=http://username:password@ip:port\" \"NO_PROXY=localhost,127.0.0.1,<your-server-ip>\"\nEOF\nsystemctl daemon-reload\nsystemctl restart docker\n```\n\n## 安装kubeadm、kubelet、kubectl\n配置谷歌yum源\n```\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n```\n\n安装kubeadm、kubelet、kubectl\n```\nyum install -y kubelet kubeadm kubectl\nsystemctl daemon-reload\nsystemctl enable kubelet\nsystemctl start kubelet\n```\n\n这里要确保docker和kubelet的cgroup driver一致，若不一致，请修改为`systemd`或`cgroupfs`。\n\n查看docker的cgroup driver：`docker info|grep Cgroup`，kubelet的启动参数`--cgroup-driver`的默认值为cgroupfs，而yum安装kubeadm和kubelet时，生成的`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`文件将这个参数值改为了systemd。可以查看该文件的内容`cat  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf|grep cgroup`。\n\n这里修改docker的cgroup driver为`systemd`\n```\ncat << EOF > /etc/docker/daemon.json\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\nEOF\n```\n\n重启docker\n```\nsystemctl daemon-reload\nsystemctl restart docker\n```\n\n## 初始化\n指定安装k8s版本为v1.8.0，第二个参数值表明pod网络指定为flannel，更多参数可以查看help\n```\nkubeadm init --kubernetes-version v1.8.0 --pod-network-cidr=10.244.0.0/16\n```\n\n因为我安装的是单master的集群，所以只在主节点服务器执行该init操作，工作节点上不要执行。\n\n若初始化失败，执行以下命令清理一些可能存在的网络问题，然后重新初始化\n```\nkubeadm reset\nifconfig cni0 down\nip link delete cni0\nifconfig flannel.1 down\nip link delete flannel.1\nrm -rf /var/lib/cni/\n```\n\n初始化完成后，你会看到如下的类似信息：\n```\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[init] Using Kubernetes version: v1.8.0\n[init] Using Authorization modes: [Node RBAC]\n[preflight] Running pre-flight checks\n[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)\n[certificates] Generated ca certificate and key.\n[certificates] Generated apiserver certificate and key.\n[certificates] apiserver serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]\n[certificates] Generated apiserver-kubelet-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Generated front-proxy-ca certificate and key.\n[certificates] Generated front-proxy-client certificate and key.\n[certificates] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[kubeconfig] Wrote KubeConfig file to disk: \"admin.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"kubelet.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"controller-manager.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"scheduler.conf\"\n[controlplane] Wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] Wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] This often takes around a minute; or longer if the control plane images have to be pulled.\n[apiclient] All control plane components are healthy after 39.511972 seconds\n[uploadconfig] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[markmaster] Will mark node master as master by adding a label and a taint\n[markmaster] Master master tainted and labelled with key/value: node-role.kubernetes.io/master=\"\"\n[bootstraptoken] Using token: <token>\n[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: kube-dns\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run (as a regular user):\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  http://kubernetes.io/docs/admin/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>\n```\n\n到这里，初始化已经完成，通过返回的最后几行信息可以看出还有些工作要做，上面最后一行的`kubeadm join --token`命令要记录下来，添加工作节点会用到。\n\n**注意：初始化完成后，要将全局代理和docker代理都去掉，否则无法将工作节点加入到集群，或遇到一些网络问题。**\n\n## 安装pod网络\n因为初始化的时候指定了flannel pod network，所以这里我安装的是flannel\n```\nwget https://raw.githubusercontent.com/coreos/flannel/v0.9.0/Documentation/kube-flannel.yml\nkubectl apply -f kube-flannel.yml\n```\n\n若安装失败，查看是否有多个网卡，如果有的话，需要在kube-flannel.yml中使用–iface参数指定集群主机内网卡的名称，否则可能会出现dns无法解析。修改-flannel.yml文件，给flanneld启动参数加上`–iface=<iface-name>`，如下：\n```yaml\n......\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n......\ncontainers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.9.0-amd64\n        command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\", \"--iface=eth1\" ]\n......\n```\n\n然后重新执行：`kubectl apply -f kube-flannel.yml`\n\n安装完成后，可以通过`kubectl get pods --all-namespaces`命令查看名为`kube-dns`的pod是否处于Running状态来确定flannel网络是否安装成功。若还是失败，请[查看troubleshooting-kubeadm](https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/)或上GitHub查阅相关问题。\n\n## 开始使用\npod网络配置好以后，需要配置常规用户访问k8s集群:\n```\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n添加其他服务器作为工作节点，在其他服务器上执行初始化返回的命令，类似如下：\n```\nkubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>    \n```\n    \n节点添加成功后，会看到类似下面的输出：\n```\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[preflight] Running pre-flight checks\n[discovery] Trying to connect to API Server \"10.138.0.4:6443\"\n[discovery] Created cluster-info discovery client, requesting info from \"https://10.138.0.4:6443\"\n[discovery] Requesting info from \"https://10.138.0.4:6443\" again to validate TLS against the pinned public key\n[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"10.138.0.4:6443\"\n[discovery] Successfully established connection with API Server \"10.138.0.4:6443\"\n[bootstrap] Detected server version: v1.8.0\n[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)\n[csr] Created API client to obtain unique certificate for this node, generating keys and certificate signing request\n[csr] Received signed certificate from the API server, generating KubeConfig...\n\nNode join complete:\n* Certificate signing request sent to master and response\n  received.\n* Kubelet informed of new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this machine join.\n```\n\n在主节点上查看所有节点状态：`kubectl get nodes`\n\n默认情况下，集群不会将pod调度到主节点，若想要调度到主节点，执行以下命令：\n```\nkubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n会看到类似下面的输出：\n```\nnode \"test-01\" untainted\ntaint key=\"dedicated\" and effect=\"\" not found.\ntaint key=\"dedicated\" and effect=\"\" not found.\n```\n\n默认情况下，工作节点上也不能使用`kubectl`执行查阅集群信息的相关命令。\n\n至此，k8s集群就算搭建完成了。\n\n## 部署dashboard\ndashboard是k8s官方出的一个插件，为集群管理提供了UI界面，很有用，搭建也非常简单。\n\n```\nwget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\nkubectl create -f kubernetes-dashboard.yaml\n```\n\n该插件依赖两个谷歌镜像：\n\ngcr.io/google_containers/kubernetes-dashboard-init-amd64:v1.0.1\n \ngcr.io/google_containers/kubernetes-dashboard-amd64:v1.7.1\n \n这里我从其他的镜像仓库pull这两个镜像，然后将kubernetes-dashboard.yaml文件的image改为自己的镜像名。**注意：这里安装的dashboard是v1.7.1版本，v1.7.x需要以https的方式访问。**官方访问dashboard并不是通过NodePort暴露服务端口的形式，这里我修改了kubernetes-dashboard.yaml文件，使其以NodePort的形式暴露服务端口，在yaml文件最后一行加上`type: NodePort`，如下：\n```yaml\n······\nspec:\n  ports:\n    - port: 443\n      targetPort: 8443\n  selector:\n    k8s-app: kubernetes-dashboard\n  type: NodePort\n```\n\n然后执行：`kubectl -n kube-system get service kubernetes-dashboard`，查看pod内443对外暴露的NodePort为30001：\n```\nNAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nkubernetes-dashboard   NodePort   10.100.111.222   <none>        443:30001/TCP   4h\n```\n\n浏览器访问https://<Node-IP>:<NodePort>，会看到登录界面，这里需要一个token来登录，也可以点击`SKIP`跳过登录直接进入dashboard，不过看不到任何集群相关的信息。\n\n获取token：\n```\n[root@GaryFeng]# kubectl get secret -n kube-system|grep kubernetes-dashboard-token|awk '{print $1}'|xargs kubectl -n kube-system describe secret\nName:         kubernetes-dashboard-token-qsgvh\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name=kubernetes-dashboard\n              kubernetes.io/service-account.uid=5cbf9d64-d139-11e7-ba2f-001517872222\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1025 bytes\nnamespace:  11 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1xc2d2aCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjVjYmY5ZDY0LWQxMzktMTFlNy1iYTJmLTAwMTUxNzg3MjUzMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.blwE2XEtrTKJSdn1zUnKTdO9gr23fub6MRhmAECfekHucuWxT2DdmHA5Jr6MnNXSY9YCxU0ynTjVSiN0AMT-aOKoFuN7ndzJ-r3hO426FTu812m9cxVB39QqP35pJ0M8RhxBfNOywtgA0mY7KK8z7UbWwE3_kDMWKgzr9nL-CIKm9swbvXq0CEjVzbEnBONoE8q3nB7WT_WmgnMy29ceZoDXc8Z45cpJM6-cV0Wl7RpsaCMNiL22WTEjkwI34KvBDXawWvTr1uwcJElPU85Z12MTZMbA1ohTBECqR8gUOrVsTY3HV1Tq8rJmfOO52PwnoQvoxT1KCFHdx6-y87JWEg\n```\n\n用token登录，会发现看不到任何集群相关的信息，这是因为dashboard是基于RBAC来控制访问权限的，而默认的ServiceAccount只有很小的权限，因此这里要创建一个kubernetes-dashboard-admin的ServiceAccount并绑定admin的权限，创建kubernetes-dashboard-admin.rbac.yaml文件：\n```yaml\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n  \n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubernetes-dashboard-admin\n  labels:\n    k8s-app: kubernetes-dashboard\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard-admin\n  namespace: kube-system\n```\n\n执行 `kubectl create -f kubernetes-dashboard-admin.rbac.yaml`\n\n查看kubernete-dashboard-admin的token:\n```\nkubectl get secret -n kube-system|grep kubernetes-dashboard-admin-token|awk '{print $1}'|xargs kubectl -n kube-system describe secret\n```\n\n再用这个token登录dashboard，就可以看到集群的全部信息了。\n\n## 总结\n总的来说，使用kubeadm安装k8s集群还是很方便的，省了很多事，我在安装过程中，主要的问题还是墙内安装时遇到的网络问题，后来上了代理之后，整个安装过程就顺畅多了。另外，官方还提供了一些插件，比如日志管理、监控等，很好用，部署也很简单，这里就暂不赘述了，后面有时间再整理。\n\n参考：  \n[https://www.kubernetes.org.cn/2906.html](https://www.kubernetes.org.cn/2906.html)\n"
    },
    {
      "slug": "deploying-a-go-application-in-docker",
      "title": "Docker部署Go应用",
      "date": "2017-09-14",
      "author": null,
      "tags": [
        "go",
        "docker"
      ],
      "content": "\n## 前言\n因为go的应用就是一个可执行的二进制文件，所以使用docker部署go应用非常简单。\n\n## 编写一个go应用\n下面是一个打印Hello World的简单go应用：\n```\n//hello.go\npackage main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    fmt.Println(\"Hello, World!\")\n}\n\n```\n执行`go build`编译生成可执行文件`hello`，这里要注意的是：**如果当前系统和拉取的镜像的系统不同，需要交叉编译。**比如我当前是在Mac下执行`go build`编译的，而我拉取的golang镜像是基于Linux的，启动容器时会发现go的二进制文件无法执行，所以要进行交叉编译：`GOOS=linux GOARCH=amd64 go build`，这里`GOOS=linux`表示编译到linux，`GOARCH=amd64`表示64位，如果镜像系统是32位，则`GOARCH=386`，更多信息请自行Google。\n\n## 构建应用镜像\n* 拉取golang基础镜像：`docker pull golang`\n\n* 编写应用镜像的Dockerfile：\n\n```\nFROM golang\n\nCOPY ./hello /tmp/hello\n\nWORKDIR /tmp/hello\n\nRUN chmod +x hello\n```\n\nDockerfile所在目录结构如下：\n```\n│── Dockerfile\n│── hello\n```\n\n构建镜像：`docker build -t hello-image .`\n\n## 运行应用\n应用镜像构建后，启动容器，如下，会打印出`Hello, World!`，然后容器退出。\n```\n➜  docker run hello-image ./hello\nHello, World!\n```\n至此，一个简单的go应用就完成了，是不是很简单？不过，这并不是本文的重点。\n\n## 最小化应用镜像\n现在，我们来看看构建的应用镜像大小：\n```\n➜  docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nhello-image         latest              a494fa9e4699        35 hours ago        732 MB\ngolang              latest              1cdc81f11b10        3 days ago          728 MB\n```\n可以看到，构建的应用镜像很大，有730多MB，这对大多数镜像来说是无法接受的，更何况我们的应用仅仅是个Hello World。\n\n由于我们拉取的基础镜像golang有728MB，导致构建的应用镜像非常大。而go应用是一个可执行的二进制文件，只需要一个系统而不需要其他的环境就可以跑起来了，所以这里我重新拉了一个叫做alpine的镜像。\n\n```\n➜  docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nhello-image         latest              a494fa9e4699        2 days ago          732 MB\ngolang              latest              1cdc81f11b10        4 days ago          728 MB\nalpine              latest              76da55c8019d        5 days ago          3.97 MB\n```\n\n可以看到，alpine镜像的大小连4MB都不到，现在修改一下Dockerfile:\n\n```\nFROM alpine\n\nCOPY ./hello /tmp/hello\n\nWORKDIR /tmp/hello\n\nRUN chmod +x hello\n```\n\n**注意：这里Hello World应用的二进制文件要重新通过以下命令生成：`CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build`，和原来相比多了`CGO_ENABLED=0`**\n\n然后我们删掉应用镜像再重新构建：`docker build -t hello-image .` ，执行以下命令启动容器：\n```\n➜  docker run hello-image ./hello\nHello, World!\n```\n可以正确打印`Hello World!`\n\n现在看看新的应用镜像大小：\n```\n➜  docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nhello-image         latest              8371d9245aa3        26 seconds ago      7.08 MB\ngolang              latest              1cdc81f11b10        4 days ago          728 MB\nalpine              latest              76da55c8019d        5 days ago          3.97 MB\n```\n新的应用镜像仅7.08MB，和原来的应用镜像相比，小了100倍！极大地节省了磁盘空间，这才是使用docker部署go应用的正确方式。\n\n## 总结\n那么，为什么编译基于alpine的应用镜像时要加上`CGO_ENABLED=0`呢？先说说golang基础镜像，因为它安装了go编译器，而go编译器又需要GCC和整个Linux发行版本，所以golang镜像才会这么大。而我们的go应用完全可以编译好了再构建到应用镜像里去，这样的话，应用镜像继承的基础镜像就不需要安装go编译器、GCC等其他的东西了。而alpine镜像仅仅包含Linux内核，所以我们的go应用在编译时要加上`CGO_ENABLED=0`来表明禁用CGO工具，否则go应用在容器中执行会出错。\n\n（完）\n"
    },
    {
      "slug": "godep-for-managing-go-package-dependencies",
      "title": "Go的包管理工具godep和dep",
      "date": "2017-07-11",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n## 前言\n目前go并没有官方的包管理工具，比较流行的是godep。godep从go 1.0开始就可以使用了，是比较老的包管理工具，早期使用go的人应该都用过godep。而dep是2017年初才发布的包管理工具，要求go >= 1.7，目前还不太成熟了，用的人很少，不过却是最有可能被纳入官方的包管理工具。下面分别介绍下godep和dep的使用。\n\n## godep\n#### install\ngodep使用起来非常简单，安装：\n```\ngo get github.com/tools/godep\n```\n安装完成后在项目根目录下执行：\n```\ngodep save\n```\n该命令会在根目录下自动生成一个Godeps和vendor目录，并将项目所依赖的第三方包信息写入Godeps/Godeps.json，同时复制包源码到vendor目录。**注意：`godep save`并不会自动从远程下载依赖包，需要我们通过`go get`或`godep get`手动下载，`godep save`只是将下载的包源码复制到vendor目录。**\n\n#### example\n这里举个例子，某项目目录结构如下：\n```\n├── myapp\n│   └── main.go\n```\nmain.go源码：\n```\npackage main\n\nimport (\n    \"fmt\"\n\n    \"github.com/shopspring/decimal\"\n)\n\nfunc main() {\n    price, err := decimal.NewFromString(\"136.204\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(price.StringFixed(2))\n}\n```\n\n这里我们导入了一个decimal包，通过`go get`下载下来后，执行`godep save`，查看项目目录结构：\n```\n├── Godeps\n│   ├── Godeps.json\n│   ├── Readme\n│── vendor\n│   └── github.com\n│       └── shopspring\n│           └── decimal\n│              │── decimal.go\n│              │── LICENSE\n│              │── README.md\n└── main.go\n```\n\n然后查看Godeps.json文件内容：\n```\n{\n    \"ImportPath\": \"myapp\",\n    \"GoVersion\": \"go1.8\",\n    \"GodepVersion\": \"v79\",\n    \"Deps\": [\n        {\n            \"ImportPath\": \"github.com/shopspring/decimal\",\n            \"Rev\": \"16a941821474ee3986fdbeab535a68a8aa5a85d2\"\n        }\n    ]\n}\n\n```\n\n如果我们的项目新增了某些依赖包，只需执行`godep save`就可以了，非常方便。但这种包管理方式，也有个不好的地方，就是除了Godep.json文件，还需要将vendor目录也提交到代码仓库，而且只要变更了引入的第三方包，则要重新提交vendor目录，只有这样才能保证其他人使用的包版本一致。如果引用的包较多，则代码仓库将变得很庞大。\n\n## dep\n#### install\n再来看看dep，安装也很简单：\n```\ngo get -u github.com/golang/dep/cmd/dep\n```\n\n#### dep init\n上面的例子我们用dep来管理项目的包，将godep生成的内容删除，在项目根目录下执行：`dep init`，加`-v`参数可以看到详细的下载过程，如下所示：\n```\n➜  dep init -v\nRoot project is \"myapp\"\n 1 transitively valid internal packages\n 1 external packages imported from 1 projects\n(0)   ✓ select (root)\n(1)\t? attempt github.com/shopspring/decimal with 1 pkgs; 1 versions to try\n(1)\t    try github.com/shopspring/decimal@master\n(1)\t✓ select github.com/shopspring/decimal@master w/1 pkgs\n  ✓ found solution with 1 packages from 1 projects\n\nSolver wall times by segment:\n     b-list-versions: 13.863003417s\n     b-source-exists:  8.254071152s\n         b-list-pkgs:  163.489659ms\n              b-gmal:  126.340874ms\n            new-atom:    1.770031ms\n         select-atom:    1.508016ms\n             satisfy:      373.44µs\n         select-root:      108.46µs\n  b-deduce-proj-root:      54.848µs\n               other:      11.261µs\n\n  TOTAL: 22.410731158s\n\n  Using master as constraint for direct dep github.com/shopspring/decimal\n  Locking in master (16a9418) for direct dep github.com/shopspring/decimal\n```\n与godep不同的是，dep可以直接从远程下载依赖包，但目前下载的速度较慢，而如果是需要翻墙才能下载的包，那么dep可能会一直堵塞。\n\n`dep init`执行完后，查看当前目录结构，dep生成了两个文件和一个vendor目录：\n```\n├── Gopkg.lock\n├── Gopkg.toml\n├── main.go\n└── vendor/\n```\n\n分别查看其中的内容：\n\nGopkg.toml\n```\n[[constraint]]\n  branch = \"master\"\n  name = \"github.com/shopspring/decimal\"\n\n```\n\nGopkg.lock\n```\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/shopspring/decimal\"\n  packages = [\".\"]\n  revision = \"16a941821474ee3986fdbeab535a68a8aa5a85d2\"\n\n[solve-meta]\n  analyzer-name = \"dep\"\n  analyzer-version = 1\n  inputs-digest = \"4611a9f68c8cdc0ab3ca83d01aa2e24d70c3a170fca0ec25e50b0669cdad6e4e\"\n  solver-name = \"gps-cdcl\"\n  solver-version = 1\n```\n\nvendor\n```\n│vendor\n│└── github.com\n│    └── shopspring\n│       └── decimal\n```\n可以看到，Gopkg.toml记录了项目所依赖的第三方包信息，Gopkg.lock则记录了包的具体版本号和其他详细信息，而下载的包源码则放在了vendor目录下。Phper会发现，dep的设计和PHP的包管理工具composer非常像，都有一个vendor目录用于存放包源码，Gopkg.toml类似composer.json，Gopkg.lock类似composer.lock。\n\ndep是如何下载依赖包的呢？**dep在下载依赖包时，会先去检查$GOPATH下是否已经存在该包，若存在，则dep直接将其拷贝到vendor目录下；若不存在，则会从远程下载。**\n\n#### dep status\n`dep status`命令用于查看当前项目依赖了哪些包，以及包的版本号：\n```\n➜  dep status\nPROJECT                        CONSTRAINT     VERSION        REVISION  LATEST   PKGS USED\ngithub.com/shopspring/decimal  branch master  branch master  16a9418   16a9418  1\n```\n\n这个命令很有用，当我们在项目中引用了新的第三方包后，比如在上面的例子中新增seelog包：\n```\npackage main\n\nimport (\n    \"fmt\"\n\n    log \"github.com/cihub/seelog\"\n    \"github.com/shopspring/decimal\"\n)\n\nfunc main() {\n    defer log.Flush()\n    price, err := decimal.NewFromString(\"136.204\")\n    if err != nil {\n        log.Error(err)\n    }\n\n    fmt.Println(price.StringFixed(2))\n}\n\n```\n\n如果我们没有执行`dep ensure`，而是先执行`dep status`：\n```\n➜  dep status\nLock inputs-digest mismatch due to the following packages missing from the lock:\n\nPROJECT                  MISSING PACKAGES\ngithub.com/cihub/seelog  [github.com/cihub/seelog]\n\nThis happens when a new import is added. Run `dep ensure` to install the missing packages.\n```\n\n发现dep给出了一个提示信息，告知我们seelog包未导入，这是因为`dep status`会去检查Gopkg.lock和vendor目录，并和项目中引入的第三方包进行比较是否匹配，若不匹配，则会提示使用`dep ensure`安装依赖包。\n\n执行`dep ensure`后再执行`dep status`查看状态：\n```\n➜  dep ensure\n➜  dep status\nPROJECT                        CONSTRAINT     VERSION        REVISION  LATEST   PKGS USED\ngithub.com/cihub/seelog        *              v2.6           d2c6e5a   d2c6e5a  1\ngithub.com/shopspring/decimal  branch master  branch master  16a9418   16a9418  1\n```\n\n查看Gopkg.lock:\n```\n[[projects]]\n  name = \"github.com/cihub/seelog\"\n  packages = [\".\"]\n  revision = \"d2c6e5aa9fbfdd1c624e140287063c7730654115\"\n  version = \"v2.6\"\n\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/shopspring/decimal\"\n  packages = [\".\"]\n  revision = \"16a941821474ee3986fdbeab535a68a8aa5a85d2\"\n\n[solve-meta]\n  analyzer-name = \"dep\"\n  analyzer-version = 1\n  inputs-digest = \"2925ca4ed4daf92cf6986c1c9b4838e3cbc8698107c2d31d42722c3ad6de44df\"\n  solver-name = \"gps-cdcl\"\n  solver-version = 1\n```\n\n查看vendor目录：\n```\n│vendor\n│└── github.com\n│    └── shopspring\n│       └── decimal\n│└── github.com\n│    └── cihub\n│       └── seelog\n```\n\n可以看到，新增的seelog包已经被正确安装，同时，Gopkg.lock和vendor也被更新了。\n\n这里有一点要说明以下，就是`dep status`的结果是根据Gopkg.lock和Gopkg.toml文件得到的，`CONSTRAINT`字段是由Gopkg.toml约束，如果toml文件不存在该包的约束，则用`*`代替；`VERSION`字段的值和lock文件一致。现在我们修改一下Gopkg.lock，删除其中一条包信息：\n```\n[[projects]]\n  branch = \"master\"\n  name = \"github.com/shopspring/decimal\"\n  packages = [\".\"]\n  revision = \"16a941821474ee3986fdbeab535a68a8aa5a85d2\"\n\n[solve-meta]\n  analyzer-name = \"dep\"\n  analyzer-version = 1\n  inputs-digest = \"2925ca4ed4daf92cf6986c1c9b4838e3cbc8698107c2d31d42722c3ad6de44df\"\n  solver-name = \"gps-cdcl\"\n  solver-version = 1\n\n```\n\n执行`dep status`：\n```\n➜  dep status\nPROJECT                        CONSTRAINT     VERSION        REVISION  LATEST   PKGS USED\ngithub.com/shopspring/decimal  branch master  branch master  16a9418   16a9418  1\n```\n\n结果只会列出decimal包信息，而如果删除Gopkg.lock文件，`dep status`的结果将为空。注意：我们一般不会去修改Gopkg.lock文件，因为修改该文件并没有什么意义，这里我只是想说明一下`dep status`的工作原理。\n\n#### dep ensure\n`dep ensure`命令会根据项目代码依赖的包，将对应包信息写入Gopkg.lock文件，将包源码下载到vendor目录，当不再使用某个包时，执行该命令也会将其从vendor中移除，并更新Gopkg.lock文件。**但是，`dep ensure`不会去更新Gopkg.toml。**\n\n那么，`dep ensure`是如何知道要下载的包的版本号呢？下面要划重点了：**`dep ensure`下载依赖包的版本是根据Gopkg.toml来的，而Gopkg.lock只是用来记录下载的各个依赖包的具体版本信息。**\n\n我们这里将dep自动生成的文件和目录都删除，重新执行一下`dep init`，然后查看Gopkg.toml:\n```\n[[constraint]]\n  name = \"github.com/cihub/seelog\"\n  version = \"2.6.0\"\n\n[[constraint]]\n  branch = \"master\"\n  name = \"github.com/shopspring/decimal\"\n\n```\n为什么这里的版本约束一个是version而另一个是branch呢？这是因为远程的decimal包没有tag，只有master分支，所以decimal的版本约束是master；而seelog是有tag的，所以它的版本约束则是该包的最大tag，也就是最大版本号。**注意：这里version对应的版本号并不是具体的版本号，而是告诉dep下载依赖包时要取大于该版本的最大版本号所对应的版本，如果没有比这个大的版本号，则该版本就是最大的。**\n\n我们看个例子，我将Gopkg.toml中seelog的version改成这样子：\n```\nversion = \"2.4.0\"\n```\n\n然后执行`dep ensure -update`来更新lock文件和vendor目录，执行完后查看Gopkg.lock的seelog部分：\n```\n[[projects]]\n  name = \"github.com/cihub/seelog\"\n  packages = [\".\"]\n  revision = \"d2c6e5aa9fbfdd1c624e140287063c7730654115\"\n  version = \"v2.6\"\n\n```\n\n可以看到version并没有变成`v2.4`，我们上面说过，dep会去取大于它的最大版本，所以还是`v2.6`。当然，要改变其版本号也是可以的，在Gopkg.toml的version约束中加个`=`就可以了，如下所示：\n```\nversion = \"=2.4.0\"\n```\n执行`dep ensure -update`，查看lock，版本号已变更：\n```\n[[projects]]\n  name = \"github.com/cihub/seelog\"\n  packages = [\".\"]\n  revision = \"607e384a1381d32741a74b66dacedcb0642d3d82\"\n  version = \"v2.4\"\n\n```\n\n>这里我非常不理解这样的设计，lock文件各个包的版本号都是指定了的，为什么不根据lock文件去下载依赖包呢？而自动生成的toml文件中的包版本约束也默认不是具体的版本号，dep在下载包时会去取大于该版本的最大版本号，这样的话就有问题了，如果我们将该Gopkg.toml和Gopkg.lock文件提交到代码仓库后，而这个包的作者又提交了一个新的版本，那么其他开发人员下载的依赖包版本和我们包版本就不一致了。\n\n## 总结\ndep的设计方向是很好的，和PHP的composer大致一样，但却远不如composer做的好，也可能还不够成熟的缘故，需要改进的地方还很多。就目前来说，不建议大家使用dep，项目的包管理工具还是使用Godep比较好，虽然要提交vendor目录到代码仓库，但至少能保证不同开发人员使用的依赖包版本是一致的。\n\n（完）\n"
    },
    {
      "slug": "the-skills-of-using-git",
      "title": "Git使用技巧",
      "date": "2017-07-09",
      "author": null,
      "tags": [
        "git"
      ],
      "content": "\n有些东西一段时间不用就特别容易忘，最近在使用git的时候老是要去查文档，所以在这里记录git的一些用法，以方便以后查阅。\n\n## 回滚\n对于已经push过的代码，回滚到某个版本，可以使用`git reset`和`git revert`两种方法。\n\n例如：远程提交记录是`A -> B -> C -> D`，要回滚到B那个版本：\n#### git reset\n```\ngit reset --hard B\ngit push origin master --force\n```\n因为reset过的当前分支版本落后远程分支，普通的push操作无法提交，必须加上`--force`参数来强制提交。执行上述操作后，远程提交记录会变为`A -> B`，`C` 和 `D` 的提交记录会被直接删除。\n#### git revert\n```\ngit revert --hard B\ngit push origin master \n```\n执行`git revert`后，不需要加`--force`强制提交，`git push`之后，远程提交记录会变为`A -> B -> C -> D -> E`，其中，版本`E`的操作就是回滚`C`和`D`的代码。\n\n**总结：`git reset`会删除提交记录，对于确实不需要的历史记录使用`git reset`回滚可以使提交记录看起来更干净；而`git revert`会保留提交记录，可以保留整个分支的所有历史记录，方便以后再次回滚。具体使用哪个命令来回滚要根据实际使用场景。**\n\n## 拉取远程新分支\n以前我要拉一个远程分支到本地，一般是通过`git pull origin b1:b1`这种方式来拉的，直到有天出现问题了才发现，`git pull`拉取的分支会和当前本地分支合并，比如我现在在master分支，执行`git pull origin b1:b1`时，git会将b1合并到master，因为`git pull`相当于`git fetch`+`git merge`，而我们往往不希望他们进行合并操作。因此，应通过以下方式拉取分支：\n```git\n#拉取origin主机的b1分支\ngit fetch origin b1\n#checkout到b1分支后，处于detached HEAD状态\ngit checkout origin/b1\n#在origin/b1中新建本地b1分支，这样就把远程b1分支拉下来了\ngit checkout -b b1\n```\n更新本地分支也应使用上述方式：\n```git\ngit fetch origin b1\ngit merge origin/b1\n```\n尽量少用`git pull`，虽然这很方便，但是容易出错。\n\n## .gitignore\n.gitignore文件可以让你忽略本地文件或目录的改动，在第一次提交到远程git仓库前，将你要忽略的文件或目录路径添加到.gitignore就行了，然后git push到remote仓库。如：\n```\ncache/\nlog/\n```\n目录或文件路径都是相对.gitignore的路径。\n\n但是，如果我们第一次并没有提交.gitignore，而是在以后加入该文件，我们会发现忽略的文件或目录改动后，git status照样会显示其已修改，这时候我们需要将本地缓存删除并重新提交才会生效：\n```bash\ngit rm -r --cached .\ngit add --all\ngit commit -m 'delete cache'\ngit push\n```\n\n## git log\n该命令用于查看提交历史记录\n\n* `git log -p` 显示每次提交的差异对比，`git log -p -2`仅显示最近的两次更新\n* `git log -stat` 显示修改的行数\n* `git log --pretty=oneline` 单行显示历史提交记录\n```\nca82a6dff817ec66f44342007202690a93763949 changed the version number\n085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test code\na11bef06a3f659402fe7563abf99ad00de2209e6 first commit\n```\n* `git log --pretty=format:\"%s\"` 定制要显示的记录格式\n```\ngit log --pretty=format:\"%h - %an, %ar : %s\"\nca82a6d - Scott Chacon, 11 months ago : changed the version number\n085bb3b - Scott Chacon, 11 months ago : removed unnecessary test code\na11bef0 - Scott Chacon, 11 months ago : first commit\n```\n\n## 子模块\n子模块在git中用的比较少，什么是子模块呢？比如我们有一个git项目A，而该项目又包含另一个git项目B，那么B就是A的子模块。\n\n向一个项目添加一个子模块：`git submodule add yourGitUrl`，如：\n```\ngit submodule add https://github.com/Seldaek/monolog.git\n```\n该命令会在当前目录clone monolog项目（也可以指定目录），并新增一个.gitmodules文件，该文件记录了所有子模块的信息。\n\n当我们使用命令`git clone mainProjectUrl`clone含有子模块的项目时，子模块会被同时clone下来，但此时子模块的目录是空的，我们需要在`.gitmodule`所在目录做初始化操作并拉取子模块的代码：\n```\ngit submodule init\ngit submodule update\n```\n这种方式比较繁琐，有个更简单的方法是加个`--recursive`参数：\n```\ngit clone --recursive mainProjectUrl\n```\n\n进入到子模块所在目录，通过`git branch`发现当前本地分支是处于一个游离的分支，我们首先需要checkout到一个本地分支：`git checkout master`，因为处于游离分支时，代码将无法提交，然后通过`git submodule update --remote --merge`来更新代码，如果忘记--merge，Git 会将子模块更新为服务器上的状态，并且会将项目当前分支重置为一个游离的分支。\n\n## 配置和取消代理\n```\ngit config --global https.proxy http://username:password@127.0.0.1:1080\ngit config --global https.proxy https://username:password@127.0.0.1:1080\n\n# 或者\ngit config --global http.proxy socks5://127.0.0.1:1080\ngit config --global https.proxy socks5://127.0.0.1:1080\n\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n```\n\n\n（待续）\n"
    },
    {
      "slug": "understanding-fastcgi-and-php-fpm",
      "title": "理解FastCGI和PHP-FPM",
      "date": "2017-07-09",
      "author": null,
      "tags": [
        "php"
      ],
      "content": "\n## 前言\n由于我一直对CGI、FastCGI、PHP-CGI、PHP-FPM这几个概念都比较模糊，所以最近花了点时间去详细了解了一下，并在此做个学习记录。\n\n## CGI和FastCGI\nCGI（Common Gateway Interface）全称是`通用网关接口`，是web服务器与应用程序之间数据交换的一种与语言无关的协议，它规定了要传递给web服务器的数据和格式，如URL、查询字符串、POST数据、header请求头等。\n\n当我们在客户端使用PHP应用程序向web服务器发起一个请求时，web服务器会启动对应的CGI程序，这里是PHP的解析器：PHP-CGI，然后PHP-CGI会解析php.ini配置文件，初始化环境，然后处理请求，处理完后CGI程序就自动退出了，最后返回结果给客户端。注意：这里的CGI程序存在性能问题，因为每有一个请求过来，CGI都会有一个`启动、初始化、退出`的过程（CGI最为人诟病的fork-and-execute模式），这是很耗费时间的，在高并发的时候，我们应用程序的响应时间就变得很长，甚至会出现无法访问的情况。\n\n在这种情况下，FastCGI就出现了，从根本上来说，FastCGI就是用来提高CGI程序的性能的。和CGI一样，FastCGI也是一种与语言无关的协议，那它是如何提高性能的呢？\n\n首先，FastCGI启动时会启一个主进程，它只做一次解析配置文件、初始化执行环境的操作，然后再启动多个子进程等待web请求。当接收到请求后，FastCGI进程管理器会将其传递给子进程，也就是一个php-cgi程序，同时可以立即接受下一个请求，子进程处理完请求后会接着等待下一个请求，这样就避免了CGI的fork模式，性能自然就高了。\n\n可以看到，FastCGI就像一个常驻的CGI程序，它是多进程的，性能自然比CGI要好，当然消耗的内存也更多。\n\n## 什么是PHP-FPM\n在修改php.ini后，需要重启php-cgi才能生效，但是php-cgi不能平滑重启，杀掉php-cgi进程后，应用程序就无法工作了。这种情况下，PHP-FPM就出现了。\n\nPHP-FPM是PHP的FastCGI进程管理器，它负责管理一个进程池，来处理web请求。它可以做到修改php.ini后平滑重启php-cgi，其处理机制是新的子进程用新的配置，已经存在的子进程处理完手上的活就可以歇着了，从而达到平滑过度的效果。其功能可以到[官方文档](http://php.net/manual/zh/install.fpm.php)查看。\n\n自PHP 5.3.3开始，PHP就已集成了PHP-FPM，在编译安装PHP时，使用--enable-fpm参数即可启用PHP-FPM了。\n\n## 使PHP-FPM监听Unix socket\nphp-fpm的监听方式默认采用TCP/Ip socket机制，Nginx默认的php文件解析配置如下：\n```nginx\nlocation ~ \\.php$ {\n    root           html;\n    fastcgi_pass   127.0.0.1:9000;\n    fastcgi_index  index.php;\n    fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;\n    include        fastcgi_params;\n}\n```\n\n查看fpm配置文件，其默认监听在9000端口，修改该配置使其监听Unix socket：\n```\n;listen = 9000\nlisten = /var/run/php-fpm.socket\n```\n\n然后修改Nginx配置文件：\n```\n#fastcgi_pass   127.0.0.1:9000;\nfastcgi_pass   unix:/var/run/php-fpm.socket;\n```\n\n重启Nginx，以后PHP的请求会被Nginx传递到Unix socket去处理。\n\n## Unix socket与Tcp/Ip socket\n为什么要采用Unix socket机制呢？下面简单说说它和TCP/Ip socket的区别。\n\nUnix socket和Tcp/Ip socket都是进程间的一种通信机制，Unix socket允许运行在同一台计算机上的的进程之间进行双向数据交换。而Tcp/Ip socket允许运行在不同计算机上的进程间通过网络通信，在某些情况下，也可以使用Tcp/Ip socket与运行在同一台计算机上的进程通信（通过使用回环接口）。\n\nUNIX socket知道进程在同一个系统上执行，所以它们可以避免一些检查和操作（如路由），这使得Unix socket进程间的通信比Tcp/Ip socket更快更轻。因此，如果你让进程在同一个主机上通信，使用Unix socket更好。\n\n## PHP-FPM的平滑操作\n启动php-fpm：`/usr/local/php/sbin/php-fpm`\n\nphp 5.3.3 以后不再支持php-fpm的start、reload、stop等操作，请使用信号控制fpm的master进程：\n* INT,TERM 立刻终止\n* QUIT 平滑终止\n* USR1 重新打开日志文件\n* USR2 平滑重载所有worker进程并重新载入配置和二进制模块\n\n重启php-fpm要先找到php-fpm的master进程pid，然后使用USR2信号kill掉：\n```\n[root@8504d5fef581 /]# ps -ef|grep php-fpm\nroot         1024     0  0 Jul04 ?        00:00:19 php-fpm: master process (/usr/local/php/etc/php-fpm.conf)\nnobody       1025     1  0 Jul04 ?        00:00:00 php-fpm: pool www\nnobody       1026     1  0 Jul04 ?        00:00:00 php-fpm: pool www\n```\n`kill -USR2 1024`，kill掉后，fpm进程会自动重启。\n\n以上方案一般用于没有生成php-fpm.pid文件时使用，我们可以在php-fpm.conf配置文件中指定pid文件的存放位置：\n```\npid = /var/run/php-fpm.pid\n```\n重启php-fpm，/var/run目录下便会生成php-fpm.pid文件了，以后就可以使用以下命令重启php-fpm：\n```\ncat /var/run/php-fpm.pid|xargs kill -USR2\n```\n\n（完）\n"
    },
    {
      "slug": "fastcgi-cache-with-nginx",
      "title": "Nginx设置FastCGI Cache",
      "date": "2017-06-29",
      "author": null,
      "tags": [
        "php",
        "nginx"
      ],
      "content": "\n## 前言\nNginx包含一个FastCGI模块，该模块具有缓存PHP后端提供的动态内容的指令。设置此操作将无需额外的页面缓存解决方案，如反向代理或特定于应用程序的插件。也可以根据请求方法，URL，Cookie或任何其他服务器变量不缓存某些内容。\n\n## 开启FastCGI Cache\n编辑nginx.conf配置文件，在`Server{}`模块的上面添加以下内容：\n```\nfastcgi_cache_path /etc/nginx/cache levels=1:2 keys_zone=MYAPP:100m inactive=60m;\nfastcgi_cache_key \"$scheme$request_method$host$request_uri\";\n```\n\n上面`fastcgi_cache_path`指令指定缓存的路径为`/etc/nginx/cache`，大小为100M，内存区域名称为`MYAPP`，缓存目录级别是子目录级别，非活动时间是`60m`。\n\n`fastcgi_cache_path`的路径可以是硬盘的任何位置，但设置的缓存目录大小必须小于`RAM+Swap`的大小，否则会报`无法分配内存`的错误。`inactive`选项表示如果在指定的时间段（60分钟）中未访问缓存，则Nginx会将其删除。\n\n`fastcgi_cache_key`指令指定缓存文件名将如何被散列，Nginx根据该指令对访问的文件进行md5加密。\n\n下一步，在`location ~ \\.php$ {}`中添加以下内容：\n```\nfastcgi_cache MYAPP;\nfastcgi_cache_valid 200 60m;\n```\n\n`fastcgi_cache`指令引用了我们在`fastcgi_cache_path`指令中指定的内存区域名称，并将缓存存储在此区域中。\n\n默认情况下，Nginx会将缓存的对象存储在任何这些头指定的持续时间内：**X-Accel-Expires / Expires / Cache-Control**。`fastcgi_cache_valid`指令用于指定默认的缓存生命周期，如果这些头没有指定缓存时间。在上面的声明中，`200`表示仅缓存状态码为200的响应，当然也可以指定其他的状态码。\n\n配置已全部完成，然后检测Nginx配置文件并重载：\n```\n/usr/local/nginx/sbin/nginx -t\n/usr/local/nginx/sbin/nginx -s reload\n```\n\n完整的配置像下面这样：\n```\nfastcgi_cache_path /etc/nginx/cache levels=1:2 keys_zone=MYAPP:100m inactive=60m;\nfastcgi_cache_key \"$scheme$request_method$host$request_uri\";\n\nserver {\n    listen   80;\n\n    root /usr/share/nginx/html;\n    index index.php index.html index.htm;\n\n    server_name example.com;\n\n    location / {\n        try_files $uri $uri/ /index.html;\n    }\n\n    location ~ \\.php$ {\n        try_files $uri =404;\n        fastcgi_pass unix:/var/run/php5-fpm.sock;\n        fastcgi_index index.php;\n        fastcgi_cache MYAPP;\n        fastcgi_cache_valid 200 60m;\n        include fastcgi_params;\n    }\n}\n```\n\n## 测试FastCGI Cache\n创建一个打印当前时间的test.php文件：\n```php\n<?php\necho date('Y-m-d H:i:s');\n```\n\n通过curl访问，发现每次访问的结果一样：\n```\n[root@703dad187862 html]# curl http://localhost/test.php;echo\n2017-06-28 07:12:48\n[root@703dad187862 html]# curl http://localhost/test.php;echo\n2017-06-28 07:12:48\n[root@703dad187862 html]# curl http://localhost/test.php;echo\n2017-06-28 07:12:48\n```\n\n由此说明，我们设置的FastCGI Cache生效了。然后进入缓存目录`/etc/nginx/cache`，可以看到缓存文件：\n```\n[root@703dad187862 ~]# cd /etc/nginx/cache/\n[root@703dad187862 cache]# ll 2/47/5f1796857e2d0d86e589be12bbd49472\n-rw------- 1 nobody nobody 399 Jun 28 07:12 2/47/5f1796857e2d0d86e589be12bbd49472\n```\n\n我们还可以让Nginx在响应中添加一个`X-Cache`头，表明缓存是否被丢失或命中。\n\n在`server{}`块上面添加以下内容：\n```\nadd_header X-Cache $upstream_cache_status;\n```\n\n重载nginx.conf配置文件，然后通过curl做一个详细的请求：\n```\n[root@703dad187862 cache]# curl -v localhost/test.php\n* About to connect() to localhost port 80 (#0)\n*   Trying ::1...\n* Connection refused\n*   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 80 (#0)\n> GET /test.php HTTP/1.1\n> User-Agent: curl/7.29.0\n> Host: localhost\n> Accept: */*\n>\n< HTTP/1.1 200 OK\n< Server: nginx/1.11.5\n< Date: Tue, 27 Jun 2017 23:29:47 GMT\n< Content-Type: text/html; charset=UTF-8\n< Transfer-Encoding: chunked\n< Connection: keep-alive\n< X-Powered-By: PHP/7.0.12\n< X-Cache: HIT\n<\n* Connection #0 to host localhost left intact\n```\n\n可以看到`X-Cache:HIT`这一行，表明命中了缓存。\n\n或者在浏览器访问http://localhost/test.php，F12查看响应内容：\n![Alt text](/img/2017/06/response.png)\n\n## 排除某些缓存\n某些动态内容（如身份验证所需的页面）不应被缓存，这样的内容可以根据诸如`$request_uri`，`$request_method`和`$http_cookie`这样的服务器变量被排除在缓存之外。\n\n下面是一个简单的配置示例，必须写在`server{}`块中。\n```\n#默认缓存所有内容\nset $no_cache 0;\n\n#不缓存POST请求\nif ($request_method = POST)\n{\n    set $no_cache 1;\n}\n\n#如果URL包含query_string查询字符串，则不缓存\nif ($query_string != \"\")\n{\n    set $no_cache 1;\n}\n\n#不缓存以下的URL内容\nif ($request_uri ~* \"/(administrator/|login.php)\")\n{\n    set $no_cache 1;\n}\n\n#如果cookie名是PHPSESSID，则不缓存\nif ($http_cookie = \"PHPSESSID\")\n{\n    set $no_cache 1;\n}\n```\n\n要将`$no_cache`变量应用于相应的指令，请将以下内容放在`location 〜 \\.php $ {}`块中：\n```\nfastcgi_cache_bypass $no_cache;\nfastcgi_no_cache $no_cache;\n```\n`fasctcgi_cache_bypass`指令忽略与先前设置的条件相关的请求的现有缓存，如果满足指定的条件，`fastcgi_no_cache`指令不会缓存请求。\n\n## 清除缓存\n缓存的命名约定基于我们为`fastcgi_cache_key`指令设置的变量:\n```\nfastcgi_cache_key \"$scheme$request_method$host$request_uri\";\n```\n\n根据这些变量，当我们请求`http://localhost/test.php`时，上面指令的变量内容被替换后的实际内容如下：\n```\nfastcgi_cache_key \"httpGETlocalhost/test.php\";\n```\n\n通过MD5对字符串进行hash后的内容为：\n```\n5f1796857e2d0d86e589be12bbd49472\n```\n\n这就是生成的缓存的文件名，就像我们设置的子目录级别“levels=1:2”，因此，第一级目录名将是MD5值的最后一个字符，也就是`2`，第二级目录名将是MD5剩余字符的最后两个字符，也就是`47`，所以，该缓存的完整路径如下：\n```\n/etc/nginx/cache/2/47/5f1796857e2d0d86e589be12bbd49472\n```\n\n基于这种缓存命名格式，你可以使用任何语言写一个清除缓存的脚本。下面我写了一个PHP脚本通过POST请求来清除缓存：\n```php\n<?php\n$cache_path = '/etc/nginx/cache/';\n$url = parse_url($_POST['url']);\nif(!$url)\n{\n    echo 'Invalid URL entered';\n    die();\n}\n$scheme = $url['scheme'];\n$host = $url['host'];\n$requesturi = $url['path'];\n$hash = md5($scheme.'GET'.$host.$requesturi);\nvar_dump(unlink($cache_path . substr($hash, -1) . '/' . substr($hash,-3,2) . '/' . $hash));\n```\n\n给该脚本发送POST请求，传递的数据是要被清除缓存的URL地址：\n```\ncurl -d 'url=http://localhost/test.php' http://localhost/purge.php\n```\n\n脚本将根据缓存是否被清除而输出true或false，请确保此脚本不会被缓存，且有访问权限。\n\n\n（完）\n"
    },
    {
      "slug": "names-in-go",
      "title": "Go的变量命名",
      "date": "2017-06-27",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n#### 名字的重要性\n**可读性是定义良好代码的标准，好的名字对于可读性至关重要**，这是对于Go的变量命名来讲的。\n\n#### 好的变量名\n一个好的变量名的标准：\n* 一致（容易猜测）\n* 简短（容易声明）\n* 准确（容易理解）\n\n#### 经验法则\n变量声明的地方与使用它的地方隔的越远，则变量名应该越长。\n\n#### 使用大小写混合\nGo中的变量名应该使用大小写混合，而不应使用names_with_underscores这种下划线分隔的，首字母缩写词应该都是大写字母，如ServeHTTP和IDProcessor.\n\n#### 局部变量\n保持简短的局部变量名，长的变量名会掩盖代码的真正作用。\n\n如，使用变量名`i`优于`index`，使用变量名`r`优于`reader`。\n\n较长的变量名在很长的函数或带有很多局部变量的函数中可能很有用，但这往往意味着你的代码该重构了。\n\n不要这样命名：\n```\nfunc RuneCount(buffer []byte) int {\n    index, count := 0, 0\n    for index < len(buffer) {\n        if buffer[index] < RuneSelf {\n            index++\n        } else {\n            _, size := DecodeRune(buffer[index:])\n            index += size\n        }\n        count++\n    }\n    return count\n}\n```\n\n而应该这样命名：\n```\nfunc RuneCount(b []byte) int {\n    i, n := 0, 0\n    for i < len(b) {\n        if b[i] < RuneSelf {\n            i++\n        } else {\n            _, size := DecodeRune(b[i:])\n            i += size\n        }\n        n++\n    }\n    return n\n}\n```\n\n#### 参数\n函数参数类似局部变量，但它们也可以用于文档说明。\n\n如果参数类型是描述性的，那么它们应该是简短的：\n```\nfunc AfterFunc(d Duration, f func()) *Timer\n\nfunc Escape(w io.Writer, s []byte)\n```\n\n如果参数类型有歧义，那么变量名应该可以提供说明：\n```\nfunc Unix(sec, nsec int64) Time\n\nfunc HasPrefix(s, prefix []byte) bool\n```\n\n#### 返回值\n导出函数的返回值命名应该仅仅被用于文档说明。\n\n下面是命名返回值的比较好的例子：\n```\nfunc Copy(dst Writer, src Reader) (written int64, err error)\n\nfunc ScanBytes(data []byte, atEOF bool) (advance int, token []byte, err error)\n```\n\n#### 接收器\n接收器是一种特殊的参数。\n\n按照惯例，它们是反映接收器类型的一个或两个字符，因为它们通常出现在几乎每一行：\n```\nfunc (b *Buffer) Read(p []byte) (n int, err error)\n\nfunc (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request)\n\nfunc (r Rectangle) Size() Point\n```\n\n接收器的命名在一个类型的方法中应该是一致的（不要在一个方法中使用r，而在另一个方法中使用rdr）。\n\n#### 导出的包级别名称\n在命名导出的变量，常量，函数和类型时记住：**导出的名称由其包名称限定**。\n\n这就是为什么我们有`bytes.Buffer`和`strings.Reader`，而不是`bytes.ByteBuffer`和`strings.StringReader`。\n\n#### 接口类型\n仅指定了一个方法的接口，该接口的命名在函数名后加`er`即可。\n```\ntype Reader interface {\n    Read(p []byte) (n int, err error)\n}\n```\n\n有时候加了`er`的接口名不是一个正确的英文单词，但我们仍然会使用该命名：\n```\ntype Execer interface {\n    Exec(query string, args []Value) (Result, error)\n}\n```\n\n有时候我们根据英语语法来命名，使其看起来更容易理解：\n```\ntype ByteReader interface {\n    ReadByte() (c byte, err error)\n}\n```\n\n当一个接口包含多个方法时，请选择一个可以准确描述其作用的名称。（例如: net.Conn, http.ResponseWriter, io.ReadWriter）\n\n#### 错误\n错误类型接口的命名应该是`FooError`这种形式：\n```\ntype ExitError struct {\n    ...\n}\n```\n\n错误的变量命名应该是`ErrFoo`这种形式：\n```\nvar ErrFormat = errors.New(\"image: unknown format\")\n```\n\n#### 包名\n选择对其导出的名称有意义的包名称，避免使用util,common等这种通用的名称。\n\n#### 导入路径\n一个包路径的最后一部分应该和包名一致：\n```\n\"compress/gzip\" // package gzip\n```\n\n避免在库和包路径的stutter：\n```\n\"code.google.com/p/goauth2/oauth2\" // bad; my fault\n```\n\n对于库来说，经常将包代码放在仓库根目录：\n```\n\"github.com/golang/oauth2\" // package oauth2\n```\n\n也避免大写字母，因为并不是所有的文件系统都区分大小写。\n\n#### 标准库\n这篇文章的许多例子来源于标准库。标准库是一个很好的可以找到良好代码的地方，多看看标准库来寻找变量命名的灵感。\n\n#### 总结\n* 使用短变量\n* 考虑上下文\n* 根据自己的判断来命名\n"
    },
    {
      "slug": "create-mongodb-replset-cluster-using-docker",
      "title": "Docker搭建Mongodb集群",
      "date": "2017-06-19",
      "author": null,
      "tags": [
        "docker"
      ],
      "content": "\n最近在搞公司内网开发环境，需要基于docker容器搭建一套mongodb复制集群，这我以前没搭过啊，于是我去Google了一下，发现还挺简单，但其中也遇到了坑，所以这里记录一下，避免以后再次踩坑。\n\n## 思路\n这里搭建集群是在单台物理机上搭建的，大致思路基本都是这样：使用mongo基础镜像启3个docker容器，一个主服务器两个备份服务器，将容器内的27017端口映射到物理机上三个端口，然后进入容器内初始化副本集就完成了。\n\n## 初步尝试\n根据上面的思路，我们一步步来实现。\n\n启动三个mongodb容器：\n```bash\ndocker run -d --name rs1 -v /data/mongodb/rs1:/data/db -p 30001:27017 mongo:3.2 mongod --dbpath /data/db --replSet mongoreplset\n\ndocker run -d --name rs2 -v /data/mongodb/rs2:/data/db -p 30002:27017 mongo:3.2 mongod --dbpath /data/db --replSet mongoreplset\n\ndocker run -d --name rs3 -v /data/mongodb/rs3:/data/db -p 30003:27017 mongo:3.2 mongod --dbpath /data/db --replSet mongoreplset\n```\n这里我将数据库挂载出来了，以免容器挂掉后导致数据全部丢失，将容器内27017端口分别映射到物理机30001、30002、30003端口，容器启动后启动mongo服务并指定副本集名称为mongoreplset。\n\n上面启动3个容器的方法比较繁琐不够灵活，所以这里我改用`docker-compose`命令来启动多个容器，编排docker-compose.yml文件如下：\n```bash\nversion: '3'\nservices:\n  rs1:\n    image: mongo:3.2\n    container_name: \"rs1\"\n    ports:\n      - \"30001:27017\"\n    volumes:\n      - /data/mongodb/rs1:/data/db\n    command: mongod --dbpath /data/db --replSet mongoreplset\n  rs2:\n    image: mongo:3.2\n    container_name: \"rs2\"\n    ports:\n      - \"30002:27017\"\n    volumes:\n      - /data/mongodb/rs2:/data/db\n    command: mongod --dbpath /data/db --replSet mongoreplset\n  rs3:\n    image: mongo:3.2\n    container_name: \"rs3\"\n    ports:\n      - \"30003:27017\"\n    volumes:\n      - /data/mongodb/rs3:/data/db\n    command: mongod --dbpath /data/db --replSet mongoreplset\n```\n在docker-compose.yml文件所在目录执行`docker-compose up -d`，这样就启动了三个容器，通过`docker ps`查看容器是否启动成功，如果启动失败，可以通过命令`docker logs 容器id`查看日志信息。\n\n容器启动成功后，集群就基本搭建完成了，但此时副本集还未初始化，进入容器rs1初始化副本集:\n```\ndocker exec -ti rs1 mongo\n\nrs.initiate()\nrs.add('rs2:27017')\nrs.add('rs3:27017')\nrs.status()  \n```\n通过`rs.status()`可以看到rs1是主服务器，rs2和rs3是备份服务器，现在来验证一下，在rs1容器写入一条数据，然后进入rs2和rs3查看数据是否同步：\n```\ndocker exec -ti rs1 mongo\nuse test\ndb.test.insert({now: new Date()})\nquit()\n\ndocker exec -ti rs2 mongo\nrs.slaveOk()    // 备份节点默认不可读写，需要通过该命令来允许读操作\nuse test\ndb.test.find()\nquit()\n\ndocker exec -ti rs3 mongo\nrs.slaveOk()\nuse test\ndb.test.find()\nquit()\n```\n通过运行结果可以看到数据已经同步，说明我们的副本集搭建成功了。哟嚯~~~\n\n## 出现问题\n我们内网环境搭了一个swarm集群，我在该集群另一台物理机上启动了一个搭好了php-fpm-nginx环境的web容器，当我在web中通过PHP客户端连接到mongo集群时，问题就来了，我发现居然连不上mongo集群！(我们的web容器与mongo集群是处于同一网络模式的，所以不存在网络不通的情况。)测试代码如下：\n```php\n<?php\ntry {\n    $options = array('replicaSet'=>'mongoreplset');\n    $mongo = new MongoClient('mongodb://rs1:30001,rs2:30002,rs3:30003', $options);\n    var_dump($mongo);\n} catch (Exception $e) {\n    echo $e->getMessage();\n}\n```\n运行后没有打印出结果，而是在等待数秒后捕获到了异常，说明连接到mongo集群失败，然后我使用非集群模式连接到mongo集群的单台服务器，可以正常打印结果，说明mongo服务是可用的。\n```php\n<?php\ntry {\n    $mongo = new MongoClient('mongodb://rs1:30001');\n    var_dump($mongo);\n} catch (Exception $e) {\n    echo $e->getMessage();\n}\n```\n\n这就很尴尬了，单台服务器可以连上，集群模式却连不上，在网上Google了一大圈也没啥收获，后来去请教我们老大，他说他之前搭redis集群也遇到过这样的问题，因为启动的容器如果不指定网络的话，默认使用`bridge`网络，但使用bridge网络连接到集群是有问题的，具体是什么问题我们也不确定。\n\n## 解决方案\n**老大给出的解决方案是采用host网络，启动mongo容器时直接指定端口。**在host网络模式下，宿主机和容器之间没有被隔离，网络和端口都是共享的，在容器中指定的端口相当于直接在宿主机上指定了该端口。比如使用host网络启动一个mongo容器并指定27017端口，就相当于在宿主机上启了一个mongo服务并指定了27017端口，这里有一点要注意的是：我们要在其他的物理机上访问的话也就只能通过宿主机的`ip+端口`来访问了。\n\n修改docker-compose.yml文件：\n```bash\nversion: '3'\nservices:\n  rs1:\n    image: mongo:3.2\n    container_name: \"rs1\"\n    network_mode: \"host\"\n    volumes:\n      - /data/mongodb/rs1:/data/db\n    command: mongod --port 27017 --dbpath /data/db --replSet mongoreplset\n  rs2:\n    image: mongo:3.2\n    container_name: \"rs2\"\n    network_mode: \"host\"\n    volumes:\n      - /data/mongodb/rs2:/data/db\n    command: mongod --port 27018 --dbpath /data/db --replSet mongoreplset\n  rs3:\n    image: mongo:3.2\n    container_name: \"rs3\"\n    network_mode: \"host\"\n    volumes:\n      - /data/mongodb/rs3:/data/db\n    command: mongod --port 27019 --dbpath /data/db --replSet mongoreplset\n```\n\n这里通过`network_mode`指令指定使用host网络，`command`指令分别指定了27017、27018、27019三个端口，然后通过`docker-compose up -d`重新启动。如果启动失败，请先`docker-compose stop`，再rm掉这几个容器，并清空挂载目录下的文件。\n\n容器重新启动后初始化副本集集群，步骤和上面是一样的，但也有点区别：\n```bash\ndocker exec -ti rs1 mongo --port 27017\n\nrs.initiate()\nrs.add('10.0.5.11:27018')\nrs.add('10.0.5.11:27019')\nrs.status()  \n```\n这里添加副本集成员必须使用宿主机ip而不能使用容器名，然后我们重新通过PHP客户端连接到mongo集群：\n```php\n<?php\ntry {\n    $options = array('replicaSet'=>'mongoreplset');\n    $mongo = new MongoClient('mongodb://10.0.5.11:27017,10.0.5.11:27018,10.0.5.11:27019', $options);\n    var_dump($mongo);\n} catch (Exception $e) {\n    echo $e->getMessage();\n}\n```\n\n运行以上代码，打印结果正常。至此，我们的mongo集群终于搭建成功并能通过集群的模式来连接了。\n\n## 注意点\n当需要向mongodb导入数据时，需要指定宿主机IP：\n```bash\nmongoimport -h 10.0.5.11 -d mydb -c mycol data.dat\n```\n\n为了让以上操作全部自动化，初始化副本集的操作可以写一个shell脚本，然后构建一个镜像，使用docker-compose启动，从而在mongo容器启动后，可以通过脚本自动初始化副本集，大大提高了开发效率。\n\n（完）\n"
    },
    {
      "slug": "the-lock-in-go",
      "title": "Go的锁机制",
      "date": "2017-06-12",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\ngo是一门并发特性非常强大的语言，我们在实现并发编程时，往往会碰到多个线程同时访问同一个变量的情况，也就是所谓的竞态，这种情况可能会导致数据混乱出错，因此，这个时候就需要对变量上锁，来保证一次只有一个线程能修改该变量，下面将详细介绍go的锁机制。\n\n## sync\ngo语言中的锁机制是通过自带的sync包来实现的，该包包含了以下几种锁类型。\n\n## sync.Mutex\nMutex是互斥锁，其定义方式很简单，先是定义了一个Mutex类型结构体，然后该类型实现了Lock()和Unlock()两个方法。\n```\ntype Mutex struct {\n    state int32\n    sema  uint32\n}\n\nfunc (m *Mutex) Lock()\n\nfunc (m *Mutex) Unlock()\n```\n\n当一个变量被上了互斥锁后，其他访问该变量的线程会被堵塞，不可对该变量进行读写操作，直到锁被释放。下面是一个互斥锁的例子：\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nvar m *sync.Mutex\n\nfunc main() {\n    m = new(sync.Mutex)\n    go read(1)\n    go read(2)\n    time.Sleep(time.Second) // 让goroutine有足够的时间执行完\n}\n\nfunc read(i int) {\n    fmt.Println(i, \"begin lock\")\n    m.Lock()\n    fmt.Println(i, \"in lock\")\n    m.Unlock()\n    fmt.Println(i, \"unlock\")\n}\n```\n\nmain函数里启了两个goroutine，调了两次read函数，无论哪个goroutine先执行，先调用read函数的会先获得互斥锁，而另一个goroutine在获取互斥锁时发现已经被占用了，其必须等待互斥锁被释放后才能获得该线程内的互斥锁，所以程序打印结果只会是以下两种:\n\n```\n1 begin lock\n2 begin lock\n1 in lock\n1 unlock\n2 in lock\n2 unlock\n\n// 或\n2 begin lock\n1 begin lock\n2 in lock\n2 unlock\n1 in lock\n1 unlock\n```\n\n而不会出现在`1 lock start`中打印出`2 in lock`的情况。\n\n## sync.RWMutex\n在上面的例子中，如果有很多goroutine并发执行的话就会存在一个问题，因为某个线程获得互斥锁后，其他的goroutine被堵塞，导致程序的效率较低，这种情况下就需要用到读写锁RWMutex了。\n\nRWMutex是基于互斥锁Mutex实现的，包含了读锁Rlock()和写锁Lock()，上读锁时，数据可以被多个goroutine并发访问但不可写，而上写锁时，数据不可被其他goroutine读或写。下面是其定义方式：\n```\ntype RWMutex struct {\n    w           Mutex  // held if there are pending writers\n    writerSem   uint32 // semaphore for writers to wait for completing readers\n    readerSem   uint32 // semaphore for readers to wait for completing writers\n    readerCount int32  // number of pending readers\n    readerWait  int32  // number of departing readers\n}\n\nfunc (*RWMutex) Lock    // 写锁\n\nfunc (*RWMutex) Unlock\n\nfunc (*RWMutex) RLock   // 读锁\n\nfunc (*RWMutex) RUnlock\n```\n\n我们将上面互斥锁的例子改写一下：\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\nvar m *sync.RWMutex\nvar val = 0\n\nfunc main() {\n\tm = new(sync.RWMutex)\n\tgo read(1)\n\tgo write(2)\n\tgo read(3)\n\ttime.Sleep(5 * time.Second)\n}\n\nfunc read(i int) {\n\tfmt.Println(i, \"begin read\")\n\tm.RLock()\n\ttime.Sleep(1 * time.Second)\n\tfmt.Println(i, \"val: \", val)\n\ttime.Sleep(1 * time.Second)\n\tm.RUnlock()\n\tfmt.Println(i, \"end read\")\n}\n\nfunc write(i int) {\n\tfmt.Println(i, \"begin write\")\n\tm.Lock()\n\tval = 10\n\tfmt.Println(i, \"val: \", val)\n\ttime.Sleep(1 * time.Second)\n\tm.Unlock()\n\tfmt.Println(i, \"end write\")\n}\n```\n\n可能的打印结果：\n```\n2 begin write\n3 begin read\n1 begin read\n2 val:  10\n2 end write\n1 val:  10\n3 val:  10\n1 end read\n3 end read\n```\n\n分析以上结果可以看到，2在写的时候，无法打印出1或3的val，只有2 end write了，才能开始打印出他们的val，说明写锁期间其他goroutine不能访问该变量。继续分析发现在1还没有end read时，已经打印出了3的val，说明读锁期间是允许多个goroutine访问同一变量的。\n\n> RWMutex只有当获得锁的大部分goroutine都是读操作，而锁在竞争条件下，也就是说，goroutine们必须等待才能获取到锁的时候，使用RWMutex才是最能带来好处的。RWMutex需要更复杂的内部记录，所以它会比一般的无竞争锁的mutex慢一些。\n\n## sync.Once\n某些情况下，多个goroutine并发执行时，我们希望goroutine中的某个函数只执行一次，这时候用Once就非常方便了。其定义方式如下：\n```\ntype Once struct {\n    m    Mutex\n    done uint32\n}\n\nfunc (o *Once) Do(f func())\n```\n该类型也是基于Mutex实现的，因为只会调用一次，其作用类似于init初始化函数，也往往用于初始化操作，请看下面的例子：\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nfunc main() {\n    var once sync.Once\n    for i := 0; i < 10; i++ {\n        go func() {\n            once.Do(read)\n        }()\n    }\n    time.Sleep(time.Second)\n}\n\nfunc read() {\n    fmt.Println(1)\n}\n```\n打印结果：\n```\n1\n```\n最终只会打印出一次1。\n\n## sync.WaitGroup\nWaitGroup用于等待一组goroutine执行完成，主线程调用Add方法来设置要等待的goroutine数量，每个goroutine运行后会调用Done方法，同时Wait方法会一直堵塞直到所有goroutine执行完成。\n```\ntype WaitGroup struct {\n    // contains filtered or unexported fields\n}\n\nfunc (wg *WaitGroup) Add(delta int)\n\nfunc (wg *WaitGroup) Done()\n\nfunc (wg *WaitGroup) Wait()\n```\n\n我们结合下面的例子来看看它是如何实现的：\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nfunc main() {\n    var wg sync.WaitGroup\n    var str = []string{\n        \"Hello, World\",\n        \"Hello, Go\",\n        \"Bye, PHP\",\n    }\n    for _, s := range str {\n        // Increment the WaitGroup counter.\n        wg.Add(1)\n        // Launch a goroutine to read the str.\n        go func(s string) {\n            // Decrement the counter when the goroutine completes.\n            defer wg.Done()\n            // Println the s.\n            read(s)\n        }(s)\n    }\n    // Wait for all goroutine to complete.\n    wg.Wait()\n}\n\nfunc read(s string) {\n    time.Sleep(time.Second * 1)\n    fmt.Println(s)\n}\n```\n\nWaitGroup中存在一个计数器，其原理其实是通过这个计数器来实现的。Add接受一个int类型的参数，当传入正整数n时，计数器的值会增加n，当传入负整数n时，计数器的值会减少n；而当计数器的值等于0时，也就意味着所有goroutine执行完了，堵塞的Wait会被释放，WaitGroup的使命也就完成了。**注意：Wait释放之前，计数器的值不能为负，否则程序会panic掉。**\n\n上述例子中，main函数执行时，Wait会一直堵塞，for循环开始都会调用一次Add(1)，使计数器加一，每个goroutine执行完成后会调用Done，使计数器减一，这个Done其实是调用了Add(-1)，大家可以查看下源码。这样，整个for循环跑完后计数器的值肯定是0，也就是说所有goroutine执行完了，然后堵塞的Wait会被释放，后面的程序会继续执行。\n\n根据以上结论，我们也可以将wg.Add()写在for循环外面：\n```\nfunc main() {\n    var wg sync.WaitGroup\n    var str = []string{\n        \"Hello, World\",\n        \"Hello, Go\",\n        \"Bye, PHP\",\n    }\n    // Increment the WaitGroup counter.\n    wg.Add(len(str))\n    for _, s := range str {\n        // Launch a goroutine to read the str.\n        go func(s string) {\n            // Decrement the counter when the goroutine completes.\n            defer wg.Done()\n            // Println the s.\n            read(s)\n        }(s)\n    }\n    wg.Wait()\n}\n```\n\n打印结果：\n```\nHello, World\nHello, Go\nBye, PHP\n```\n\n## sync.Cond\nCond的作用和WaitGroup是一样的，都是让goroutine堵塞，不同的是WaitGroup是被动堵塞，所有goroutine跑完后，wait会自动释放，而Cond是主动堵塞，我们必须给cond发送一个信号，来通知wait释放。\n```\ntype Cond struct {\n    noCopy noCopy\n\n    // L is held while observing or changing the condition\n    L Locker\n\n    notify  notifyList\n    checker copyChecker\n}\n\nfunc NewCond(l Locker) *Cond\n\nfunc (c *Cond) Signal()\n\nfunc (c *Cond) Broadcast()\n\nfunc (c *Cond) Wait()\n```\n\n通过Cond的定义方式可以看到，通过调用NewCond函数来获得一个Cond对象，每个Cond都关联一个Locker L（通常是一个\\*Mutex或\\*RWMutex），在更改条件和调用Wait方法时必须持有该Locker。\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nfunc main() {\n    locker := new(sync.Mutex)\n    cond := sync.NewCond(locker)\n    done := false\n\n    cond.L.Lock()\n\n    go func() {\n        time.Sleep(time.Second * 1)\n        done = true\n        cond.Signal()    // 发送信号，通知Wait()释放\n    }()\n\n    if !done {\n        cond.Wait()      // 堵塞主goroutine\n    }\n\n    fmt.Println(\"now done is\", done)    //一秒钟后会打印出 now done is true\n}\n```\n这里的cond.Signal()就是用来发送一个信号给Wait来通知其释放的，sync.Cond还有一个BroadCast方法，用来通知释放所有堵塞的gouroutine。\n```\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nvar locker = new(sync.Mutex)\nvar cond = sync.NewCond(locker)\n\nfunc read(x int) {\n    cond.L.Lock()    // 获取锁\n    cond.Wait()      // 等待通知，暂时阻塞\n    fmt.Println(x)\n    time.Sleep(time.Second * 1)\n    cond.L.Unlock()  // 释放锁，不释放的话将只会有一次输出\n}\n\nfunc main() {\n    for i := 0; i < 40; i++ {\n        go read(i)\n    }\n    fmt.Println(\"start all\")\n    time.Sleep(time.Second * 1)\n    cond.Broadcast() // 下发广播给所有等待的goroutine\n    time.Sleep(time.Second * 60)\n}\n```\n\n（完）\n"
    },
    {
      "slug": "slice-how-to-work-in-go",
      "title": "详解Go的slice及其工作原理",
      "date": "2017-06-04",
      "author": null,
      "tags": [
        "go"
      ],
      "content": "\n## 前言\nslice也叫切片，是一种建立在数组类型之上的抽象类型，和数组很像，要理解slice必须先理解数组，这里简单介绍下数组。\n\n在go语言中，数组是一个由特定类型组成的序列，它的长度是固定的，即数组的长度一旦被定义，则不可再对数组中的元素进行添加或删除操作，因此使用起来不太方便。而切片的长度是可变的，这在使用过程中带来了很大的便利，我们也往往使用更加灵活的切片来代替数组。\n\n## Slice\n下面有段程序，我们分别定义一个数组a和切片b，对其每个元素乘以2，然后查看打印结果。\n```\npackage main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    a := [5]int{1, 2, 3, 4, 5}\n    b := []int{1, 2, 3, 4, 5}\n    mulA(a)\n    fmt.Println(a) \n\n    mulB(b)\n    fmt.Println(b)\n}\n\nfunc mulA(a [5]int) {\n    for i, _ := range a {\n        a[i] *= 2\n    }\n}\n\nfunc mulB(a []int) {\n    for i, _ := range a {\n        a[i] *= 2\n    }\n}\n\n```\n\n打印结果：\n```\n[1 2 3 4 5]\n[2 4 6 8 10]\n```\n\n通过打印的结果，我们可以看到，数组a的元素还是原来的值，而切片b的元素被修改了。在go语言中，数组的传递是值传递，所以其值未变，这个大家都知道。那为什么切片的值就被成功修改了呢？我们先弄清楚切片的概念。\n\n> **一个slice由三部分构成：指针、长度和容量，slice的底层引用了一个数组对象，多个slice之间可以共享同一底层数据，指针指向第一个slice元素对应的底层数组元素的地址，要注意的是slice的第一个元素并不一定就是数组的第一个元素。**\n\n这个弄明白，slice基本就懂了。因为slice是对一个底层数组的引用，所以向函数传递slice将允许在函数内部修改slice的值，因此上述切片b的元素被成功修改。当然，我们要使用上述方法修改数组a的值也是可以的，将数组以指针的形式传递给函数，如下：\n```\nfunc main() {\n    a := [5]int{1, 2, 3, 4, 5}\n    mulA(&a)\n    fmt.Println(a) \n\n}\n\nfunc mulA(a *[5]int) {\n    for i, _ := range a {\n        a[i] *= 2\n    }\n}\n\n```\n虽然这样可以实现同样的效果，但一般不建议这么做，因为实现起来更加繁琐，而使用slice则更加便利。\n\n下面再看一个例子：\n```\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    months := [...]string{\n            1: \"January\",\n            2: \"February\",\n            3: \"March\",\n            4: \"April\",\n            5: \"May\",\n            6: \"June\",\n            7: \"July\",\n            8: \"August\",\n            9: \"September\",\n            10: \"October\",\n            11: \"November\",\n            12: \"December\",\n        }\n    Q2 := months[4:7]\n    summer := months[6:9]\n    fmt.Println(Q2)     \n    fmt.Println(summer) \n}\n```\n这里定义了一个包含十二月份的数组，其长度和容量都为13，第0个元素未定义会被自动初始化为空字符串。然后分别定义表示第二季度和北方夏天月份的slice，它们有重叠部分。\n\n打印结果:\n```\n[April May June]\n[June July August]\n```\n\n这里可以看出Q2和summer两个slice的长度为3，那它们的容量又分别是多少呢？也是3吗？\n```\nQ2 := months[4:7]\nsummer := months[6:9]\nfmt.Printf(\"len(Q2)=%v,cap(Q2)=%v\\n\",len(Q2),cap(Q2))\nfmt.Printf(\"len(summer)=%v,cap(summer)=%v\\n\",len(summer),cap(summer))\n```\n\n输出：\n```\nlen(Q2)=3,cap(Q2)=9\nlen(summer)=3,cap(summer)=7\n```\n\n有些同学可能会觉得很奇怪，为什么是这样的结果？我们通过下面的图来看一下Q2和summer是如何取数组切片的:\n![原理图](/img/2017/06/month.png)\n前面我们提到过，slice的底层引用了一个数组对象，而Q2和summer都是从months数组里取的切片，即他们共享同一个底层数组，所以Q2的容量是从Q2的第一个元素Q2[0]即months[4]开始到数组最后一个元素，summer的容量是从summer的第一个元素summer[0]即months[6]开始到数组最后一个元素。\n\nOK，现在我们对summer切片再进行切片操作，看看取不同长度会产生什么样的结果。\n```\n// 从summer切片第0个元素开始，取5个元素\nendlessSummer := summer[:5]   \nfmt.Println(endlessSummer)    \n```\n\n会是这样的结果吗？\n```\n[June July August \"\" \"\"]\n```\n并不是。\n\n运行程序，得到的正确结果如下：\n```\n[June July August September October]\n```\n咦，summer不是只取了months数组的`[June July August]`三个元素吗？为什么summer[:5]会取出5个元素呢？是的，summer确实是只取了底层数组的三个元素，但由于`cap(summer)=7`，所以对summer进行`summer[:5]`操作时会扩展summer切片，即新生成的切片`len(endlessSummer)=5`，又由于共享了底层数组，所以`summer[:5]`会从底层数组中取出5个元素，注意新生成的切片和summer是也是共享同一底层数组的，所以`cap(endlessSummer)=7`\n\n举一反三，如果是这样取切片呢：\n```\n// 从summer切片第0个元素开始，取10个元素\nfmt.Println(summer[:10])\n```\n\n结果会是下面这样吗？\n```\n[June July August September October November December \"\" \"\" \"\"]\n```\n\n也不是。\n\n运行程序时会发现报panic异常，咦，为什么这次没有扩展summer呢？其实是扩展了，但summer的容量只有7，所以只能将summer扩展到7个元素。**在go语言中，超出cap的切片操作将导致panic异常。**\n\n当获取一个切片时，若省略结束值，则其默认是切片的长度而不是容量；若指定结束元素，则可指定为容量：\n```\nfmt.Println(summer[:])    // [June July August]\nfmt.Println(summer[:7])   // [June July August September October November December]\nfmt.Println(summer[:8])   // panic\n```\n\n#### Slice比较\n请看下面的例子，b和c切片取自同一个底层数组，然后比较他们是否相等\n```\nfunc main() {\n    a := [...]int{1, 2, 3, 4, 5}\n    b := a[:]\n    c := a[:]\n    fmt.Println(b==c) //invalid operation: b == c (slice can only be compared to nil)\n}\n```\n\n运行程序会发现直接报错，这是因为在go语言中不能直接比较两个slice是否相等，这是一个不合法的操作（数组是可以直接比较的），要想比较两个slice，我们只能通过间接比较slice中的每个元素来确定slice是否相等。\n```\nfunc equal(x, y []string) bool {\n    if len(x) != len(y) {\n        return false\n    }\n    for i := range x {\n        if x[i] != y[i] {\n            return false\n        }\n    }\n    return true\n}\n```\n\n> **slice之间之所以不能进行比较，是因为slice的元素是间接引用的。slice引用的底层数组的元素随时可能会被修改，即slice在不同的时间可能包含不同的值，所以无法进行比较。**\n\n#### Slice追加\nslice的长度是可变的，意味着我们可以对其添加或删除元素。go提供了一个内置的`append`函数用于向slice追加元素，当超出切片容量时也可以继续追加，那这其中到底是如何实现的呢？我们来一探究竟。\n\n先看下面的例子：\n```\nfunc main() {\n    var x []int\n    for i := 0; i < 10; i++ {\n        x = append(x, i)\n        fmt.Printf(\"%d len=%d,cap=%d\\t%v\\n\", i, len(x), cap(x), x)\n    }\n}\n```\n\n打印结果：\n```\n0 len=1,cap=1\t[0]\n1 len=2,cap=2\t[0 1]\n2 len=3,cap=4\t[0 1 2]\n3 len=4,cap=4\t[0 1 2 3]\n4 len=5,cap=8\t[0 1 2 3 4]\n5 len=6,cap=8\t[0 1 2 3 4 5]\n6 len=7,cap=8\t[0 1 2 3 4 5 6]\n7 len=8,cap=8\t[0 1 2 3 4 5 6 7]\n8 len=9,cap=16\t[0 1 2 3 4 5 6 7 8]\n9 len=10,cap=16\t[0 1 2 3 4 5 6 7 8 9]\n```\n让我们仔细查看i=3时的迭代。当时x包含了[0 1 2]三个元素，但是容量是4，因此可以简单将新的元素添加到末尾，不需要新的内存分配，此时x的长度和容量都是4。\n\n当i=4，现在x没有多余的空间来存放新的元素了，但此时x的长度变成5，容量变成8。由此可知，当向切片追加元素时，若没有多余空间存放新的元素，go会自动扩展切片，将切片容量直接增加一倍。当i=8时，切片容量也由8变为16，扩大了一倍。\n\n这里有个问题是无法确认append函数在增加切片容量时是否分配了新的内存，所以往往将追加元素后的切片值赋给原来切片变量：`slice := append(slice, s)`\n\n更新slice变量不仅对调用append函数是必要的，实际上对应任何可能导致长度、容量或底层数组变化的操作都是必要的。要正确地使用slice，需要记住尽管底层数组的元素是间接访问的，但是slice对应结构体本身的指针、长度和容量部分是直接访问的。从这个角度看，slice并不是一个纯粹的引用类型，它实际上是一个类似下面结构体的聚合类型(摘自go语言圣经)：\n```\ntype IntSlice struct {\n    ptr      *int\n    len, cap int\n}\n```\n\n官方没有提供append函数的源码，这里我们基于以上原理自己实现了一个类似append的函数：\n```\nfunc appendInt(x []int, y int) []int {\n    var z []int\n    zlen := len(x) + 1\n    if zlen <= cap(x) {\n        z = x[:zlen]\n    } else {\n        zcap := zlen\n        if zcap < 2*len(x) {\n            zcap = 2 * len(x)\n        }\n        z = make([]int, zlen, zcap)\n        copy(z, x) \n    }\n    z[len(x)] = y\n    return z\n}\n```\n每次调用appendInt函数时，会先检测slice底层数组是否有足够的容量来保存新添加的元素。如果有足够空间，则直接在原有的底层数组基础上扩展slice，将新添加的y元素复制到新扩展的空间，并返回slice。因此，输入的x和输出的z共享相同的底层数组。\n\n如果没有足够的增长空间，即追加元素后slice的长度大于容量，此时appendInt函数会创建一个新的slice，其长度为原有slice追加元素后的长度，容量为长度的两倍，然后将原有的slice复制到新的slice，最后添加y元素。因此，结果z和输入的x引用的将是不同的底层数组。\n\n#### Slice删除元素\ngo没有提供直接删除slice元素的函数，要删除slice中的元素，实现方法比较有意思。\n\n要删除slice中间的某个元素并保存原有的元素顺序，可以通过内置的copy函数将后面的子slice向前依次移动一位，然后删除切片到倒数第二个元素：\n\n```\nfunc remove(x []int, i int) []int {\n    copy(x[i:], x[i+1:])\n    return x[:len(x)-1]\n}\n\nfunc main() {\n    s := []int{1, 2, 3, 4, 5}\n    fmt.Println(remove(s, 2))\n}\n```\n\n打印结果：\n```\n[1 2 4 5]\n```\n\n如果删除元素后不用保持原来顺序的话，我们可以直接用最后一个元素覆盖被删除的元素：\n\n```\nfunc remove(x []int, i int) []int {\n    x[i] = x[len(x)-1]\n    return x[:len(x)-1]\n}\n\nfunc main() {\n    s := []int{1, 2, 3, 4, 5}\n    fmt.Println(remove(s, 2))\n}\n```\n\n打印结果：\n```\n[1 2 5 4]\n```\n\n#### Slice内存技巧\n比如我们要去除一个slice中的空字符串：\n```\npackage main\n\nimport \"fmt\"\n\nfunc nonempty(strings []string) []string {\n    i := 0\n    for _, s := range strings {\n        if s != \"\" {\n            strings[i] = s\n            i++\n        }\n    }\n    return strings[:i]\n}\n```\n这里直接在原有的slice上进行修改，输入和输出的slice共享同一个底层数组，避免了重新分配内存。因此我们通常会这样使用nonempty函数：data = nonempty(data)。\n\nnonempty函数也可以使用append函数实现：\n```\nfunc nonempty2(strings []string) []string {\n    out := strings[:0] \n    for _, s := range strings {\n        if s != \"\" {\n            out = append(out, s)\n        }\n    }\n    return out\n}\n```\n\n## 总结\n在实际开发过程中，使用切片往往比数组会更有优势，在某些情况下也能够节省内存空间，但同时要注意修改切片会同时修改掉所引用的底层数组的数据。\n\n（完）\n"
    },
    {
      "slug": "best-practices-for-writing-dockerfiles",
      "title": "编写Dockerfile最佳实践",
      "date": "2017-04-06",
      "author": null,
      "tags": [
        "docker"
      ],
      "content": "\nDocker可以通过从`Dockerfile`读取指令来自动构建镜像，`Dockerfile`是一个包含构建给定镜像所需指令的文本文件，它遵循特定的格式并使用一组特定的指令。\n\n## 一般准则和建议\n#### 容器应该是短暂的\n由`Dockerfile`定义的镜像生成的容器应尽可能短暂。对于“短暂”这个词，我们指的是容器可以被停止和销毁，并且使用最少的设置和配置来创建一个新的容器。如果你想了解在这样一个无状态方式下运行容器的机制，可以看看应用程序方法[12要素](https://12factor.net/zh_cn/)的Processes部分。\n\n#### 使用`.dockerignore`\n在大多数情况下，最好将`Dockerfile`放在一个空目录中，然后只添加`Dockerfile`构建镜像所需的文件。要提升构建的性能，你也可以添加一个`.dockerignore`文件到该目录来排除一些文件和目录，它的用法类似于`.gitignore`。\n\n#### 避免安装不必要的包\n为了减少镜像的复杂性、依赖性、大小和构建时间，你不应该为了方便使用而安装不必要的包。例如，在一个数据库镜像中安装VIM非常没有必要的。\n\n#### 每个容器应该只有一个进程\n将应用程序解耦到多个容器中可以更方便地对容器进行水平扩展和重用。例如，一个Web应用可能由三个独立的容器组成，每个容器都拥有自己的镜像，这种做法就是以解耦的方式来管理Web应用、数据库和缓存。\n\n你可能已经听说`一个容器一个进程`的经验法则，虽然其意图很好，但并不是说每个容器只能有一个进程。事实上，除了可以使用init进程产生容器外，一些程序可能会自行产生其他的进程，例如，Apache可以根据每个请求来创建一个进程。通常来讲，`一个容器一个进程`是很好的经验法则，但不是固定死的，你应该根据自己的最佳的判断来保持容器尽可能干净、模块化。\n\n如果容器之间相互依赖，你可以使用`Docker container networks`来确保容器之间可以互相通信。\n\n#### 最小化镜像层\n在编写`Dockerfile`时，你需要综合考虑`Dockerfile`文件的可读性（为了长期维护）和最小化镜像层数。\n\n#### 对多行参数排序\n在任何情况下，尽可能地通过首字母来排序多行参数，这可以让你避免安装重复的包，使得参数列表更容易维护。在反斜杠`\\`前添加一个空格也会让`Dockerfile`更易阅读和review。\n\n如下面的例子：\n```ruby\nRUN apt-get update && apt-get install -y \\\n    bzr \\\n    cvs \\\n    git \\\n    mercurial \\\n    subversion\n```\n\n#### 构建缓存\n在构建镜像的过程中，Docker将按照指定的顺序逐步执行`Dockerfile`中的指令。每执行一条指令前，Docker都会在其缓存中查找是否有可重用的镜像，而不是创建一个新的（重复）镜像。如果你不想使用缓存，可以在`docker build`命令中使用`--no-cache = true`选项来强制重新构建。\n\n但是，如果你确实要让Docker使用缓存，那么了解何时会找到匹配的镜像非常重要。 Docker查找镜像缓存时将遵循以下基本原则：\n* 从已经存在缓存中的基础镜像开始，将下一个指令与从该基础镜像导出的所有子镜像进行比较，看其中是否有使用完全相同指令构建的子镜像，如果没有，则缓存无效。\n* 在大多数情况下，只需将`Dockerfile`中的指令与其中一个子镜像比较即可，但是，某些指令需要更进一步的检测是否匹配。\n* 对于`ADD`和`COPY`指令，将检查镜像中文件的内容，并计算每个文件的校验和，校验和的计算不包括文件的最后修改和最后访问时间。在查找镜像缓存时，将校验和与现有镜像的校验和进行比较，如果文件（如内容和元数据）中有任何变化，则缓存无效。\n* 除了`ADD`和`COPY`指令，其他指令在查找缓存镜像时不会通过检查容器中的文件的方式来匹配缓存。例如，在处理`RUN apt-get -y update`命令时，将不会检查在容器中文件是否更新来确定是否命中缓存。在这种情况下，只需要通过检查命令字符串本身是否改变即可查找匹配的缓存。\n\n注意：一旦缓存无效，则`Dockerfile`中所有后续的指令将不再使用缓存，而是重新生成新的子镜像。\n\n## Dockerfile指令\n接下来的内容是关于在`Dockerfile`中使用各个指令的最佳方式（只列出了一些有用的部分）。\n\n#### FROM\n尽可能的使用官方镜像作为基础镜像。\n\n#### LABEL\n你可以为镜像添加标签，这将为你按项目组织镜像、记录许可信息、自动化等起到帮助。每个标签可以添加一个以`LABEL`开头的行和一个或多个键值对。\n\n> **注意：**如果你的字符串包含空格，那么你必须使用引号来包裹或者对空格进行转义，如果字符串内部包含引号，也要进行转义。\n\n```ruby\n# 设置一个或多个标签\nLABEL com.example.version=\"0.0.1-beta\"\nLABEL vendor=\"ACME Incorporated\"\nLABEL com.example.release-date=\"2015-02-12\"\nLABEL com.example.version.is-production=\"\"\n\n# 在一行设置设置多个标签\nLABEL com.example.version=\"0.0.1-beta\" com.example.release-date=\"2015-02-12\"\n\n# 一次设置多个标签，使用`\\`符号来连接\nLABEL vendor=ACME\\ Incorporated \\\n      com.example.is-beta= \\\n      com.example.is-production=\"\" \\\n      com.example.version=\"0.0.1-beta\" \\\n      com.example.release-date=\"2015-02-12\"\n```\n\n#### RUN\n通常来讲，为了使`Dockerfile`更易阅读、易于理解、容易维护，请使用反斜杠`\\`将一行复杂的`RUN`命令分隔成多行。\n\n#### APT-GET\n`RUN`指令最常用的情形应该是应用程序的`apt-get`，在使用`RUN apt-get`命令安装软件包时，有几个地方需要注意一下。\n\n你应该避免使用`RUN apt-get upgrade`或`dist-upgrade`，因为基础镜像中的许多必需软件包无法在无权限容器内升级。如果基础镜像中包含的软件包过期了，你应该联系该镜像的维护人员。如果你知道有一个特定的包需要更新，请使用`apt-get install -y`来自动更新。\n\n请务必将`RUN apt-get update`与`apt-get install`组合在同一个`RUN`语句中，例如：\n```ruby\nRUN apt-get update && apt-get install -y \\\n    package-bar \\\n    package-baz \\\n    package-foo\n```\n\n在`RUN`语句中单独使用`apt-get update`会导致缓存问题，并且其后的`apt-get install`指令会失败。例如，假设你有这样一个`Dockerfile`文件：\n```ruby\nFROM ubuntu:14.04\nRUN apt-get update\nRUN apt-get install -y curl\n```\n\n构建镜像后，所有镜像层都已经缓存到Docker中，假设你以后要修改`Dockerfile`中的`apt-get install`来安装其他的包：\n```ruby\nFROM ubuntu:14.04\nRUN apt-get update\nRUN apt-get install -y curl nginx\n```\nDocker会把最初的`apt-get update`和修改后的`apt-get update`当做同样的指令，并且使用之前的缓存镜像，导致`apt-get update`不会执行，所以可能会安装比较旧的curl和nginx包版本。\n\n而使用`RUN apt-get update && apt-get install -y`可确保安装的软件包版本是最新的，无需进一步的编码或手动干预，这种方式被称为`缓存破解`。你也可以通过一种叫做`版本锁定`的方式来指定包版本以达到同样的目的，例如：\n```ruby\nRUN apt-get update && apt-get install -y \\\n    package-bar \\\n    package-baz \\\n    package-foo=1.3.*\n```\n不论镜像缓存是否存在，`版本锁定`都会强制获取指定版本的包来构建镜像，这种方式还可以降低由于所需软件包的意外更改而导致构建失败的几率。\n\n以下是一个良好格式的`RUN`指令示例：\n```ruby\nRUN apt-get update && apt-get install -y \\\n    aufs-tools \\\n    automake \\\n    build-essential \\\n    curl \\\n    dpkg-sig \\\n    libcap-dev \\\n    libsqlite3-dev \\\n    mercurial \\\n    reprepro \\\n    ruby1.9.1 \\\n    ruby1.9.1-dev \\\n    s3cmd=1.1.* \\\n&& rm -rf /var/lib/apt/lists/*\n```\n\ns3cmd指令指定了1.1.\\*版本。如果镜像以前使用的是老版本，则指定新版本会让`apt-get update`镜像层缓存失效，并确保新版本的安装。\n\n另外，通过删除`/var/lib/apt/lists`可以清理apt缓存，因此apt缓存不会存储于镜像层中，也就减小了镜像大小。由于`RUN`语句以`apt-get update`开头，所以在执行`apt-get install`之前，包缓存将始终被刷新。\n\n#### CMD\n`CMD`指令被用于运行包含在镜像中的软件和参数，它几乎总是以`CMD [“executable”, “param1”, “param2”…]`的形式调用。因此，对于服务类型的镜像，例如Apache和Rails，则可以这样运行`CMD [“apache2”，“-DFOREGROUND”]`。实际上，这种形式的指令也是推荐用于任何基于服务的镜像的。\n\n在大多数情况下，应该给`CMD`一个交互式的shell，如bash，python和perl。例如，`CMD [\"perl\", \"-de0\"]`, `CMD [\"python\"]`或`CMD [“php”, “-a”]`，使用这种形式就意味着当你执行像`docker run -it python`这样的操作时，进入容器后将处于可用的shell中。尽量不要将`CMD`以`CMD [“param”，“param”]`的形式与`ENTRYPOINT`一起使用，除非你非常熟悉`ENTRYPOINT`的工作原理。\n\n#### EXPOSE\n`EXPOSE`指令指明容器将监听用于连接的端口，因此，你应该为应用程序使用通用的、默认的端口，例如，包含Apache web服务器的镜像应该使用`EXPOSE 80`，而包含MongoDB的镜像应该使用`EXPOSE 27017`等。\n\n#### ADD或COPY\n虽然`ADD`和`COPY`在功能上相似，但通常优先使用`COPY`，因为它比`ADD`更直观。`COPY`只支持将本地文件复制到容器中，而`ADD`具有一些隐藏的功能（如本地的tar提取和远程URL支持），因此，`ADD`最适合用于将本地tar文件自动提取到镜像中，如`ADD rootfs.tar.xz /`。\n\n如果你的`Dockerfile`需要使用上下文中的多个文件，请单独使用`COPY`多次，而不是一次`COPY`，因为如果指定的文件更改了，这可以确保每一步的构建缓存失效（即强制重新构建）。\n\n由于镜像大小很重要，因此不应该使用`ADD`从远程URL获取包，而应该用`curl`或`wget`来代替，这样你就可以删除在解压后不再需要的文件，也就不会在镜像中添加另一个镜像层。例如，你不应这样做：\n```ruby\nADD http://example.com/big.tar.xz /usr/src/things/\nRUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/things\nRUN make -C /usr/src/things all\n```\n而应该使用下面的方式来代替：\n```ruby\nRUN mkdir -p /usr/src/things \\\n    && curl -SL http://example.com/big.tar.xz \\\n    | tar -xJC /usr/src/things \\\n    && make -C /usr/src/things all\n```\n\n对于不需要用到`ADD`自动提取功能的一些项目（如文件，目录），应该始终使用`COPY`指令。\n\n#### USER\n如果一个服务可以在无特定权限下运行，请使用`USER`指令来切换到非root用户，如果要创建用户和组，在`Dockerfile`中请使用`RUN groupadd -r postgres && useradd -r -g postgres postgres`的形式来创建。\n\n你应该避免安装或使用`sudo`，因为一些无法预期的行为可能会导致更多问题。如果你确实要使用类似于`sudo`的功能（例如，以root用户身份初始化守护程序，但以非root身份运行），则可以使用`gosu`。\n\n最后，为了减少镜像层和复杂性，不要频繁地使用`USER`切换用户。\n\n#### WORKDIR\n为了清晰和可靠，`WORKDIR`应该始终使用绝对路径，而且，你应该使用`WORKDIR`来切换目录，而不是像`RUN cd ... && do-something`这些难以阅读和维护的命令。\n\n本文译自官方文档：[Best practices for writing Dockerfiles](https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/)，译者水平有限，有翻译差错请指正。\n\n（完）\n"
    },
    {
      "slug": "the-providers-of-angularjs",
      "title": "AngularJS之Providers",
      "date": "2017-03-26",
      "author": null,
      "tags": [
        "javascript"
      ],
      "content": "\n最近因为工作需要接触到了AngularJS，在看到Providers的时候很迷惑，对factory、service和 provider 不太理解，也不知道什么时候该用哪一个来创建服务，后面通过阅读文档和查阅大量资料才弄清楚，所以在这里记录一下他们的用法和区别。\n\n## 用法\nAngularJS内置了很多服务，如`$scope`、`$http`、`$location`、`$q`等，我们可以通过依赖注入的方式来使用各个服务，其实这些服务可以看做是 angular 封装好的一些函数，在任何地方都可以方便地调用。如下面的例子：\n\n```javascript\nvar app = angular.module('myApp', [])\napp.controller('myController', ['$scope', '$http', function($scope, $http) {\n    var url = 'http://localhost';\n    $scope.title   = 'my title';\n    $scope.content = $http.get(url);\n}])\n```\n\n当然，在开发过程中，我们也往往有一些代码需要封装，以达到重用的目的，这个时候就需要我们自定义一些服务了。\n\nangularjs的Providers提供了`constant`、`value`、`factory`、`service`和`provider`五种方法来让我们创建自己的服务，下面我们来看看这五种方法有什么区别以及什么时候该使用哪种方法来创建服务。\n\n## 区别\n\n#### value\n这个方法用于定义一些仅包含简单字符串的服务，比如下面定义一个名为MYHOST的服务，他的值是一个主机地址：\n\n```javascript\nvar app = angular.module('myApp', [])\napp.value('MYHOST', 'http://localhost/')\n```\n\n然后在controller中你可以这样来使用MYHOST服务：\n\n```javascript\napp.controller('myController', ['$scope', 'MYHOST', function($scope, MYHOST) {\n    $scope.host = MYHOST\n}])\n```\n\n当然，你也可以在定义其他服务时使用MYHOST，如下面的例子：\n```javascript\napp.factory('myFactory', ['$scope', 'MYHOST', function($scope, MYHOST) {\n    var service = {};\n    service.host = MYHOST;\n    service.content = function() {\n        // do something\n    };\n\n    return service;\n}])\n```\n**注意： value 方法创建的服务的值在程序中可以被重新赋值，但该服务不能在配置和程序运行阶段使用。**\n\n#### angular生命周期\n这里先解释下angular的生命周期。\n\nangular程序的生命周期可以为两部分：`配置阶段`和`运行阶段`。配置阶段就是angular程序创建服务前的一些初始化和实例化的操作，这时，controller所引用的各个服务是不可用的，因为他们还没有被创建。配置阶段一旦完成，angular程序就开始创建服务，也就是开始运行阶段。\n\n\n#### constant\n该方法也是用于定义仅包含简单字符串的服务的，如下面的例子:\n\n```javascript\nvar app = angular.module('myApp', [])\napp.constant('MYHOST', 'http://localhost/')\n```\n\n然后在controller中使用该服务\n```javascript\napp.controller('myController', ['$scope', 'MYHOST', function($scope, MYHOST) {\n    $scope.host = MYHOST\n}])\n\n```\n\n看到这里，你是不是觉得他和 value 是一样的？没错，用法确实是一样的，但还是有区别的。\n\n constant 与 value 有以下两点区别：\n*  constant 定义的服务在程序中是不能改变的（通过其方法名`constant`就可以看出），而 value 可以。\n*  constant 定义的服务可以在配置阶段使用，而 value 不可以。\n\n看下面的例子，如果你想要在config中使用MYHOST服务，那么你不能使用 value 来定义该服务，而应该用 constant 。\n\n```javascript\nvar app = angular.module('myApp', [])\napp.constant('MYHOST', 'http://localhost/')\n```\n\n```javascript\napp.config('MYHOST', ['$scope', 'MYHOST', function($scope, MYHOST) {\n    $scope.host = MYHOST\n}])\n```\n\n> 虽然 value 和 constant 使用起来非常简单，但他们也只能用于定义一些简单特性的服务，如果要定义比较复杂的服务就需要用到factory、service、provider等。\n\n#### factory\n对于 value 和 constant 定义的服务，同样可以用更高级的factory、service、provider方法来实现，非常简单，这里只介绍 factory 。\n\n```javascript\napp.factory('MYHOST', function() {\n    return 'http://localhost';\n})\n```\n\n很明显，使用 value 和 constant 来定义这个服务更加适合。\n\n factory 定义服务的过程往往是先定义一个空的对象，然后给这个对象添加一些你所需要的属性或方法，最后返回这个对象，下面我们创建一个功能多点、复杂点的服务。\n\n```javascript\napp.factory('myFactory', ['$http', 'MYHOST', function($http, MYHOST) {\n    var service = {};\n    service.prefix  = MYHOST;\n    service.getData = function() {\n        // getdata from remote by $http\n    }\n\n    return service;\n}])\n```\n\nOK，我们创建了一个包含 prefix 属性和 getData() 方法的服务，服务名是myFactory，现在我们可以在其他地方通过注入的方式使用这个服务了。\n\n```javascript\napp.controller('myController', ['$scope', 'myFactory', function($scope, myFactory) {\n    $scope.myPrefix = myFactory.prefix;\n    $scope.myData   = myFactory.getData();\n}])\n```\n\n#### service\n可能大部分人容易混淆的是 service 和 factory ，不知道使用哪一个来创建服务更加合适。\n\n service 和 factory 的区别就在于 service 可以看做是一个类，而 factory 看做是个function，所以 factory 里面需要return出来，而 service 不用， service 是直接通过this来赋值的。\n\n```javascript\napp.service('myService', ['$http', 'MYHOST', function($http, MYHOST) {\n    this.prefix  = MYHOST;\n    this.getData = function() {\n        // getdata from remote by $http\n    }\n}])\n```\n\n在controller中调用 service \n\n```javascript\napp.controller('myController', ['$scope', 'myService', function($scope, myService) {\n    $scope.myPrefix = myService.prefix;\n    $scope.myData   = myService.getData();\n}])\n```\n\n到这里你可能会感到奇怪，既然 service 是一个类，为什么调用的时候没有进行实例化呢？这是因为angular会自动通过`new`关键词调用构造函数来创建对象。\n\n> 通过比较 factory 和 service ，我们可以看出两者其实并没有什么差别，只是一个通过函数式的方式实现，一个通过类的方式实现，具体使用哪一个不必太过深究，根据自己的编码习惯，喜欢使用哪个就使用哪个。\n\n#### provider\n这个方法是这几个创建服务的方法中最核心最全面的，事实上，其他四个仅仅是他的语法糖，每个 provider 必须要有一个`$get`方法。\n\n该方法与 factory 和 service 最主要的区别在于他创建的服务可以在配置阶段重新赋值。\n\nangular提供了一个叫config的方法，可以在配置阶段改变某些服务的值，下面我们通过一个例子来看看如何在配置阶段改变 provider 中定义的服务的值。\n\n```javascript\napp.provider('myProvider', function() {\n    var myHost = 'http://localhost';\n    this.modifyHost = function(host) {\n        myHost = host;\n    }\n    this.$get = function() {\n        var service = {};\n        if (myHost == 'http://localhost') {\n            service.title = 'your host is localhost, dont\\'t you modify?';\n        } else {\n            service.title = 'your host is ' + myHost;\n        }\n\n        return service;\n})\n```\n\n在配置阶段对myHost重新赋值\n\n```javascript\napp.config('myProviderProvider', function(myProviderProvider) {\n    myProviderProvider.modifyHost('http://GaryFeng.com')\n})\n```\n\n重新赋值后，我们在controller中使用myProvider服务\n```javascript\napp.controller('myController', ['$scope', 'myProvider', function($scope, myProvider) {\n    $scope.myHost = myProvider\n}])\n```\n将会得到结果`your host is http://GaryFeng.com`\n\n可能大家注意到，config中注入myProvider时多加了一个 provider ，为什么呢？这是为了识别 provider 服务。\n\n到这里，其实可以发现， provider 服务其实是通过`$get`来实现的。\n\n## 总结\n对于大多数服务来说，是不需要使用 provider 的，用 factory 或 service 就已经足够了，那么什么时候才用 provider 呢？**只有你想要在配置阶段动态修改某个服务的值的时候才使用。**\n\n到这里，相信大家已经很清楚了，其实这几个创建服务的方法并没有实质上的区别，只要你乐意，你想怎么用就怎么用，想用哪个就用哪个，只是用哪个更加方便而已。\n\n（完）\n\n"
    },
    {
      "slug": "using-imagecopy-lost-transparency",
      "title": "PHP使用imagecopy添加水印丢失透明度的问题",
      "date": "2017-02-25",
      "author": null,
      "tags": [
        "php"
      ],
      "content": "\n## 问题描述\n最近在工作中遇到一个很奇怪的问题，在使用php GD库的`imagecopy()`给图片添加带有透明度的图片水印时，水印的透明度变得不透明了，而且在不同浏览器上查看加水印后的图片，显示效果也不一样。\n\n我不知道是不是GD库和php版本的问题，php => 5.6.3，GD => bundled (2.1.0 compatible)。\n\n## 问题重现\n我要给目标图片加上一个水印，根据以往的实现方法一般都是通过GD库的`imagecopymerge()`或`imagecopy()`函数来实现的，这两个函数的作用是一样的：拷贝并合并图像的一部分。\n\n这里简单介绍一下它们的区别：`imagecopymerge()`比`imagecopy()`多了一个参数$pct，这个参数用来设置水印的透明度，取值是从0到100的整数，值越大透明度越高。当$pct=0，即不透明，实际上什么也没做；当$pct=100时，表示完全透明，此时该函数和`imagecopy()`完全一样。\n\n因为我的水印图背景是完全透明的，所以我使用的是`imagecopy()`，也建议大家在添加背景完全透明的水印时使用此函数而不是`imagecopymerge()`。\n\n水印图（png格式）：\n![Alt text](/img/2017/02/ths-logo.png)\n目标图片（gif格式）：\n![Alt text](/img/2017/02/destination.PNG)\n\n以下是实现代码：\n\n```php\n<?php\n$dstPath = '/path/destination.gif';\n$srcPath = '/path/ths-logo.png';\n$dst = imagecreatefromgif($dstPath);\n$src = imagecreatefrompng($srcPath);\nlist($dstWidth, $dstHeight) = getimagesize($dstPath);\nlist($srcWidth, $srcHeight) = getimagesize($srcPath);\n$dstX = $dstWidth - $srcWidth - 20;\n$dstY = $dstHeight - $srcHeight - 70;\nimagecopy($dst, $src, $dstX, $dstY, 0, 0, $srcWidth, $srcHeight);\nimagepng($dst);\nimagedestroy($dst);\nimagedestroy($src);\nheader('Content-Type:image/png');\nexit;\n```\n\n最终生成的带水印图在chrome、firefox、IE上显示效果分别如下：\n![Alt text](/img/2017/02/chrome.PNG)\n![Alt text](/img/2017/02/firefox.PNG)\n![Alt text](/img/2017/02/IE.PNG)\n\n可以看到，三种浏览器上显示的效果都不一样，原来有透明背景的水印变得不透明了，生成的图片没有达到预期。\n\n将上述代码中`imagecopy()`换为`imagecopymerge()`，如下：\n```php\nimagecopymerge($dst, $src, $dstX, $dstY, 0, 0, $srcWidth, $srcHeight, 100);\n```\n\n生成的效果图在chrome、firefox、IE下：\n![Alt text](/img/2017/02/chrome-3.PNG)\n惨不忍睹，字都看不到了。\n\n为什么原来是透明背景的水印会变得不透明呢？这个问题我也想不通。\n\n## 解决方案\n遇到问题总要解决啊，于是开始寻找解决方案，Google搜了一堆也没有遇到有和我一样问题的，都是使用`imagecopy()`代替`imagecopymerge()`就解决了，可我就是用的`imagecopy()`啊，wtf!\n\n皇天不负有心人，在看文档的过程中，我发现了`imagecolortransparent()`这个函数，它的作用是将某个颜色定义为透明色，将某个颜色定义为透明色！将某个颜色定义为透明色！看到这里我突然想到，如果我先将水印图添加到一张同样大小的白色背景图上，然后再通过`imagecolortransparent()`将白色背景定义为透明色，再把这张图当做水印添加到目标图片上不就可以了吗？！想到这里我激动不已，马上开始实现看看是不是我要的效果，以下是实现过程：\n\n```php\n<?php\n$dstPath = '/path/destination.gif';\n$srcPath = '/path/ths-logo.png';\n$dst = imagecreatefromgif($dstPath);\n$src = imagecreatefrompng($srcPath);\nlist($dstWidth, $dstHeight) = getimagesize($dstPath);\nlist($srcWidth, $srcHeight) = getimagesize($srcPath);\n$dstX = $dstWidth - $srcWidth - 20;\n$dstY = $dstHeight - $srcHeight - 70;\n// 创建一个同样大小的白色背景图像\n$im = imagecreatetruecolor($srcWidth, $srcHeight);\n$white = imagecolorallocate($im, 255, 255, 255);\nimagefill($im, 0, 0, $white);\n// 将白色定义为透明色\nimagecolortransparent($im, $white);\n// 将水印图添加到白色背景图上\nimagecopy($im, $src, 0, 0, 0, 0, $srcWidth, $srcHeight);\n// 将新的水印图添加到目标图片上\nimagecopy($dst, $im, $dstX, $dstY, 0, 0, $srcWidth, $srcHeight);\nimagepng($dst);\nimagedestroy($im);\nimagedestroy($dst);\nimagedestroy($src);\nheader('Content-Type:image/png');\nexit;\n```\n\n运行以上代码后，生成的图片在chrome、firefox、IE上显示效果都一样，如下：\n![Alt text](/img/2017/02/IE.PNG)\n生成的图片显示效果比原来好多了，但并没有达到我的预期，水印背景仍然是不透明的，白色背景挡住了目标图像。\nwtf again!不应该啊，照上面的逻辑，水印图应该是透明背景才对啊，为什么是白色的？？我没想明白为什么，但我想到了`imagecopymerge()`，于是我尝试这将最后一步的`imagecopy()`换成`imagecopymerge()`，代码如下：\n\n```php\nimagecopymerge($dst, $im, $dstX, $dstY, 0, 0, $srcWidth, $srcHeight, 100);\n```\n\n换掉后再运行以上代码，哈哈，没想到居然成功了，生成的图片水印是透明的，实现了我想要的效果。如下图：\n![Alt text](/img/2017/02/chrome-2.PNG)\n\n## 总结\n这个问题最终是解决了，但仍然有一点疑惑。对于`imagecopy()`和`imagecopymerge()`这两个函数，虽然官方文档是那样解释的，但实际使用起来，为什么结果会不一样呢？有哪位同仁知道的还请不吝赐教，感激不尽。\n最后遇到问题还是要多看文档，说不定就打个激灵灵机一动问题就解决了呢。\n\n（完）\n\n"
    },
    {
      "slug": "annual-plan",
      "title": "2017年度计划",
      "date": "2017-02-12",
      "author": null,
      "tags": [
        "生活"
      ],
      "content": "\n\n本来不打算再写年度计划的，因为公司年终总结的时候一起写了，但仔细想想个人计划和工作计划还是得分开，所以在个人博客上再写一份个人年度计划。\n\n## 技术\n\n换工作大半年了，也学到了不少东西，但很多东西都不够深入，要学的东西实在太多，以至于之前一度不知道要重点学习什么，后来和我们老大谈过后，决定以后的学习重点还是根据工作需要来安排：\n\n#### 设计模式\n因为最近写了一些偏底层的代码，感觉自己在设计模式方面的知识还是过于欠缺，在代码重构方面也不知道如何下手，所以这方面的知识必须要深入学习并在工作中尝试使用。\n\n#### Docker\n学习Docker完全是因为工作需要，Docker作为一门新技术虽然还没有得到普及，但我感觉这将是未来的趋势。\n\n#### 消息队列，分布式应用 \n工作一年半，目前的技术水平只能算是一个初级程序员，消息队列，分布式应用，这些并没有深入的了解，今年要学习并深入了解。\n\n#### 微信小程序\n做一个微信小程序，具体什么内容还没确定。（个人觉得微信小程序只是昙花一现，并翻不起什么大风大浪）\n\n#### 学习一门语言\ngo\n\n#### 博客更新\n博客很早就搭好了，也就在公司小组内部做分享的时候，顺手写了一篇关于composer的博客，自此就没有更新过了，这是第二篇，以后要做到每个月至少一篇。\n\n#### 技术书籍\n《Redis 实战》<br>\n《Docker进阶与实践》<br>\n《RESTful Web APIs》<br>\n《重构 改善既有代码的设计》<br>\n\n后面会根据阅读书籍不断更新。\n\n\n## 生活\n\n#### 读书\n一个月读一本书（非技术），前年买的kindle，以为买了会督促自己去看书，结果头两个月看了两本就搁一边吃灰去了，因为没什么时间来看闲书，今年争取每个月读一本，每读完一本会同时更新在这篇博客上。\n\n#### 摄影\n一直以来觉得自己的个人生活实在是太过单调（可能是缺女朋友的缘故），所以今年打算将摄影作为业余兴趣爱好，丰富个人生活，提高生活质量。其实从上大学开始就想学摄影，可惜那时候没钱买单反，只能手机上拍拍，自认为在取景、拍照角度和曝光上拿捏的还过得去。要是有时间的话，顺便再看看PS，处理下后期图像。\n\n#### 健身\n活了20多年，体重没有超过115，夏天都不好意思穿短袖，太瘦了。去年在公司健身房坚持了小半个月，后来因为各种原因放弃了（主要还是懒），今年计划增重到115~120，坚持一周去三次健身房。\n\n## 最后\n最后，重中之重，今年争取找到女盆友。\n\n（完）\n"
    },
    {
      "slug": "composer-introduction",
      "title": "Composer使用与原理分析",
      "date": "2016-10-27",
      "author": null,
      "tags": [
        "php"
      ],
      "content": "\n\n## 什么是composer\nComposer 是 PHP 的一个依赖管理工具。它允许你申明项目所依赖的代码库，并根据所声明的依赖在项目中为你安装他们。composer不是包管理工具，因为真正的包位于packagist和github上面，composer相当于中间介质帮你安装你所声明的包，所以composer是一个依赖管理工具。\n\n## 安装composer\ncomposer需要php>=5.3.2，安装之前请确保你已经安装php，且版本不低于5.3.2。\ncomposer支持多平台，这里只介绍linux上安装，我们使用[composer中国全量镜像](http://pkg.phpcomposer.com/#how-to-install-composer)上提供的安装方式，执行以下命令：\n`php -r \"readfile('https://getcomposer.org/installer');\" > composer-setup.php`\n`php composer-setup.php`\n`php -r \"unlink('composer-setup.php');\"`\n\n这时composer就已经下载好了，你会在当前目录找到一个composer.phar二进制文件，执行`php composer.phar` 即可看到composer相关信息和一些命令的用法，但是往往我们不会这么使用composer，而是通过`composer + 命令`这样使用，这里我们需要对composer进行全局安装，将composer.phar文件移动到PATH变量的某个目录中即可，如`mv composer.phar /usr/local/bin/composer`，这样你就可以在任意目录执行`composer`使用composer了。\n<img src=\"/img/composer-start.png\" alt=\"composer相关信息\">\n\n## 如何使用composer\n\n1. 创建composer.json文件\n2. 执行composer install安装依赖包\n3. 在php文件中引入依赖，require vendor/autoload.php\n\n> **注：**这里可以不用创建composer.json文件，直接通过`composer require 供应商/包名`即可安装依赖包，也推荐这样使用，更加方便。\n\n#### 配置资源镜像\n由于默认的包是从国外的服务器上下载下来的，下载速度很慢，在使用composer之前请将资源下载地址配置到国内的镜像服务器上，执行：\n`composer config -g repo.packagist composer https://packagist.phpcomposer.com`\n如果不加-g参数，请确保当前目录存在一个合法的composer.json文件，且该配置只对当前composer.json有效。\n<img src=\"/img/config.png\" alt=\"配置信息\">\n\n#### demo\n这里我用一个例子来演示下，比如我的项目中现在需要一个解析配置文件的通用方法，我在github上找到一个包[hassankhan/config](https://github.com/hassankhan/config)，它支持ini,xml,json,yaml,php等多种文件格式的解析(该包的具体使用方法请自行查看)，下面我来安装这个包到我的项目中并解析获取一个ini配置文件的参数。\n首先，在项目根目录下建立一个包含以下内容的composer.json文件，声明需要依赖的包。\n\n```json\n{\n    \"require\": {\n        \"hassankhan/config\": \"0.10.*\"\n    }\n}\n```\nOK，执行`composer install`，然后再创建一个config.ini和index.php文件：\n\n```ini\n# config.ini\n[mysql]\nuser=root\npassword=root\nport=3306\n\n[mongodb]\nuser=root\npassword=root\nport=27017\n```\n\n```php\n<?php\n// index.php\nrequire ('vendor/autoload.php');\nuse Noodlehaus\\Config;\n\n$conf = Config::load('config.ini');\necho $config->get('mysql.port') . PHP_EOL;\n```\n\n在index.php中，我要获取config.ini中mysql区块下的port值，执行`php index.php`，得到以下结果，至此，我们成功的引用了这个包到我们的项目中，so easy!\n<img src=\"/img/result.png\" alt=\"3306\">\n\n## composer 的原理\n通过上面的demo，我们已经知道如何使用composer了，但它是如何工作的呢？现在来分析下composer的原理，在执行composer install时究竟发生了什么？\n<img src=\"/img/composer-install.png\" alt=\"安装过程\">\n可以看出，composer先是安装依赖包，然后将安装的包信息写入lock锁文件，最后生成自动加载文件。查看当前目录下的内容，会发现多了一个composer.lock文件和一个vendor目录，这是composer自动生成的。\n<img src=\"/img/dir.png\" alt=\"安装过程\">\n\n#### composer.lock\ncomposer.lock记录了安装的包的具体版本，包的信息，md5值等。**注意**：在你执行`composer install`时，composer会检查当前目录是否存在composer.lock文件，若存在，则根据该lock文件来安装依赖包而忽略composer.json声明的依赖；若不存在才根据composer.json中的声明来安装。\n\n那么，如果我更改了composer.json需要用到其他的依赖包呢？执行`composer update`（或者通过`composer require 供应商/包名称`来引入）即可，update命令会同时更新composer.lock文件和vendor目录里面的内容。\n\n#### vendor目录\n再来看看vendor目录\n<img src=\"/img/vendor-dir.png\" alt=\"vendor目录\">\nautoload.php  需要引入的入口文件\ncomposer      自动加载文件的目录\nhasankhan     项目源代码目录\n\n大家有没有想过，为什么只需要引入autoload.php就能调用这个包呢？composer是如何处理自动加载关系的呢？来看看autoload.php\n<img src=\"/img/autoload.png\" alt=\"autoload\">\n可以看到，它引入了composer/autoload_real.php文件，然后调用了这个类文件的getLoader()静态方法\n\n```php\n<?php\n\n// autoload_real.php @generated by Composer\n\nclass ComposerAutoloaderInitba35a65e937248ede03565e79e5211fe\n{\n    private static $loader;\n\n    public static function loadClassLoader($class)\n    {\n        if ('Composer\\Autoload\\ClassLoader' === $class) {\n            require __DIR__ . '/ClassLoader.php';\n        }\n    }\n\n    public static function getLoader()\n    {\n        if (null !== self::$loader) {\n            return self::$loader;\n        }\n\n        spl_autoload_register(array('ComposerAutoloaderInitba35a65e937248ede03565e79e5211fe', 'loadClassLoader'), true, true);\n        self::$loader = $loader = new \\Composer\\Autoload\\ClassLoader();\n        spl_autoload_unregister(array('ComposerAutoloaderInitba35a65e937248ede03565e79e5211fe', 'loadClassLoader'));\n\n        $useStaticLoader = PHP_VERSION_ID >= 50600 && !defined('HHVM_VERSION');\n        if ($useStaticLoader) {\n            require_once __DIR__ . '/autoload_static.php';\n\n            call_user_func(\\Composer\\Autoload\\ComposerStaticInitba35a65e937248ede03565e79e5211fe::getInitializer($loader));\n        } else {\n            $map = require __DIR__ . '/autoload_namespaces.php';\n            foreach ($map as $namespace => $path) {\n                $loader->set($namespace, $path);\n            }\n\n            $map = require __DIR__ . '/autoload_psr4.php';\n            foreach ($map as $namespace => $path) {\n                $loader->setPsr4($namespace, $path);\n            }\n\n            $classMap = require __DIR__ . '/autoload_classmap.php';\n            if ($classMap) {\n                $loader->addClassMap($classMap);\n            }\n        }\n\n        $loader->register(true);\n\n        return $loader;\n    }\n}\n```\n看看getLoader()这个静态方法，前面几行spl_autoload_register之类的其实就是函数注册，不用细看，主要看后面几个require，它引入了几个autoload_*.php文件，composer加载依赖关系的精髓就在这几个文件中。\n\n#### 自动加载\ncomposer是通过composer.json文件中的autoload字段来声明自动加载方式的。它提供了4种加载类型:\n\n* psr-0\n* psr-4\n* classmap \n* files\n\n**psr-0**\n\n主要用于带有命名空间的类的自动加载，对应了命名空间和目录的映射，可用于带有下划线的类。此规范已被FIG官方废弃，请使用psr-4代替。\n\n```json\n{\n    \"autoload\": {\n        \"psr-0\": {\n            \"Monolog\\\\\": \"src/\",\n            \"Vendor\\\\Namespace\\\\\": \"src/\",\n            \"Vendor_Namespace_\": \"src/\"\n        }\n    }\n}\n```\n如若要查找Monolog\\Logger这个class时，则其会去寻找src/Monolog/Logger.php，在生成自动加载文件时，会将命名空间与目录的映射以数组的形式写入vendor/composer/autoload_namespaces.php\n\n**psr-4**\n\n和psr-0作用一样，用于替代psr-0，但对于相同的命名空间与目录映射关系，psr-4对应的目录结构比psr-0浅，例如要查找Monolog\\Logger这个class，则其会去寻找src/Logger.php，与psr-0相比少了一层命名空间目录，使用起来更加方便。\n在生成自动加载文件时，会将命名空间与目录的映射以数组的形式写入vendor/composer/autoload_psr4.php\n\n```json\n{\n    \"autoload\": {\n        \"psr-4\": {\n            \"Monolog\\\\\": \"src/\"\n        }\n    }\n}\n```\n\n> **注意：**命名空间必须以\\\\\\结尾，第一个\\是对第二个\\进行转义，\\\\分隔符是为了防止匹配到相似的命名空间，假设存在MonologTest这样的命名空间，若不加\\\\\\，在匹配Monolog时可能会匹配到MonologTest。\n\n**classmap**\n\n该自动加载类型用于声明不包含命名空间的类\n\n```json\n{\n    \"autoload\": {\n        \"classmap\":[\"lib/\",\"src/other.php\"] \n    }\n}\n```\n如上述classmap加载方式会包含lib目录下所有.php和.inc结尾的文件和src/other.php，在生成自动加载文件时，会将类名与文件路径的映射以数组的形式写入vendor/composer/autoload_classmap.php\n\n**files**\n\n主要用于声明全局函数的文件而不是类文件\n\n```json\n{\n    \"autoload\": {\n        \"files\": [\"src/functions.php\"]\n    }\n}\n```\n这种加载方式使得src/functions.php中的函数可以在全局调用了，同样，在生成自动加载文件时，会将映射关系以数组的形式写入vendor/composer/autoload_files.php\n\n#### 自动加载演示\n\n例如我的项目根目录test下的类库文件目录结构是这样的\n\n```\ntest/\n  src/\n    lib/\n      DB.php\n    Main.php\n    functions.php\n  index.php\n  composer.json\n```\nsrc目录下的内容是源代码，其代码分别如下\n\n```php\n<?php\n// DB.php\nclass DB\n{\n    static public function getInstance()\n    {\n        echo 'you are calling getInstance method' . PHP_EOL;\n    }\n}\n```\n\n```php\n<?php\n// Main.php\nnamespace Ths;\n\nclass Main\n{\n    public function getData()\n    {\n        echo 'you are calling getData method' . PHP_EOL;\n    }\n}\n```\n\n```php\n<?php\n// functions.php\nfunction getCommon()\n{\n    echo 'you are calling getCommon method' . PHP_EOL;\n}\n```\n\n根据类库文件，在composer.json中定义自动加载方式（其路径都是相对composer.json所在路径来定义的）\n\n```json\n{\n    \"autoload\": {\n        \"psr-4\": { \"Ths\\\\\": \"src/\"\n        },\n        \"classmap\":[\"src/lib/\"], \n        \"files\": [\"src/functions.php\"]\n    }\n}\n```\n然后执行`composer install`，执行完后进入vendor/composer/，查看autoload_*.php文件，均已生成自动加载对应的映射关系（因为并未定义psr-0，所以其返回的是一个空数组）。\n\n```php\n<?php\n\n// autoload_psr4.php @generated by Composer\n\n$vendorDir = dirname(dirname(__FILE__));\n$baseDir = dirname($vendorDir);\n\nreturn array(\n    'Ths\\\\' => array($baseDir . '/src'),\n);\n```\n\n```php\n<?php\n\n// autoload_classmap.php @generated by Composer\n\n$vendorDir = dirname(dirname(__FILE__));\n$baseDir = dirname($vendorDir);\n\nreturn array(\n    'DB' => $baseDir . '/src/lib/DB.php',\n);\n```\n\n```php\n<?php\n\n// autoload_files.php @generated by Composer\n\n$vendorDir = dirname(dirname(__FILE__));\n$baseDir = dirname($vendorDir);\n\nreturn array(\n    '5d80ba682afba25d348d62676196765b' => $baseDir . '/src/functions.php',\n);\n```\n\n```php\n<?php\n\n// autoload_namespaces.php @generated by Composer\n\n$vendorDir = dirname(dirname(__FILE__));\n$baseDir = dirname($vendorDir);\n\nreturn array(\n);\n```\n\n现在我们在index.php中调用这些文件里的几个方法，代码如下：\n\n```php\n<?php\nrequire 'vendor/autoload.php';\nuse Ths\\Main;\n\n$main = new Main();\n$main->getData();   // 输出：you are calling getData method\n\nDB::getInstance();  // 输出：you are calling getInstance method\n\ngetCommon();        // 输出：you are calling getCommon method\n```\n到这里，我们成功的定义了自动加载方式，并使用了它，我想现在大家应该很清楚了，composer的自动加载其实就是根据autoload的定义，将对应的映射关系写入各自的autoload_*.php中，然后require进来，就可以直接调用各个类中的方法了。建议大家亲自试试，对自动加载的更加容易理解。\n\n那么，我现在需要修改类库，比如在lib/下新增了一个类文件，现在能直接调用吗?不能，因为我们的autoload_*.php中并没有该类文件的映射，这时我们就要更新composer/下的autoload_*.php了，执行`composer dump-autoload`命令即可，这样我们就可以调用新增的类库文件了。\n\n## composer.json架构\n具体架构请参考官方文档，这里只讲解几个字段\nautoload 自动加载\nautoload-dev 开发环境自动加载\nrequrie 依赖声明\nrequrie-dev 开发环境依赖声明\n比如在开发时，我们往往会写一个单元测试包，需要用到phpunit，这时可以在composer.json中这样定义\n\n```json\n{\n    \"require-dev\":{\n        \"phpunit/phpunit\":\"~4.0\"\n    },\n    \"autoload-dev\": {\n        \"psr-4\": { \"Ths\\\\Test\\\\\": \"tests/\"\n        }\n    }\n}\n```\n其中，tests是自己写的单元测试。\n如果composer.json包含require-dev字段，则composer install会默认安装dev声明的依赖包，如果不想这么做，在install命令后面加上--no-dev参数即可，即`composer install --no-dev`\n\n（完）\n"
    }
  ]
}